{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM trained on gridded forcings for each station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn import preprocessing\n",
    "import netCDF4 as nc\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from src import load_data, evaluate\n",
    "import torch.autograd as autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = False\n",
    "if torch.cuda.is_available():\n",
    "    print('CUDA Available')\n",
    "    USE_CUDA = True\n",
    "device = torch.device('cuda' if USE_CUDA else 'cpu')\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../src/load_data.py:18: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support skipfooter; you can avoid this warning by specifying engine='python'.\n",
      "  data = pd.read_csv(os.path.join(dir, f), skiprows=2, skipfooter=1, index_col=False, header=None, names=['runoff'], na_values='-1.2345')\n",
      "/home/mgauch/runoff-nn/gwf/lib/python3.6/site-packages/pandas/core/frame.py:6692: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    }
   ],
   "source": [
    "station_data_dict = load_data.load_train_test_lstm()\n",
    "data_runoff = load_data.load_discharge_gr4j_vic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMRegression(nn.Module):\n",
    "        def __init__(self, input_dim, hidden_dim, num_layers, batch_size, dropout):\n",
    "            super(LSTMRegression, self).__init__()\n",
    "            self.batch_size = batch_size\n",
    "            self.hidden_dim = hidden_dim\n",
    "            self.num_layers = num_layers\n",
    "            self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, dropout=dropout)\n",
    "            self.linear = nn.Linear(hidden_dim, 1)\n",
    "            self.hidden = self.init_hidden()\n",
    "        def init_hidden(self):\n",
    "            return (torch.randn(self.num_layers, self.batch_size, self.hidden_dim, device=device),\n",
    "                    torch.randn(self.num_layers, self.batch_size, self.hidden_dim, device=device))\n",
    "\n",
    "        def forward(self, input):\n",
    "            lstm_out, self.hidden = self.lstm(input, self.hidden)\n",
    "            return self.linear(lstm_out[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['02GC026',\n",
       " '02GD004',\n",
       " '02GE007',\n",
       " '02GG002',\n",
       " '02GG003',\n",
       " '02GG006',\n",
       " '02GG009',\n",
       " '02GG013',\n",
       " '04159492',\n",
       " '04159900',\n",
       " '04160600',\n",
       " '04161820',\n",
       " '04164000',\n",
       " '04165500',\n",
       " '04166100',\n",
       " '04166500',\n",
       " '04174500',\n",
       " '04176500',\n",
       " '04177000',\n",
       " '04193500',\n",
       " '04195820',\n",
       " '04196800',\n",
       " '04197100',\n",
       " '04198000',\n",
       " '04199000',\n",
       " '04199500',\n",
       " '04200500',\n",
       " '04207200',\n",
       " '04208504',\n",
       " '04209000',\n",
       " '04212100',\n",
       " '04213000',\n",
       " '04213500',\n",
       " '04214500',\n",
       " '04215000',\n",
       " '04215500']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(station_data_dict.keys())[10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 2012-01-07 - 2012-12-31, Test: 2013-01-01 - 2013-02-28\n",
      "Patience exhausted in epoch 171. Best loss was 0.17365201848442666\n",
      "Using best model from epoch 120 which had loss 0.17365201848442666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mgauch/runoff-nn/gwf/lib/python3.6/site-packages/pandas/plotting/_converter.py:129: FutureWarning: Using an implicitly registered datetime converter for a matplotlib plotting method. The converter was registered by pandas on import. Future versions of pandas will require you to explicitly register matplotlib converters.\n",
      "\n",
      "To register the converters:\n",
      "\t>>> from pandas.plotting import register_matplotlib_converters\n",
      "\t>>> register_matplotlib_converters()\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patience exhausted in epoch 208. Best loss was 0.09215442987697316\n",
      "Using best model from epoch 157 which had loss 0.09215442987697316\n",
      "Patience exhausted in epoch 163. Best loss was 0.10070072326683051\n",
      "Using best model from epoch 112 which had loss 0.10070072326683051\n",
      "Patience exhausted in epoch 156. Best loss was 0.22471882685980138\n",
      "Using best model from epoch 105 which had loss 0.22471882685980138\n",
      "Patience exhausted in epoch 251. Best loss was 5.482428623487552\n",
      "Using best model from epoch 200 which had loss 5.482428623487552\n",
      "Patience exhausted in epoch 170. Best loss was 0.041212958513066646\n",
      "Using best model from epoch 119 which had loss 0.041212958513066646\n",
      "Patience exhausted in epoch 160. Best loss was 0.09987562822619414\n",
      "Using best model from epoch 109 which had loss 0.09987562822619414\n",
      "Patience exhausted in epoch 127. Best loss was 0.12189095471815865\n",
      "Using best model from epoch 76 which had loss 0.12189095471815865\n",
      "Patience exhausted in epoch 174. Best loss was 0.07522337467623098\n",
      "Using best model from epoch 123 which had loss 0.07522337467623098\n",
      "Station 02GC018 had NA runoff values. Skipping.\n",
      "  NSEs: [0.050337667307326917, 0.31410102416785723, 0.41286484309510196, 0.19126674729100324, 0.2776757499108159, -0.02948046113988667, 0.46254789976523447, 0.2244072726989298, 0.43688584082484627]:\n",
      "Train: 2012-01-07 - 2013-02-28, Test: 2013-03-01 - 2013-04-30\n",
      "Patience exhausted in epoch 209. Best loss was 0.6038836969030023\n",
      "Using best model from epoch 158 which had loss 0.6038836969030023\n",
      "Patience exhausted in epoch 230. Best loss was 0.17875261190338998\n",
      "Using best model from epoch 179 which had loss 0.17875261190338998\n",
      "Patience exhausted in epoch 180. Best loss was 0.1084561181759317\n",
      "Using best model from epoch 129 which had loss 0.1084561181759317\n",
      "Patience exhausted in epoch 152. Best loss was 0.31629249367782536\n",
      "Using best model from epoch 101 which had loss 0.31629249367782536\n",
      "Patience exhausted in epoch 232. Best loss was 9.18495463992623\n",
      "Using best model from epoch 181 which had loss 9.18495463992623\n",
      "Patience exhausted in epoch 173. Best loss was 0.06639832011229281\n",
      "Using best model from epoch 122 which had loss 0.06639832011229281\n",
      "Patience exhausted in epoch 243. Best loss was 0.10129522928748641\n",
      "Using best model from epoch 192 which had loss 0.10129522928748641\n",
      "Patience exhausted in epoch 127. Best loss was 0.17198383289459665\n",
      "Using best model from epoch 76 which had loss 0.17198383289459665\n",
      "Patience exhausted in epoch 192. Best loss was 0.058802697117643084\n",
      "Using best model from epoch 141 which had loss 0.058802697117643084\n",
      "Station 02GC018 had NA runoff values. Skipping.\n",
      "  NSEs: [-0.34403091757310866, -0.17095487720817037, -0.1515330916262534, -1.0786986236050593, -0.5316458145259191, -0.1169073669932077, -0.02643624038407566, -0.551346265192028, -0.17724980381846822]:\n",
      "Train: 2012-01-07 - 2013-04-30, Test: 2013-05-01 - 2013-06-30\n",
      "Patience exhausted in epoch 270. Best loss was 0.728716877050465\n",
      "Using best model from epoch 219 which had loss 0.728716877050465\n",
      "Patience exhausted in epoch 262. Best loss was 0.3410022011329602\n",
      "Using best model from epoch 211 which had loss 0.3410022011329602\n",
      "Patience exhausted in epoch 294. Best loss was 0.15625841892560857\n",
      "Using best model from epoch 243 which had loss 0.15625841892560857\n",
      "Patience exhausted in epoch 278. Best loss was 0.29097639316532875\n",
      "Using best model from epoch 227 which had loss 0.29097639316532875\n",
      "Patience exhausted in epoch 260. Best loss was 23.270684199081735\n",
      "Using best model from epoch 209 which had loss 23.270684199081735\n",
      "Patience exhausted in epoch 242. Best loss was 0.14371395390280667\n",
      "Using best model from epoch 191 which had loss 0.14371395390280667\n",
      "Patience exhausted in epoch 240. Best loss was 0.1515052565577207\n",
      "Using best model from epoch 189 which had loss 0.1515052565577207\n",
      "Patience exhausted in epoch 213. Best loss was 0.18032635047347867\n",
      "Using best model from epoch 162 which had loss 0.18032635047347867\n",
      "Patience exhausted in epoch 247. Best loss was 0.28789130471559476\n",
      "Using best model from epoch 196 which had loss 0.28789130471559476\n",
      "Station 02GC018 had NA runoff values. Skipping.\n",
      "  NSEs: [-0.33012695199776076, -0.2299018267842936, -0.16835345547090275, -4.551405404230547, -0.8816935344912025, -0.4286692368085874, -0.1270265031478941, -2.1007618030398008, -0.6443893718237192]:\n",
      "Train: 2012-01-07 - 2013-06-30, Test: 2013-07-01 - 2013-08-31\n",
      "Patience exhausted in epoch 250. Best loss was 1.1072083991986195\n",
      "Using best model from epoch 199 which had loss 1.1072083991986195\n",
      "Patience exhausted in epoch 200. Best loss was 0.5839353678980842\n",
      "Using best model from epoch 149 which had loss 0.5839353678980842\n",
      "Using best model from epoch 271 which had loss 0.38856659973066093\n",
      "Patience exhausted in epoch 241. Best loss was 0.2654595622230166\n",
      "Using best model from epoch 190 which had loss 0.2654595622230166\n",
      "Patience exhausted in epoch 258. Best loss was 30.93128160279658\n",
      "Using best model from epoch 207 which had loss 30.93128160279658\n",
      "Patience exhausted in epoch 201. Best loss was 0.22972149402420555\n",
      "Using best model from epoch 150 which had loss 0.22972149402420555\n",
      "Patience exhausted in epoch 223. Best loss was 0.11509388204625187\n",
      "Using best model from epoch 172 which had loss 0.11509388204625187\n",
      "Patience exhausted in epoch 262. Best loss was 0.09913197875850731\n",
      "Using best model from epoch 211 which had loss 0.09913197875850731\n",
      "Patience exhausted in epoch 185. Best loss was 0.39987227695172706\n",
      "Using best model from epoch 134 which had loss 0.39987227695172706\n",
      "Station 02GC018 had NA runoff values. Skipping.\n",
      "  NSEs: [0.18185783529945443, 0.5766051238447546, -0.043649860724097156, -0.22800368157960094, -0.2636127492742797, 0.037855641123626826, -0.14102652817979333, -1.7088701039311713, -0.47208680506943224]:\n",
      "Train: 2012-01-07 - 2013-08-31, Test: 2013-09-01 - 2013-10-31\n",
      "Patience exhausted in epoch 273. Best loss was 1.056562165919673\n",
      "Using best model from epoch 222 which had loss 1.056562165919673\n",
      "Using best model from epoch 285 which had loss 0.45900130204610823\n",
      "Patience exhausted in epoch 263. Best loss was 0.2902899801456578\n",
      "Using best model from epoch 212 which had loss 0.2902899801456578\n",
      "Using best model from epoch 257 which had loss 0.27446050647952\n",
      "Using best model from epoch 256 which had loss 22.509780378484013\n",
      "Patience exhausted in epoch 204. Best loss was 0.17340404698648373\n",
      "Using best model from epoch 153 which had loss 0.17340404698648373\n",
      "Patience exhausted in epoch 270. Best loss was 0.2320032034666435\n",
      "Using best model from epoch 219 which had loss 0.2320032034666435\n",
      "Patience exhausted in epoch 238. Best loss was 0.19783122121125915\n",
      "Using best model from epoch 187 which had loss 0.19783122121125915\n",
      "Patience exhausted in epoch 246. Best loss was 0.29170504565679684\n",
      "Using best model from epoch 195 which had loss 0.29170504565679684\n",
      "Station 02GC018 had NA runoff values. Skipping.\n",
      "  NSEs: [-0.26749744656177565, -0.13638789181295463, -0.0504227155880006, -1.1062557574127254, -0.2744113110107076, -0.0010859789517794205, -0.055832578355291984, -0.10396760858808118, -0.29538919686238185]:\n",
      "Train: 2012-01-07 - 2013-10-31, Test: 2013-11-01 - 2013-12-31\n",
      "Patience exhausted in epoch 284. Best loss was 1.4744996843792126\n",
      "Using best model from epoch 233 which had loss 1.4744996843792126\n",
      "Using best model from epoch 267 which had loss 0.4851664611016194\n",
      "Using best model from epoch 299 which had loss 0.32043215423128124\n",
      "Using best model from epoch 258 which had loss 0.21905966413763617\n",
      "Using best model from epoch 269 which had loss 35.97104007019177\n",
      "Patience exhausted in epoch 180. Best loss was 0.3576520357028912\n",
      "Using best model from epoch 129 which had loss 0.3576520357028912\n",
      "Using best model from epoch 267 which had loss 0.15295872749545827\n",
      "Patience exhausted in epoch 208. Best loss was 0.31863888809288854\n",
      "Using best model from epoch 157 which had loss 0.31863888809288854\n",
      "Patience exhausted in epoch 297. Best loss was 0.313245151621791\n",
      "Using best model from epoch 246 which had loss 0.313245151621791\n",
      "Station 02GC018 had NA runoff values. Skipping.\n",
      "  NSEs: [-0.03635542391153357, 0.019579790206707193, -0.00395837975089286, -0.4684837501991077, 0.10085904261014178, -0.11502398021437332, 0.006094236932483343, -0.35480126585322447, 0.1141511158406332]:\n"
     ]
    }
   ],
   "source": [
    "predictions = {}\n",
    "actuals = {}\n",
    "models = {}\n",
    "seq_len = 5 * 24\n",
    "train_start = datetime.strptime('2012-01-01', '%Y-%m-%d') + timedelta(days=seq_len // 24 + 1)\n",
    "train_ends = ['2012-12-31', '2013-02-28', '2013-04-30', '2013-06-30', '2013-08-31', '2013-10-31']\n",
    "test_ends = train_ends[1:] + ['2013-12-31']\n",
    "\n",
    "\n",
    "plot_list = ['04159492']\n",
    "median_nse_list = []\n",
    "for cv_iter in range(len(train_ends)):\n",
    "    train_end = train_ends[cv_iter]\n",
    "    test_start = datetime.strptime(train_end, '%Y-%m-%d') + timedelta(days=1)\n",
    "    test_end = test_ends[cv_iter]\n",
    "    print('Train: {} - {}, Test: {} - {}'.format(train_start.strftime('%Y-%m-%d'), train_end, test_start.strftime('%Y-%m-%d'), test_end))\n",
    "    \n",
    "    nse_list = []\n",
    "    cv_name = '_CV_{}-{}'.format(test_start.strftime('%Y-%m-%d'), test_end)\n",
    "    for station in list(station_data_dict.keys())[:10]:\n",
    "        station_rdrs = station_data_dict[station]\n",
    "        station_runoff = data_runoff[data_runoff['station'] == station].set_index('date')\n",
    "        if any(station_runoff['runoff'].isna()):\n",
    "            print('Station', station, 'had NA runoff values. Skipping.')\n",
    "            continue\n",
    "\n",
    "        station_train = station_rdrs.loc[train_start : train_end]\n",
    "        station_test = station_rdrs.loc[test_start : test_end]\n",
    "        num_train_days = len(pd.date_range(train_start, train_end, freq='D'))\n",
    "\n",
    "        x = np.zeros((seq_len, len(pd.date_range(train_start, test_end, freq='D')), station_rdrs.shape[1]))\n",
    "        for day in range(x.shape[1]):\n",
    "            x[:,day,:] = station_rdrs[train_start - timedelta(hours = seq_len - 1) + timedelta(days=day) : train_start + timedelta(days=day)]\n",
    "\n",
    "        # Scale training data\n",
    "        scalers = []  # save scalers to apply them to test data later\n",
    "        x_train = x[:,:num_train_days,:]\n",
    "        for i in range(x.shape[2]):\n",
    "            scalers.append(preprocessing.StandardScaler())\n",
    "            x_train[:,:,i] = scalers[i].fit_transform(x_train[:,:,i].reshape((-1, 1))).reshape(x_train[:,:,i].shape)\n",
    "        x_train = torch.from_numpy(x_train).float().to(device)\n",
    "        y_train = torch.from_numpy(station_runoff.loc[train_start:train_end, 'runoff'].to_numpy()).float().to(device)\n",
    "\n",
    "        # Train model\n",
    "        learning_rate = 2e-3\n",
    "        patience = 50\n",
    "        min_improvement = 0.05\n",
    "        best_loss_model = (-1, np.inf, None)\n",
    "\n",
    "        # Prepare model\n",
    "        H = 200\n",
    "        batch_size = 3\n",
    "        lstm_layers = 2\n",
    "        dropout = 0.3\n",
    "        model = LSTMRegression(station_rdrs.shape[1], H, lstm_layers, batch_size, dropout).to(device)\n",
    "        loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "        for epoch in range(300):\n",
    "            epoch_losses = []\n",
    "            for i in range(num_train_days // batch_size):\n",
    "                model.hidden = model.init_hidden()\n",
    "                y_pred = model(x_train[:,i*batch_size : (i+1)*batch_size,:])\n",
    "\n",
    "                loss = loss_fn(y_pred, y_train[i*batch_size : (i+1)*batch_size].reshape((batch_size,1))).to(device)\n",
    "                epoch_losses.append(loss.item())\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            epoch_loss = np.array(epoch_losses).mean()\n",
    "            writer.add_scalar('loss_' + station + cv_name, epoch_loss, epoch)\n",
    "            if epoch_loss < best_loss_model[1] - min_improvement:\n",
    "                best_loss_model = (epoch, epoch_loss, model.state_dict())  # new best model\n",
    "            elif epoch > best_loss_model[0] + patience:\n",
    "                print('Patience exhausted in epoch {}. Best loss was {}'.format(epoch, best_loss_model[1]))\n",
    "                break\n",
    "\n",
    "        print('Using best model from epoch', str(best_loss_model[0]), 'which had loss', str(best_loss_model[1]))\n",
    "        model.load_state_dict(best_loss_model[2])\n",
    "        model.eval()        \n",
    "\n",
    "        # scale test data\n",
    "        x_test = x[:,num_train_days:,:]\n",
    "        for i in range(x.shape[2]):\n",
    "            x_test[:,:,i] = scalers[i].transform(x_test[:,:,i].reshape((-1, 1))).reshape(x_test[:,:,i].shape)\n",
    "        # if batch size doesn't align with number of samples, add dummies to the last batch\n",
    "        if x_test.shape[1] % batch_size != 0:\n",
    "            x_test = np.concatenate([x_test, np.zeros((x_test.shape[0], batch_size - (x_test.shape[1] % batch_size), x_test.shape[2]))], axis=1)\n",
    "\n",
    "        x_test = torch.from_numpy(x_test).float().to(device)\n",
    "        predict = station_runoff[test_start:test_end].copy()\n",
    "        predict['runoff'] = np.nan\n",
    "        pred_array = np.array([])\n",
    "        for i in range(x_test.shape[1] // batch_size):\n",
    "            pred_array = np.concatenate([pred_array, model(x_test[:,i*batch_size : (i+1)*batch_size,:]).detach().cpu().numpy().reshape(batch_size)])\n",
    "        predict['runoff'] = pred_array[:predict.shape[0]]  # ignore dummies\n",
    "        predictions[station] = predict\n",
    "        actuals[station] = station_runoff['runoff'].loc[test_start:test_end]\n",
    "        models[station] = model\n",
    "        \n",
    "        nse = evaluate.evaluate_daily(station + cv_name, predict['runoff'], actuals[station], writer=writer)\n",
    "        nse_list.append(nse)\n",
    "    \n",
    "    print('  NSEs: {}:'.format(nse_list))\n",
    "    writer.add_histogram('NSE', np.array(nse_list), cv_iter)\n",
    "    median_nse_list.append(np.median(nse_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2776757499108159,\n",
       " -0.17724980381846822,\n",
       " -0.4286692368085874,\n",
       " -0.14102652817979333,\n",
       " -0.13638789181295463,\n",
       " -0.00395837975089286]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_nse_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
