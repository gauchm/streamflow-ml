{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STGCN trained on simulated streamflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20190825-153140'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt \n",
    "from datetime import datetime, timedelta\n",
    "import netCDF4 as nc\n",
    "import torch\n",
    "from torch import nn, utils\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from src import load_data, evaluate, conv_lstm, datasets, utils, stgcn\n",
    "import random\n",
    "import pickle\n",
    "import json\n",
    "import networkx as nx\n",
    "\n",
    "time_stamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "time_stamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "fhandler = logging.FileHandler(filename='../log.out', mode='a')\n",
    "chandler = logging.StreamHandler(sys.stdout)\n",
    "formatter = logging.Formatter('%(asctime)s - {} - %(message)s'.format(time_stamp))\n",
    "fhandler.setFormatter(formatter)\n",
    "chandler.setFormatter(formatter)\n",
    "logger.addHandler(fhandler)\n",
    "logger.addHandler(chandler)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available\n",
      "2019-08-25 15:31:40,335 - 20190825-153140 - cuda devices: ['Tesla V100-SXM2-16GB']\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = False\n",
    "if torch.cuda.is_available():\n",
    "    print('CUDA Available')\n",
    "    USE_CUDA = True\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "device = torch.device('cuda:0' if USE_CUDA else 'cpu')\n",
    "num_devices = torch.cuda.device_count() if USE_CUDA else 0\n",
    "logger.warning('cuda devices: {}'.format(list(torch.cuda.get_device_name(i) for i in range(num_devices))))\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioning_strategy = 'distance'  # 'distance' or 'unilabel', see https://arxiv.org/abs/1801.07455\n",
    "max_hops = 3 if partitioning_strategy == 'distance' else None\n",
    "rdrs_vars = list(range(8))\n",
    "agg = ['minmax','minmax','minmax','minmax','sum','minmax','minmax','minmax']\n",
    "include_month = True\n",
    "dem, landcover, soil, groundwater = False, False, False, False\n",
    "landcover_types = []\n",
    "seq_len = 14\n",
    "seq_steps = 1\n",
    "\n",
    "train_start = datetime.strptime('2010-01-01', '%Y-%m-%d') + timedelta(days=seq_len * seq_steps)  # first day for which to make a prediction in train set\n",
    "train_end = '2012-12-31'\n",
    "test_start = '2013-01-01'\n",
    "test_end = '2014-12-31'\n",
    "spatial_val = True  # Whether the val set does spatial or temporal validation\n",
    "val_fraction = 0.1 if not spatial_val else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/simulations_shervan/subbasins.geojson', 'r') as f:\n",
    "     subbasin_shapes = json.loads(f.read())\n",
    "\n",
    "subbasin_graph = utils.create_subbasin_graph()\n",
    "component_graph = subbasin_graph.copy()\n",
    "component_graph.remove_nodes_from(['sub-1', 'sub1', 'sub474'])  # remove Lake Erie and sink to get connected components\n",
    "connected_components = sorted(sorted(c) for c in nx.connected_components(nx.Graph(component_graph)))\n",
    "\n",
    "# Split into train/test/val regions\n",
    "test_subbasins = [1, 474]\n",
    "train_subbasins = []\n",
    "val_subbasins = []\n",
    "train_components = []\n",
    "for component in connected_components:\n",
    "    max_x = -999\n",
    "    for node in component:\n",
    "        subbasin = list(s['properties'] for s in subbasin_shapes['features'] if 'sub' + str(s['properties']['SubId']) == node)[0]\n",
    "        max_x = max(max_x, subbasin['INSIDE_X'])\n",
    "    if max_x < -81.9:\n",
    "        train_components.append(list(int(c[3:]) for c in component))\n",
    "        train_subbasins += train_components[-1]\n",
    "    elif -80.6 > max_x and max_x >= -81.9:\n",
    "        val_subbasins += list(int(c[3:]) for c in component)\n",
    "    else:\n",
    "        test_subbasins += list(int(c[3:]) for c in component)\n",
    "        \n",
    "if not spatial_val:  # if no spatial validation, use same graph but different samples\n",
    "    train_subbasins += val_subbasins\n",
    "    val_subbasins = train_subbasins\n",
    "train_subbasins, val_subbasins, test_subbasins = sorted(train_subbasins), sorted(val_subbasins), sorted(test_subbasins)\n",
    "\n",
    "# For each train-component, get its nodes' indices into train_subbasins. \n",
    "# We'll use them to create the adjacency matrix of the component during training.\n",
    "train_component_indices = []\n",
    "for component in train_components:\n",
    "    train_component_indices.append(sorted(train_subbasins.index(c) for c in component))\n",
    "\n",
    "train_subgraph = subbasin_graph.subgraph(list('sub' + str(t) for t in train_subbasins))\n",
    "val_subgraph = subbasin_graph.subgraph(list('sub' + str(t) for t in val_subbasins))\n",
    "test_subgraph = subbasin_graph.subgraph(list('sub' + str(t) for t in test_subbasins))\n",
    "if partitioning_strategy == 'unilabel':\n",
    "    train_adjacency = torch.unsqueeze(torch.from_numpy(nx.to_numpy_array(train_subgraph, nodelist=list('sub' + str(t) for t in train_subbasins))), 0).float().to(device)\n",
    "    val_adjacency = torch.unsqueeze(torch.from_numpy(nx.to_numpy_array(val_subgraph, nodelist=list('sub' + str(t) for t in val_subbasins))), 0).float().to(device)\n",
    "    test_adjacency = torch.unsqueeze(torch.from_numpy(nx.to_numpy_array(test_subraph, nodelist=list('sub' + str(t) for t in test_subbasins))), 0).float().to(device)\n",
    "elif partitioning_strategy == 'distance':  # use distances in upstream-graph, i.e. in reversed downstream-graph\n",
    "    train_adjacency = utils.create_hop_matrix(train_subgraph.reverse(), max_hops, list('sub' + str(t) for t in train_subbasins)).float().to(device)\n",
    "    val_adjacency = utils.create_hop_matrix(val_subgraph.reverse(), max_hops, list('sub' + str(t) for t in val_subbasins)).float().to(device)\n",
    "    test_adjacency = utils.create_hop_matrix(test_subgraph.reverse(), max_hops, list('sub' + str(t) for t in test_subbasins)).float().to(device)\n",
    "else:\n",
    "    raise Exception('Unsupported partitioning strategy')\n",
    "\n",
    "subbasins = sorted(set(train_subbasins + test_subbasins + val_subbasins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using saved dataset in file /home/mgauch/runoff-nn/src/../data/train_test/SubbasinAggregatedDataset_0-1-2-3-4-5-6-7_14-1_2010-01-15000000-2012-12-31___minmax-minmax-minmax-minmax-sum-minmax-minmax-minmax_month_a173b099769a13df5073c0445e2a7542.pkl\n",
      "Using saved dataset in file /home/mgauch/runoff-nn/src/../data/train_test/SubbasinAggregatedDataset_0-1-2-3-4-5-6-7_14-1_2010-01-15000000-2012-12-31___minmax-minmax-minmax-minmax-sum-minmax-minmax-minmax_month_638af19f0a2e2c08293bc446025de5a0.pkl\n",
      "Using saved dataset in file /home/mgauch/runoff-nn/src/../data/train_test/SubbasinAggregatedDataset_0-1-2-3-4-5-6-7_14-1_2013-01-01-2014-12-31___minmax-minmax-minmax-minmax-sum-minmax-minmax-minmax_month_70c2afaf065ec0efccaee01a5e56aa07.pkl\n",
      "Using saved dataset in file /home/mgauch/runoff-nn/src/../data/train_test/SubbasinAggregatedDataset_0-1-2-3-4-5-6-7_14-1_2013-01-01-2014-12-31___minmax-minmax-minmax-minmax-sum-minmax-minmax-minmax_month_a173b099769a13df5073c0445e2a7542.pkl\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.SubbasinAggregatedDataset(rdrs_vars, train_subbasins, seq_len, seq_steps, train_start, train_end, aggregate_daily=agg, include_months=include_month, \n",
    "                                                   dem=dem, landcover=landcover, soil=soil, groundwater=groundwater, landcover_types=landcover_types)\n",
    "if spatial_val:\n",
    "    val_dataset = datasets.SubbasinAggregatedDataset(rdrs_vars, val_subbasins, seq_len, seq_steps, train_start, train_end, aggregate_daily=agg, include_months=include_month, \n",
    "                                                     conv_scalers=train_dataset.scalers, dem=dem, landcover=landcover, soil=soil, groundwater=groundwater, landcover_types=landcover_types)\n",
    "else:\n",
    "    val_dataset = train_dataset\n",
    "\n",
    "# Two test datasets: one with spatial and temporal validation (i.e., different graph, different time), and one with only temporal validation (i.e. different time period only)\n",
    "spatial_test_dataset = None\n",
    "if test_subbasins != train_subbasins:\n",
    "    spatial_test_dataset = datasets.SubbasinAggregatedDataset(rdrs_vars, test_subbasins, seq_len, seq_steps, test_start, test_end, aggregate_daily=agg, include_months=include_month, \n",
    "                                                              conv_scalers=train_dataset.scalers, dem=dem, landcover=landcover, soil=soil, groundwater=groundwater, landcover_types=landcover_types)\n",
    "temporal_test_dataset = datasets.SubbasinAggregatedDataset(rdrs_vars, train_subbasins, seq_len, seq_steps, test_start, test_end, aggregate_daily=agg, include_months=include_month, \n",
    "                                                           conv_scalers=train_dataset.scalers, dem=dem, landcover=landcover, soil=soil, groundwater=groundwater, landcover_types=landcover_types)\n",
    "\n",
    "station_subbasins = train_dataset.simulated_streamflow[~pd.isna(train_dataset.simulated_streamflow['StationID'])]['subbasin'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'time_stamp': '20190825-153140', 'batch_size': 16, 'loss': NSELoss(), 'include_month': True, 'aggregate_daily': ['minmax', 'minmax', 'minmax', 'minmax', 'sum', 'minmax', 'minmax', 'minmax'], 'rdrs_vars': [0, 1, 2, 3, 4, 5, 6, 7], 'dropout': 0.3, 'spatial_validation': True, 'val_fraction': None, 'temp_kernel': 3, 'optimizer': Adam (\\nParameter Group 0\\n    amsgrad: False\\n    betas: (0.9, 0.999)\\n    eps: 1e-08\\n    lr: 0.002\\n    weight_decay: 1e-05\\n), 'lr': 0.002, 'patience': 800, 'min_improvement': 0.01, 'x_train_shape': torch.Size([1082, 14, 82, 328]), 'x_val_shape': torch.Size([1082, 14, 82, 258]), 'partitioning_strategy': 'distance', 'max_hops': 3, 'spatial_x_test_shape': torch.Size([730, 14, 82, 138]), 'temporal_x_test_shape': torch.Size([730, 14, 82, 328]), 'num_epochs': 1000, 'seq_len': 14, 'seq_steps': 1, 'train_start': datetime.datetime(2010, 1, 15, 0, 0), 'train_end': '2012-12-31', 'weight_decay': 1e-05, 'landcover_types': [], 'test_start': '2013-01-01', 'test_end': '2014-12-31', 'model': 'Model((data_bn):Identity()(st_gcn_networks):ModuleList((0):st_gcn((gcn):ConvTemporalGraphical((conv):Conv2d(82,192,kernel_size=(1,1),stride=(1,1)))(tcn):Sequential((0):BatchNorm2d(64,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)(1):ReLU(inplace=True)(2):Conv2d(64,64,kernel_size=(3,1),stride=(1,1),padding=(1,0))(3):BatchNorm2d(64,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)(4):Dropout(p=0,inplace=True))(relu):ReLU(inplace=True))(1):st_gcn((gcn):ConvTemporalGraphical((conv):Conv2d(64,192,kernel_size=(1,1),stride=(1,1)))(tcn):Sequential((0):BatchNorm2d(64,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)(1):ReLU(inplace=True)(2):Conv2d(64,64,kernel_size=(3,1),stride=(1,1),padding=(1,0))(3):BatchNorm2d(64,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)(4):Dropout(p=0.3,inplace=True))(relu):ReLU(inplace=True))(2):st_gcn((gcn):ConvTemporalGraphical((conv):Conv2d(64,192,kernel_size=(1,1),stride=(1,1)))(tcn):Sequential((0):BatchNorm2d(64,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)(1):ReLU(inplace=True)(2):Conv2d(64,64,kernel_size=(3,1),stride=(1,1),padding=(1,0))(3):BatchNorm2d(64,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)(4):Dropout(p=0.3,inplace=True))(relu):ReLU(inplace=True))(3):st_gcn((gcn):ConvTemporalGraphical((conv):Conv2d(64,192,kernel_size=(1,1),stride=(1,1)))(tcn):Sequential((0):BatchNorm2d(64,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)(1):ReLU(inplace=True)(2):Conv2d(64,64,kernel_size=(3,1),stride=(1,1),padding=(1,0))(3):BatchNorm2d(64,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)(4):Dropout(p=0.3,inplace=True))(relu):ReLU(inplace=True))(4):st_gcn((gcn):ConvTemporalGraphical((conv):Conv2d(64,384,kernel_size=(1,1),stride=(1,1)))(tcn):Sequential((0):BatchNorm2d(128,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)(1):ReLU(inplace=True)(2):Conv2d(128,128,kernel_size=(3,1),stride=(2,1),padding=(1,0))(3):BatchNorm2d(128,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)(4):Dropout(p=0.3,inplace=True))(residual):Sequential((0):Conv2d(64,128,kernel_size=(1,1),stride=(2,1))(1):BatchNorm2d(128,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True))(relu):ReLU(inplace=True))(5):st_gcn((gcn):ConvTemporalGraphical((conv):Conv2d(128,384,kernel_size=(1,1),stride=(1,1)))(tcn):Sequential((0):BatchNorm2d(128,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)(1):ReLU(inplace=True)(2):Conv2d(128,128,kernel_size=(3,1),stride=(1,1),padding=(1,0))(3):BatchNorm2d(128,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)(4):Dropout(p=0.3,inplace=True))(relu):ReLU(inplace=True))(6):st_gcn((gcn):ConvTemporalGraphical((conv):Conv2d(128,384,kernel_size=(1,1),stride=(1,1)))(tcn):Sequential((0):BatchNorm2d(128,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)(1):ReLU(inplace=True)(2):Conv2d(128,128,kernel_size=(3,1),stride=(1,1),padding=(1,0))(3):BatchNorm2d(128,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)(4):Dropout(p=0.3,inplace=True))(relu):ReLU(inplace=True))(7):st_gcn((gcn):ConvTemporalGraphical((conv):Conv2d(128,768,kernel_size=(1,1),stride=(1,1)))(tcn):Sequential((0):BatchNorm2d(256,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)(1):ReLU(inplace=True)(2):Conv2d(256,256,kernel_size=(3,1),stride=(2,1),padding=(1,0))(3):BatchNorm2d(256,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)(4):Dropout(p=0.3,inplace=True))(residual):Sequential((0):Conv2d(128,256,kernel_size=(1,1),stride=(2,1))(1):BatchNorm2d(256,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True))(relu):ReLU(inplace=True))(8):st_gcn((gcn):ConvTemporalGraphical((conv):Conv2d(256,768,kernel_size=(1,1),stride=(1,1)))(tcn):Sequential((0):BatchNorm2d(256,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)(1):ReLU(inplace=True)(2):Conv2d(256,256,kernel_size=(3,1),stride=(1,1),padding=(1,0))(3):BatchNorm2d(256,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)(4):Dropout(p=0.3,inplace=True))(relu):ReLU(inplace=True))(9):st_gcn((gcn):ConvTemporalGraphical((conv):Conv2d(256,768,kernel_size=(1,1),stride=(1,1)))(tcn):Sequential((0):BatchNorm2d(256,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)(1):ReLU(inplace=True)(2):Conv2d(256,256,kernel_size=(3,1),stride=(1,1),padding=(1,0))(3):BatchNorm2d(256,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)(4):Dropout(p=0.3,inplace=True))(relu):ReLU(inplace=True)))(fcn):Conv2d(256,1,kernel_size=(1,1),stride=(1,1)))'}\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "num_epochs = 1000\n",
    "learning_rate = 2e-3\n",
    "patience = 800\n",
    "min_improvement = 0.01\n",
    "best_loss_model = (-1, np.inf, None)\n",
    "dropout = 0.3\n",
    "weight_decay = 1e-5\n",
    "\n",
    "batch_size = 16\n",
    "temp_kernel_size = 3\n",
    "model = stgcn.Model(train_dataset.x.shape[2], train_adjacency.shape[0], temp_kernel_size, dropout=dropout).to(device)\n",
    "if num_devices > 1:\n",
    "    model = torch.nn.DataParallel(model, device_ids=list(range(num_devices)))\n",
    "loss_fn = evaluate.NSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "writer = SummaryWriter(comment='STGCN_simulationTraining')\n",
    "param_description = {'time_stamp': time_stamp, 'batch_size': batch_size, 'loss': loss_fn, 'include_month': include_month, 'aggregate_daily': agg, 'rdrs_vars': rdrs_vars, 'dropout': dropout, 'spatial_validation': spatial_val, 'val_fraction': val_fraction, 'temp_kernel': temp_kernel_size,\n",
    "                     'optimizer': optimizer, 'lr': learning_rate, 'patience': patience, 'min_improvement': min_improvement, 'x_train_shape': train_dataset.x.shape, 'x_val_shape': val_dataset.x.shape, 'partitioning_strategy': partitioning_strategy, 'max_hops': max_hops,\n",
    "                     'spatial_x_test_shape': spatial_test_dataset.x.shape if spatial_test_dataset is not None else '', 'temporal_x_test_shape': temporal_test_dataset.x.shape, 'num_epochs': num_epochs, 'seq_len': seq_len, 'seq_steps': seq_steps, 'train_start': train_start, 'train_end': train_end, 'weight_decay': weight_decay, \n",
    "                     'landcover_types': landcover_types, 'test_start': test_start, 'test_end': test_end, 'model': str(model).replace('\\n','').replace(' ', ''),}\n",
    "writer.add_text('Parameter Description', str(param_description))\n",
    "str(param_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if spatial_val:\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=True, pin_memory=True, drop_last=False)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size, shuffle=True, pin_memory=True, drop_last=False)\n",
    "else:\n",
    "    val_indices = np.random.choice(len(train_dataset), size=int(val_fraction * len(train_dataset)), replace=False)\n",
    "    train_indices = list(i for i in range(len(train_dataset)) if i not in val_indices)\n",
    "    train_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\n",
    "    val_sampler = torch.utils.data.SubsetRandomSampler(val_indices)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size, sampler=train_sampler, pin_memory=True, drop_last=False)\n",
    "    val_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size, sampler=val_sampler, pin_memory=True, drop_last=False)\n",
    "    \n",
    "if spatial_test_dataset is not None:\n",
    "    spatial_test_dataloader = torch.utils.data.DataLoader(spatial_test_dataset, batch_size, shuffle=False, pin_memory=True, drop_last=False)\n",
    "temporal_test_dataloader = torch.utils.data.DataLoader(temporal_test_dataset, batch_size, shuffle=False, pin_memory=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subbasins with constant streamflow will divide by zero in loss calculation. Calculate loss without them.\n",
    "train_non_constant_subbasin_mask = ((train_dataset.y_sim.min(dim=0)[0] - train_dataset.y_sim.max(dim=0)[0]) != 0).to(device)\n",
    "val_non_constant_subbasin_mask = ((val_dataset.y_sim.min(dim=0)[0] - val_dataset.y_sim.max(dim=0)[0]) != 0).to(device)\n",
    "\n",
    "y_train_means = train_dataset.y_sim_means.to(device)\n",
    "y_val_means = val_dataset.y_sim_means[val_non_constant_subbasin_mask].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 mean train loss:\t13.929893493652344\n",
      "Epoch 0 mean val loss:\t16.676801681518555\n",
      "Saved model as /home/mgauch/runoff-nn/src/../pickle/models/STGCN_simulationTraining_allStations_20190825-153140.pkl\n",
      "Epoch 1 mean train loss:\t1.6110553741455078\n",
      "Epoch 1 mean val loss:\t2.7570455074310303\n",
      "Saved model as /home/mgauch/runoff-nn/src/../pickle/models/STGCN_simulationTraining_allStations_20190825-153140.pkl\n",
      "Epoch 2 mean train loss:\t1.4047967195510864\n",
      "Epoch 2 mean val loss:\t2.5647623538970947\n",
      "Saved model as /home/mgauch/runoff-nn/src/../pickle/models/STGCN_simulationTraining_allStations_20190825-153140.pkl\n",
      "Epoch 3 mean train loss:\t1.3792766332626343\n",
      "Epoch 3 mean val loss:\t3.135155200958252\n",
      "Epoch 4 mean train loss:\t1.3696426153182983\n",
      "Epoch 4 mean val loss:\t6.130382537841797\n",
      "Epoch 5 mean train loss:\t1.34093177318573\n",
      "Epoch 5 mean val loss:\t5.102980613708496\n",
      "Epoch 6 mean train loss:\t1.2953453063964844\n",
      "Epoch 6 mean val loss:\t2.3612868785858154\n",
      "Saved model as /home/mgauch/runoff-nn/src/../pickle/models/STGCN_simulationTraining_allStations_20190825-153140.pkl\n",
      "Epoch 7 mean train loss:\t1.2954130172729492\n",
      "Epoch 7 mean val loss:\t3.6823525428771973\n",
      "Epoch 8 mean train loss:\t1.2890385389328003\n",
      "Epoch 8 mean val loss:\t5.867376804351807\n",
      "Epoch 9 mean train loss:\t1.2412426471710205\n",
      "Epoch 9 mean val loss:\t2.4051930904388428\n",
      "Epoch 10 mean train loss:\t1.2400288581848145\n",
      "Epoch 10 mean val loss:\t1.6835530996322632\n",
      "Saved model as /home/mgauch/runoff-nn/src/../pickle/models/STGCN_simulationTraining_allStations_20190825-153140.pkl\n",
      "Epoch 11 mean train loss:\t1.195190191268921\n",
      "Epoch 11 mean val loss:\t2.0099384784698486\n",
      "Epoch 12 mean train loss:\t1.1646925210952759\n",
      "Epoch 12 mean val loss:\t4.259929656982422\n",
      "Epoch 13 mean train loss:\t1.1238598823547363\n",
      "Epoch 13 mean val loss:\t1.8159390687942505\n",
      "Epoch 14 mean train loss:\t1.055956244468689\n",
      "Epoch 14 mean val loss:\t2.734361171722412\n",
      "Epoch 15 mean train loss:\t1.0090988874435425\n",
      "Epoch 15 mean val loss:\t1.8346052169799805\n",
      "Epoch 16 mean train loss:\t1.0937353372573853\n",
      "Epoch 16 mean val loss:\t7.628185272216797\n",
      "Epoch 17 mean train loss:\t1.014176368713379\n",
      "Epoch 17 mean val loss:\t4.913206577301025\n",
      "Epoch 18 mean train loss:\t0.9893032312393188\n",
      "Epoch 18 mean val loss:\t2.864964723587036\n",
      "Epoch 19 mean train loss:\t0.9664602875709534\n",
      "Epoch 19 mean val loss:\t2.785846710205078\n",
      "Epoch 20 mean train loss:\t0.9776644110679626\n",
      "Epoch 20 mean val loss:\t3.049280881881714\n",
      "Epoch 21 mean train loss:\t0.8788461089134216\n",
      "Epoch 21 mean val loss:\t3.2515525817871094\n",
      "Epoch 22 mean train loss:\t0.894506573677063\n",
      "Epoch 22 mean val loss:\t2.6298773288726807\n",
      "Epoch 23 mean train loss:\t0.9191783666610718\n",
      "Epoch 23 mean val loss:\t11.240274429321289\n",
      "Epoch 24 mean train loss:\t0.9958800077438354\n",
      "Epoch 24 mean val loss:\t5.5486979484558105\n",
      "Epoch 25 mean train loss:\t0.9815465211868286\n",
      "Epoch 25 mean val loss:\t1.7642961740493774\n",
      "Epoch 26 mean train loss:\t0.9432814121246338\n",
      "Epoch 26 mean val loss:\t4.306992053985596\n",
      "Epoch 27 mean train loss:\t0.9033136367797852\n",
      "Epoch 27 mean val loss:\t1.728509545326233\n",
      "Epoch 28 mean train loss:\t0.9057767987251282\n",
      "Epoch 28 mean val loss:\t2.663886547088623\n",
      "Epoch 29 mean train loss:\t0.9063110947608948\n",
      "Epoch 29 mean val loss:\t2.348095655441284\n",
      "Epoch 30 mean train loss:\t0.8548476696014404\n",
      "Epoch 30 mean val loss:\t2.9235317707061768\n",
      "Epoch 31 mean train loss:\t0.9339957237243652\n",
      "Epoch 31 mean val loss:\t2.8870058059692383\n",
      "Epoch 32 mean train loss:\t0.9742735624313354\n",
      "Epoch 32 mean val loss:\t38.642765045166016\n",
      "Epoch 33 mean train loss:\t1.0397558212280273\n",
      "Epoch 33 mean val loss:\t8.028003692626953\n",
      "Epoch 34 mean train loss:\t0.926638126373291\n",
      "Epoch 34 mean val loss:\t6.286419868469238\n",
      "Epoch 35 mean train loss:\t0.854949951171875\n",
      "Epoch 35 mean val loss:\t4.391540050506592\n",
      "Epoch 36 mean train loss:\t0.8919488787651062\n",
      "Epoch 36 mean val loss:\t3.171346664428711\n",
      "Epoch 37 mean train loss:\t0.8771434426307678\n",
      "Epoch 37 mean val loss:\t5.045874118804932\n",
      "Epoch 38 mean train loss:\t0.8503639101982117\n",
      "Epoch 38 mean val loss:\t7.123290061950684\n",
      "Epoch 39 mean train loss:\t0.7948207259178162\n",
      "Epoch 39 mean val loss:\t3.704369306564331\n",
      "Epoch 40 mean train loss:\t0.8176811933517456\n",
      "Epoch 40 mean val loss:\t2.514535665512085\n",
      "Epoch 41 mean train loss:\t0.8132704496383667\n",
      "Epoch 41 mean val loss:\t5.657931804656982\n",
      "Epoch 42 mean train loss:\t0.778724193572998\n",
      "Epoch 42 mean val loss:\t4.874266147613525\n",
      "Epoch 43 mean train loss:\t0.8359606266021729\n",
      "Epoch 43 mean val loss:\t2.8638529777526855\n",
      "Epoch 44 mean train loss:\t0.7956873774528503\n",
      "Epoch 44 mean val loss:\t10.533517837524414\n",
      "Epoch 45 mean train loss:\t0.8131505250930786\n",
      "Epoch 45 mean val loss:\t5.976005554199219\n",
      "Epoch 46 mean train loss:\t0.7656241059303284\n",
      "Epoch 46 mean val loss:\t4.207275390625\n",
      "Epoch 47 mean train loss:\t0.7802140712738037\n",
      "Epoch 47 mean val loss:\t5.794417381286621\n",
      "Epoch 48 mean train loss:\t0.80586177110672\n",
      "Epoch 48 mean val loss:\t9.679800987243652\n",
      "Epoch 49 mean train loss:\t0.7802280783653259\n",
      "Epoch 49 mean val loss:\t11.762167930603027\n",
      "Epoch 50 mean train loss:\t0.7754114270210266\n",
      "Epoch 50 mean val loss:\t6.908514022827148\n",
      "Epoch 51 mean train loss:\t0.8155989646911621\n",
      "Epoch 51 mean val loss:\t14.218971252441406\n",
      "Epoch 52 mean train loss:\t0.8071161508560181\n",
      "Epoch 52 mean val loss:\t6.748128890991211\n",
      "Epoch 53 mean train loss:\t0.8031923770904541\n",
      "Epoch 53 mean val loss:\t2.529111862182617\n",
      "Epoch 54 mean train loss:\t0.758892297744751\n",
      "Epoch 54 mean val loss:\t9.878089904785156\n",
      "Epoch 55 mean train loss:\t0.7846185564994812\n",
      "Epoch 55 mean val loss:\t6.845884323120117\n",
      "Epoch 56 mean train loss:\t0.741971492767334\n",
      "Epoch 56 mean val loss:\t4.870120525360107\n",
      "Epoch 57 mean train loss:\t0.8646138310432434\n",
      "Epoch 57 mean val loss:\t10.489745140075684\n",
      "Epoch 58 mean train loss:\t0.7914576530456543\n",
      "Epoch 58 mean val loss:\t7.613250732421875\n",
      "Epoch 59 mean train loss:\t0.765889048576355\n",
      "Epoch 59 mean val loss:\t13.359428405761719\n",
      "Epoch 60 mean train loss:\t0.7325378656387329\n",
      "Epoch 60 mean val loss:\t11.512303352355957\n",
      "Epoch 61 mean train loss:\t0.7342877984046936\n",
      "Epoch 61 mean val loss:\t12.791075706481934\n",
      "Epoch 62 mean train loss:\t0.7162830829620361\n",
      "Epoch 62 mean val loss:\t7.177095890045166\n",
      "Epoch 63 mean train loss:\t0.7340059280395508\n",
      "Epoch 63 mean val loss:\t6.832277774810791\n",
      "Epoch 64 mean train loss:\t0.7055653929710388\n",
      "Epoch 64 mean val loss:\t8.687399864196777\n",
      "Epoch 65 mean train loss:\t0.6978231072425842\n",
      "Epoch 65 mean val loss:\t8.261698722839355\n",
      "Epoch 66 mean train loss:\t0.7292962074279785\n",
      "Epoch 66 mean val loss:\t10.687539100646973\n",
      "Epoch 67 mean train loss:\t0.6995921730995178\n",
      "Epoch 67 mean val loss:\t31.969701766967773\n",
      "Epoch 68 mean train loss:\t0.7700480818748474\n",
      "Epoch 68 mean val loss:\t18.58738899230957\n",
      "Epoch 69 mean train loss:\t0.7172306776046753\n",
      "Epoch 69 mean val loss:\t17.199037551879883\n",
      "Epoch 70 mean train loss:\t0.6923425197601318\n",
      "Epoch 70 mean val loss:\t27.69846534729004\n",
      "Epoch 71 mean train loss:\t0.9989262819290161\n",
      "Epoch 71 mean val loss:\t9878.5595703125\n",
      "Epoch 72 mean train loss:\t3.9061548709869385\n",
      "Epoch 72 mean val loss:\t9.121785163879395\n",
      "Epoch 73 mean train loss:\t1.3234717845916748\n",
      "Epoch 73 mean val loss:\t9.739617347717285\n",
      "Epoch 74 mean train loss:\t1.3255836963653564\n",
      "Epoch 74 mean val loss:\t5.117015838623047\n",
      "Epoch 75 mean train loss:\t1.2082490921020508\n",
      "Epoch 75 mean val loss:\t6.070949554443359\n",
      "Epoch 76 mean train loss:\t1.205582618713379\n",
      "Epoch 76 mean val loss:\t6.676068305969238\n",
      "Epoch 77 mean train loss:\t1.199418067932129\n",
      "Epoch 77 mean val loss:\t5.934917449951172\n",
      "Epoch 78 mean train loss:\t1.129009485244751\n",
      "Epoch 78 mean val loss:\t11.039047241210938\n",
      "Epoch 79 mean train loss:\t1.10103178024292\n",
      "Epoch 79 mean val loss:\t11.403944969177246\n",
      "Epoch 80 mean train loss:\t1.0507838726043701\n",
      "Epoch 80 mean val loss:\t10.5816068649292\n",
      "Epoch 81 mean train loss:\t1.046825885772705\n",
      "Epoch 81 mean val loss:\t12.603453636169434\n",
      "Epoch 82 mean train loss:\t0.9749367833137512\n",
      "Epoch 82 mean val loss:\t16.482219696044922\n",
      "Epoch 83 mean train loss:\t0.9836993217468262\n",
      "Epoch 83 mean val loss:\t20.548019409179688\n",
      "Epoch 84 mean train loss:\t0.9861229062080383\n",
      "Epoch 84 mean val loss:\t13.387384414672852\n",
      "Epoch 85 mean train loss:\t0.9895806908607483\n",
      "Epoch 85 mean val loss:\t26.113447189331055\n",
      "Epoch 86 mean train loss:\t0.9487908482551575\n",
      "Epoch 86 mean val loss:\t14.694426536560059\n",
      "Epoch 87 mean train loss:\t0.9516536593437195\n",
      "Epoch 87 mean val loss:\t18.489160537719727\n",
      "Epoch 88 mean train loss:\t0.9569748044013977\n",
      "Epoch 88 mean val loss:\t19.327606201171875\n",
      "Epoch 89 mean train loss:\t0.9130528569221497\n",
      "Epoch 89 mean val loss:\t22.642810821533203\n",
      "Epoch 90 mean train loss:\t0.8822634220123291\n",
      "Epoch 90 mean val loss:\t13.897411346435547\n",
      "Epoch 91 mean train loss:\t0.8818188905715942\n",
      "Epoch 91 mean val loss:\t8.390345573425293\n",
      "Epoch 92 mean train loss:\t0.8728989958763123\n",
      "Epoch 92 mean val loss:\t21.83998680114746\n",
      "Epoch 93 mean train loss:\t0.8960504531860352\n",
      "Epoch 93 mean val loss:\t7.570620536804199\n",
      "Epoch 94 mean train loss:\t0.8789086937904358\n",
      "Epoch 94 mean val loss:\t19.04710578918457\n",
      "Epoch 95 mean train loss:\t0.8910132646560669\n",
      "Epoch 95 mean val loss:\t18.640901565551758\n",
      "Epoch 96 mean train loss:\t2.652921676635742\n",
      "Epoch 96 mean val loss:\t1516.147216796875\n",
      "Epoch 97 mean train loss:\t1.2971687316894531\n",
      "Epoch 97 mean val loss:\t76.400146484375\n",
      "Epoch 98 mean train loss:\t1.2053776979446411\n",
      "Epoch 98 mean val loss:\t97.69512939453125\n",
      "Epoch 99 mean train loss:\t1.176106572151184\n",
      "Epoch 99 mean val loss:\t103.59867858886719\n",
      "Epoch 100 mean train loss:\t1.1292545795440674\n",
      "Epoch 100 mean val loss:\t89.59716033935547\n",
      "Epoch 101 mean train loss:\t1.1004005670547485\n",
      "Epoch 101 mean val loss:\t36.31875228881836\n",
      "Epoch 102 mean train loss:\t1.0470956563949585\n",
      "Epoch 102 mean val loss:\t73.78434753417969\n",
      "Epoch 103 mean train loss:\t1.0224297046661377\n",
      "Epoch 103 mean val loss:\t41.96339797973633\n",
      "Epoch 104 mean train loss:\t1.0103662014007568\n",
      "Epoch 104 mean val loss:\t52.217769622802734\n",
      "Epoch 105 mean train loss:\t1.0123159885406494\n",
      "Epoch 105 mean val loss:\t57.445716857910156\n",
      "Epoch 106 mean train loss:\t0.9818568229675293\n",
      "Epoch 106 mean val loss:\t37.91095733642578\n",
      "Epoch 107 mean train loss:\t0.9759101867675781\n",
      "Epoch 107 mean val loss:\t66.14217376708984\n",
      "Epoch 108 mean train loss:\t0.9836561679840088\n",
      "Epoch 108 mean val loss:\t35.47323226928711\n",
      "Epoch 109 mean train loss:\t0.9474154114723206\n",
      "Epoch 109 mean val loss:\t23.076675415039062\n",
      "Epoch 110 mean train loss:\t0.9577262997627258\n",
      "Epoch 110 mean val loss:\t55.35872268676758\n",
      "Epoch 111 mean train loss:\t0.9353076815605164\n",
      "Epoch 111 mean val loss:\t35.131072998046875\n",
      "Epoch 112 mean train loss:\t0.9206161499023438\n",
      "Epoch 112 mean val loss:\t44.7252311706543\n",
      "Epoch 113 mean train loss:\t0.9653130769729614\n",
      "Epoch 113 mean val loss:\t35.66224670410156\n",
      "Epoch 114 mean train loss:\t0.9784801602363586\n",
      "Epoch 114 mean val loss:\t69.00406646728516\n",
      "Epoch 115 mean train loss:\t0.9187626242637634\n",
      "Epoch 115 mean val loss:\t35.81869125366211\n",
      "Epoch 116 mean train loss:\t0.932929277420044\n",
      "Epoch 116 mean val loss:\t26.777931213378906\n",
      "Epoch 117 mean train loss:\t0.9230524897575378\n",
      "Epoch 117 mean val loss:\t70.96524047851562\n",
      "Epoch 118 mean train loss:\t0.9747478365898132\n",
      "Epoch 118 mean val loss:\t20.605140686035156\n",
      "Epoch 119 mean train loss:\t0.9468666315078735\n",
      "Epoch 119 mean val loss:\t6.886135101318359\n",
      "Epoch 120 mean train loss:\t0.9117989540100098\n",
      "Epoch 120 mean val loss:\t46.04804992675781\n",
      "Epoch 121 mean train loss:\t0.9285804033279419\n",
      "Epoch 121 mean val loss:\t68.64735412597656\n",
      "Epoch 122 mean train loss:\t0.8944439888000488\n",
      "Epoch 122 mean val loss:\t22.052839279174805\n",
      "Epoch 123 mean train loss:\t0.908202588558197\n",
      "Epoch 123 mean val loss:\t28.34815788269043\n",
      "Epoch 124 mean train loss:\t0.8900573253631592\n",
      "Epoch 124 mean val loss:\t47.33546447753906\n",
      "Epoch 125 mean train loss:\t0.8719423413276672\n",
      "Epoch 125 mean val loss:\t55.264122009277344\n",
      "Epoch 126 mean train loss:\t0.9026694893836975\n",
      "Epoch 126 mean val loss:\t40.637325286865234\n",
      "Epoch 127 mean train loss:\t0.8911935091018677\n",
      "Epoch 127 mean val loss:\t32.509952545166016\n",
      "Epoch 128 mean train loss:\t0.8673175573348999\n",
      "Epoch 128 mean val loss:\t46.290733337402344\n",
      "Epoch 129 mean train loss:\t0.8710624575614929\n",
      "Epoch 129 mean val loss:\t38.139137268066406\n",
      "Epoch 130 mean train loss:\t0.8810508847236633\n",
      "Epoch 130 mean val loss:\t45.14623260498047\n",
      "Epoch 131 mean train loss:\t0.849108099937439\n",
      "Epoch 131 mean val loss:\t60.45719528198242\n",
      "Epoch 132 mean train loss:\t0.8392239212989807\n",
      "Epoch 132 mean val loss:\t50.257423400878906\n",
      "Epoch 133 mean train loss:\t0.8441683053970337\n",
      "Epoch 133 mean val loss:\t27.5140438079834\n",
      "Epoch 134 mean train loss:\t0.8329489231109619\n",
      "Epoch 134 mean val loss:\t82.76661682128906\n",
      "Epoch 135 mean train loss:\t0.8134759664535522\n",
      "Epoch 135 mean val loss:\t72.05789947509766\n",
      "Epoch 136 mean train loss:\t0.8376114964485168\n",
      "Epoch 136 mean val loss:\t107.15191650390625\n",
      "Epoch 137 mean train loss:\t0.8097278475761414\n",
      "Epoch 137 mean val loss:\t49.09394836425781\n",
      "Epoch 138 mean train loss:\t0.8181281089782715\n",
      "Epoch 138 mean val loss:\t66.89546203613281\n",
      "Epoch 139 mean train loss:\t0.8240461349487305\n",
      "Epoch 139 mean val loss:\t108.22196197509766\n",
      "Epoch 140 mean train loss:\t0.8364623188972473\n",
      "Epoch 140 mean val loss:\t62.50364685058594\n",
      "Epoch 141 mean train loss:\t0.804121732711792\n",
      "Epoch 141 mean val loss:\t63.591209411621094\n",
      "Epoch 142 mean train loss:\t0.7888425588607788\n",
      "Epoch 142 mean val loss:\t66.90936279296875\n",
      "Epoch 143 mean train loss:\t0.8352776169776917\n",
      "Epoch 143 mean val loss:\t65.11911010742188\n",
      "Epoch 144 mean train loss:\t0.8220956325531006\n",
      "Epoch 144 mean val loss:\t95.25846099853516\n",
      "Epoch 145 mean train loss:\t0.8681279420852661\n",
      "Epoch 145 mean val loss:\t36.75938415527344\n",
      "Epoch 146 mean train loss:\t0.831439733505249\n",
      "Epoch 146 mean val loss:\t35.105735778808594\n",
      "Epoch 147 mean train loss:\t0.8084914684295654\n",
      "Epoch 147 mean val loss:\t42.513580322265625\n",
      "Epoch 148 mean train loss:\t0.7704926133155823\n",
      "Epoch 148 mean val loss:\t72.07733154296875\n",
      "Epoch 149 mean train loss:\t0.7897129058837891\n",
      "Epoch 149 mean val loss:\t52.33582305908203\n",
      "Epoch 150 mean train loss:\t0.757206916809082\n",
      "Epoch 150 mean val loss:\t65.75855255126953\n",
      "Epoch 151 mean train loss:\t0.7588698863983154\n",
      "Epoch 151 mean val loss:\t78.78570556640625\n",
      "Epoch 152 mean train loss:\t0.7768148183822632\n",
      "Epoch 152 mean val loss:\t56.0847282409668\n",
      "Epoch 153 mean train loss:\t0.7835450172424316\n",
      "Epoch 153 mean val loss:\t103.44847106933594\n",
      "Epoch 154 mean train loss:\t0.8034796118736267\n",
      "Epoch 154 mean val loss:\t119.72323608398438\n",
      "Epoch 155 mean train loss:\t0.7572887539863586\n",
      "Epoch 155 mean val loss:\t66.02207946777344\n",
      "Epoch 156 mean train loss:\t0.8027952313423157\n",
      "Epoch 156 mean val loss:\t109.14982604980469\n",
      "Epoch 157 mean train loss:\t0.7351682782173157\n",
      "Epoch 157 mean val loss:\t57.09343719482422\n",
      "Epoch 158 mean train loss:\t0.7446210384368896\n",
      "Epoch 158 mean val loss:\t85.06379699707031\n",
      "Epoch 159 mean train loss:\t0.722804069519043\n",
      "Epoch 159 mean val loss:\t73.52527618408203\n",
      "Epoch 160 mean train loss:\t0.7123993635177612\n",
      "Epoch 160 mean val loss:\t114.91974639892578\n",
      "Epoch 161 mean train loss:\t0.726196825504303\n",
      "Epoch 161 mean val loss:\t84.71839141845703\n",
      "Epoch 162 mean train loss:\t0.7091237902641296\n",
      "Epoch 162 mean val loss:\t151.62916564941406\n",
      "Epoch 163 mean train loss:\t0.7093251347541809\n",
      "Epoch 163 mean val loss:\t185.5093994140625\n",
      "Epoch 164 mean train loss:\t0.7463126182556152\n",
      "Epoch 164 mean val loss:\t101.81871032714844\n",
      "Epoch 165 mean train loss:\t0.7206727266311646\n",
      "Epoch 165 mean val loss:\t86.69023132324219\n",
      "Epoch 166 mean train loss:\t0.723395586013794\n",
      "Epoch 166 mean val loss:\t228.2987823486328\n",
      "Epoch 167 mean train loss:\t0.7189360857009888\n",
      "Epoch 167 mean val loss:\t103.55168914794922\n",
      "Epoch 168 mean train loss:\t0.7136940956115723\n",
      "Epoch 168 mean val loss:\t84.15245819091797\n",
      "Epoch 169 mean train loss:\t0.7403425574302673\n",
      "Epoch 169 mean val loss:\t96.25977325439453\n",
      "Epoch 170 mean train loss:\t0.6879547834396362\n",
      "Epoch 170 mean val loss:\t89.97074127197266\n",
      "Epoch 171 mean train loss:\t0.6894742250442505\n",
      "Epoch 171 mean val loss:\t128.24436950683594\n",
      "Epoch 172 mean train loss:\t0.6598593592643738\n",
      "Epoch 172 mean val loss:\t188.6959228515625\n",
      "Epoch 173 mean train loss:\t0.6918990015983582\n",
      "Epoch 173 mean val loss:\t218.97727966308594\n",
      "Epoch 174 mean train loss:\t0.6804978251457214\n",
      "Epoch 174 mean val loss:\t209.25120544433594\n",
      "Epoch 175 mean train loss:\t0.7538554668426514\n",
      "Epoch 175 mean val loss:\t120.51321411132812\n",
      "Epoch 176 mean train loss:\t0.7130343317985535\n",
      "Epoch 176 mean val loss:\t88.86257934570312\n",
      "Epoch 177 mean train loss:\t0.7160664200782776\n",
      "Epoch 177 mean val loss:\t120.5677719116211\n",
      "Epoch 178 mean train loss:\t0.6474564075469971\n",
      "Epoch 178 mean val loss:\t299.78125\n",
      "Epoch 179 mean train loss:\t0.6758056282997131\n",
      "Epoch 179 mean val loss:\t145.74945068359375\n",
      "Epoch 180 mean train loss:\t0.6818196177482605\n",
      "Epoch 180 mean val loss:\t422.1593933105469\n",
      "Epoch 181 mean train loss:\t0.7057071924209595\n",
      "Epoch 181 mean val loss:\t65.74275207519531\n",
      "Epoch 182 mean train loss:\t0.6623042225837708\n",
      "Epoch 182 mean val loss:\t95.66389465332031\n",
      "Epoch 183 mean train loss:\t0.6638428568840027\n",
      "Epoch 183 mean val loss:\t470.3126220703125\n",
      "Epoch 184 mean train loss:\t0.656118631362915\n",
      "Epoch 184 mean val loss:\t291.5509948730469\n",
      "Epoch 185 mean train loss:\t0.6608940362930298\n",
      "Epoch 185 mean val loss:\t252.1023406982422\n",
      "Epoch 186 mean train loss:\t0.6630963087081909\n",
      "Epoch 186 mean val loss:\t414.383544921875\n",
      "Epoch 187 mean train loss:\t0.6267754435539246\n",
      "Epoch 187 mean val loss:\t180.66107177734375\n",
      "Epoch 188 mean train loss:\t0.9518524408340454\n",
      "Epoch 188 mean val loss:\t73.11782836914062\n",
      "Epoch 189 mean train loss:\t0.7913576364517212\n",
      "Epoch 189 mean val loss:\t52.3922004699707\n",
      "Epoch 190 mean train loss:\t0.7077805399894714\n",
      "Epoch 190 mean val loss:\t222.21437072753906\n",
      "Epoch 191 mean train loss:\t0.7053800225257874\n",
      "Epoch 191 mean val loss:\t339.8295593261719\n",
      "Epoch 192 mean train loss:\t0.7367053031921387\n",
      "Epoch 192 mean val loss:\t474.8354187011719\n",
      "Epoch 193 mean train loss:\t0.680662214756012\n",
      "Epoch 193 mean val loss:\t362.5610656738281\n",
      "Epoch 194 mean train loss:\t0.6721503734588623\n",
      "Epoch 194 mean val loss:\t348.84033203125\n",
      "Epoch 195 mean train loss:\t0.6766343116760254\n",
      "Epoch 195 mean val loss:\t161.40049743652344\n",
      "Epoch 196 mean train loss:\t0.6523057222366333\n",
      "Epoch 196 mean val loss:\t340.1311950683594\n",
      "Epoch 197 mean train loss:\t0.6500526666641235\n",
      "Epoch 197 mean val loss:\t126.80162811279297\n",
      "Epoch 198 mean train loss:\t0.640048086643219\n",
      "Epoch 198 mean val loss:\t393.4742431640625\n",
      "Epoch 199 mean train loss:\t0.617053747177124\n",
      "Epoch 199 mean val loss:\t522.4276123046875\n",
      "Epoch 200 mean train loss:\t0.6171736717224121\n",
      "Epoch 200 mean val loss:\t288.5493469238281\n",
      "Epoch 201 mean train loss:\t0.5769559144973755\n",
      "Epoch 201 mean val loss:\t461.51708984375\n",
      "Epoch 202 mean train loss:\t0.6214122176170349\n",
      "Epoch 202 mean val loss:\t411.1999816894531\n",
      "Epoch 203 mean train loss:\t0.5919916033744812\n",
      "Epoch 203 mean val loss:\t801.7894287109375\n",
      "Epoch 204 mean train loss:\t0.6235883831977844\n",
      "Epoch 204 mean val loss:\t639.5733642578125\n",
      "Epoch 205 mean train loss:\t0.5825247168540955\n",
      "Epoch 205 mean val loss:\t605.0704345703125\n",
      "Epoch 206 mean train loss:\t0.6347023248672485\n",
      "Epoch 206 mean val loss:\t496.1234436035156\n",
      "Epoch 207 mean train loss:\t0.6009649634361267\n",
      "Epoch 207 mean val loss:\t761.7227783203125\n",
      "Epoch 208 mean train loss:\t0.592498779296875\n",
      "Epoch 208 mean val loss:\t1032.323974609375\n",
      "Epoch 209 mean train loss:\t0.5509224534034729\n",
      "Epoch 209 mean val loss:\t696.9647216796875\n",
      "Epoch 210 mean train loss:\t0.5327048301696777\n",
      "Epoch 210 mean val loss:\t869.5985107421875\n",
      "Epoch 211 mean train loss:\t0.5136616230010986\n",
      "Epoch 211 mean val loss:\t1152.6331787109375\n",
      "Epoch 212 mean train loss:\t0.5132891535758972\n",
      "Epoch 212 mean val loss:\t1094.826171875\n",
      "Epoch 213 mean train loss:\t0.524161159992218\n",
      "Epoch 213 mean val loss:\t908.87841796875\n",
      "Epoch 214 mean train loss:\t0.5055047273635864\n",
      "Epoch 214 mean val loss:\t469.7822265625\n",
      "Epoch 215 mean train loss:\t0.5203763246536255\n",
      "Epoch 215 mean val loss:\t1035.2176513671875\n",
      "Epoch 216 mean train loss:\t0.5070538520812988\n",
      "Epoch 216 mean val loss:\t715.8067016601562\n",
      "Epoch 217 mean train loss:\t0.5264352560043335\n",
      "Epoch 217 mean val loss:\t846.84814453125\n",
      "Epoch 218 mean train loss:\t0.5199338793754578\n",
      "Epoch 218 mean val loss:\t1604.3062744140625\n",
      "Epoch 219 mean train loss:\t0.5014680027961731\n",
      "Epoch 219 mean val loss:\t307.7962341308594\n",
      "Epoch 220 mean train loss:\t0.5371121764183044\n",
      "Epoch 220 mean val loss:\t2432.028564453125\n",
      "Epoch 221 mean train loss:\t0.4712463319301605\n",
      "Epoch 221 mean val loss:\t1996.5791015625\n",
      "Epoch 222 mean train loss:\t0.4877997040748596\n",
      "Epoch 222 mean val loss:\t2198.934814453125\n",
      "Epoch 223 mean train loss:\t0.4836416244506836\n",
      "Epoch 223 mean val loss:\t650.1797485351562\n",
      "Epoch 224 mean train loss:\t0.4724936783313751\n",
      "Epoch 224 mean val loss:\t1361.3497314453125\n",
      "Epoch 225 mean train loss:\t0.48197388648986816\n",
      "Epoch 225 mean val loss:\t787.9956665039062\n",
      "Epoch 226 mean train loss:\t0.4856582581996918\n",
      "Epoch 226 mean val loss:\t1945.336181640625\n",
      "Epoch 227 mean train loss:\t0.43602198362350464\n",
      "Epoch 227 mean val loss:\t1456.6915283203125\n",
      "Epoch 228 mean train loss:\t0.4560594856739044\n",
      "Epoch 228 mean val loss:\t2260.22412109375\n",
      "Epoch 229 mean train loss:\t0.6474924683570862\n",
      "Epoch 229 mean val loss:\t1466.09423828125\n",
      "Epoch 230 mean train loss:\t0.4980138838291168\n",
      "Epoch 230 mean val loss:\t3619.48828125\n",
      "Epoch 231 mean train loss:\t0.5177971720695496\n",
      "Epoch 231 mean val loss:\t1526.0863037109375\n",
      "Epoch 232 mean train loss:\t0.4491848945617676\n",
      "Epoch 232 mean val loss:\t2972.9384765625\n",
      "Epoch 233 mean train loss:\t0.46355289220809937\n",
      "Epoch 233 mean val loss:\t1726.51953125\n",
      "Epoch 234 mean train loss:\t0.4780042767524719\n",
      "Epoch 234 mean val loss:\t5049.708984375\n",
      "Epoch 235 mean train loss:\t0.4476846158504486\n",
      "Epoch 235 mean val loss:\t1843.3232421875\n",
      "Epoch 236 mean train loss:\t0.4502970278263092\n",
      "Epoch 236 mean val loss:\t1873.4613037109375\n",
      "Epoch 237 mean train loss:\t0.4736381471157074\n",
      "Epoch 237 mean val loss:\t2531.31201171875\n",
      "Epoch 238 mean train loss:\t0.44985103607177734\n",
      "Epoch 238 mean val loss:\t3698.68798828125\n",
      "Epoch 239 mean train loss:\t0.43575069308280945\n",
      "Epoch 239 mean val loss:\t2252.811767578125\n",
      "Epoch 240 mean train loss:\t0.4296378791332245\n",
      "Epoch 240 mean val loss:\t3074.351318359375\n",
      "Epoch 241 mean train loss:\t0.42604508996009827\n",
      "Epoch 241 mean val loss:\t2951.27783203125\n",
      "Epoch 242 mean train loss:\t0.42287468910217285\n",
      "Epoch 242 mean val loss:\t7509.22265625\n",
      "Epoch 243 mean train loss:\t0.4123258590698242\n",
      "Epoch 243 mean val loss:\t3867.7666015625\n",
      "Epoch 244 mean train loss:\t0.37727850675582886\n",
      "Epoch 244 mean val loss:\t3657.692138671875\n",
      "Epoch 245 mean train loss:\t0.3883335292339325\n",
      "Epoch 245 mean val loss:\t5046.37255859375\n",
      "Epoch 246 mean train loss:\t0.4231472611427307\n",
      "Epoch 246 mean val loss:\t5262.90087890625\n",
      "Epoch 247 mean train loss:\t0.41055822372436523\n",
      "Epoch 247 mean val loss:\t3024.162109375\n",
      "Epoch 248 mean train loss:\t0.3959503769874573\n",
      "Epoch 248 mean val loss:\t5778.421875\n",
      "Epoch 249 mean train loss:\t0.40051859617233276\n",
      "Epoch 249 mean val loss:\t3748.419677734375\n",
      "Epoch 250 mean train loss:\t0.4170164465904236\n",
      "Epoch 250 mean val loss:\t5420.47998046875\n",
      "Epoch 251 mean train loss:\t0.3820216655731201\n",
      "Epoch 251 mean val loss:\t2520.516357421875\n",
      "Epoch 252 mean train loss:\t0.4339274764060974\n",
      "Epoch 252 mean val loss:\t3875.911376953125\n",
      "Epoch 253 mean train loss:\t0.4121190011501312\n",
      "Epoch 253 mean val loss:\t2071.587158203125\n",
      "Epoch 254 mean train loss:\t0.3968334197998047\n",
      "Epoch 254 mean val loss:\t2115.376708984375\n",
      "Epoch 255 mean train loss:\t0.3931177854537964\n",
      "Epoch 255 mean val loss:\t8034.36572265625\n",
      "Epoch 256 mean train loss:\t0.36809641122817993\n",
      "Epoch 256 mean val loss:\t3918.1396484375\n",
      "Epoch 257 mean train loss:\t0.36210134625434875\n",
      "Epoch 257 mean val loss:\t1889.1328125\n",
      "Epoch 258 mean train loss:\t0.36396002769470215\n",
      "Epoch 258 mean val loss:\t2096.91455078125\n",
      "Epoch 259 mean train loss:\t0.3706929683685303\n",
      "Epoch 259 mean val loss:\t2350.274658203125\n",
      "Epoch 260 mean train loss:\t0.357674241065979\n",
      "Epoch 260 mean val loss:\t1586.2998046875\n",
      "Epoch 261 mean train loss:\t0.45944342017173767\n",
      "Epoch 261 mean val loss:\t7093.2197265625\n",
      "Epoch 262 mean train loss:\t0.4141070246696472\n",
      "Epoch 262 mean val loss:\t945.2646484375\n",
      "Epoch 263 mean train loss:\t0.39941978454589844\n",
      "Epoch 263 mean val loss:\t3161.0615234375\n",
      "Epoch 264 mean train loss:\t0.38834619522094727\n",
      "Epoch 264 mean val loss:\t3562.90771484375\n",
      "Epoch 265 mean train loss:\t0.36601632833480835\n",
      "Epoch 265 mean val loss:\t2008.08251953125\n",
      "Epoch 266 mean train loss:\t0.36830365657806396\n",
      "Epoch 266 mean val loss:\t3898.747314453125\n",
      "Epoch 267 mean train loss:\t0.32970261573791504\n",
      "Epoch 267 mean val loss:\t6903.29052734375\n",
      "Epoch 268 mean train loss:\t0.33995118737220764\n",
      "Epoch 268 mean val loss:\t3147.36669921875\n",
      "Epoch 269 mean train loss:\t0.37436458468437195\n",
      "Epoch 269 mean val loss:\t570.2957153320312\n",
      "Epoch 270 mean train loss:\t0.38687142729759216\n",
      "Epoch 270 mean val loss:\t2929.119384765625\n",
      "Epoch 271 mean train loss:\t0.36438480019569397\n",
      "Epoch 271 mean val loss:\t2551.8173828125\n",
      "Epoch 272 mean train loss:\t0.41239097714424133\n",
      "Epoch 272 mean val loss:\t2096.42431640625\n",
      "Epoch 273 mean train loss:\t0.35608553886413574\n",
      "Epoch 273 mean val loss:\t2913.8427734375\n",
      "Epoch 274 mean train loss:\t0.33673474192619324\n",
      "Epoch 274 mean val loss:\t3685.6083984375\n",
      "Epoch 275 mean train loss:\t0.36999818682670593\n",
      "Epoch 275 mean val loss:\t2589.663330078125\n",
      "Epoch 276 mean train loss:\t0.3277513384819031\n",
      "Epoch 276 mean val loss:\t6368.16064453125\n",
      "Epoch 277 mean train loss:\t0.36265936493873596\n",
      "Epoch 277 mean val loss:\t4916.4716796875\n",
      "Epoch 278 mean train loss:\t0.3171611726284027\n",
      "Epoch 278 mean val loss:\t3848.205810546875\n",
      "Epoch 279 mean train loss:\t0.3598770797252655\n",
      "Epoch 279 mean val loss:\t6380.7392578125\n",
      "Epoch 280 mean train loss:\t0.34564441442489624\n",
      "Epoch 280 mean val loss:\t1649.32373046875\n",
      "Epoch 281 mean train loss:\t0.47410258650779724\n",
      "Epoch 281 mean val loss:\t515.3900146484375\n",
      "Epoch 282 mean train loss:\t0.3599708080291748\n",
      "Epoch 282 mean val loss:\t712.4244384765625\n",
      "Epoch 283 mean train loss:\t0.35127148032188416\n",
      "Epoch 283 mean val loss:\t1609.4326171875\n",
      "Epoch 284 mean train loss:\t0.4020880162715912\n",
      "Epoch 284 mean val loss:\t1742.674072265625\n",
      "Epoch 285 mean train loss:\t0.3856353759765625\n",
      "Epoch 285 mean val loss:\t1372.04541015625\n",
      "Epoch 286 mean train loss:\t0.33412396907806396\n",
      "Epoch 286 mean val loss:\t4354.20556640625\n",
      "Epoch 287 mean train loss:\t0.3131028711795807\n",
      "Epoch 287 mean val loss:\t1155.3233642578125\n",
      "Epoch 288 mean train loss:\t0.32959413528442383\n",
      "Epoch 288 mean val loss:\t1347.1646728515625\n",
      "Epoch 289 mean train loss:\t0.3497377634048462\n",
      "Epoch 289 mean val loss:\t2790.656494140625\n",
      "Epoch 290 mean train loss:\t0.36920374631881714\n",
      "Epoch 290 mean val loss:\t1548.5206298828125\n",
      "Epoch 291 mean train loss:\t0.35923874378204346\n",
      "Epoch 291 mean val loss:\t2309.510498046875\n",
      "Epoch 292 mean train loss:\t0.3229966163635254\n",
      "Epoch 292 mean val loss:\t1859.721435546875\n",
      "Epoch 293 mean train loss:\t0.3291541635990143\n",
      "Epoch 293 mean val loss:\t10679.6923828125\n",
      "Epoch 294 mean train loss:\t0.32994723320007324\n",
      "Epoch 294 mean val loss:\t1963.3851318359375\n",
      "Epoch 295 mean train loss:\t0.330419659614563\n",
      "Epoch 295 mean val loss:\t7512.0166015625\n",
      "Epoch 296 mean train loss:\t0.30465376377105713\n",
      "Epoch 296 mean val loss:\t4330.640625\n",
      "Epoch 297 mean train loss:\t0.31320998072624207\n",
      "Epoch 297 mean val loss:\t2125.224609375\n",
      "Epoch 298 mean train loss:\t0.3272177577018738\n",
      "Epoch 298 mean val loss:\t1036.3245849609375\n",
      "Epoch 299 mean train loss:\t0.345514714717865\n",
      "Epoch 299 mean val loss:\t1766.7694091796875\n",
      "Epoch 300 mean train loss:\t0.3182426989078522\n",
      "Epoch 300 mean val loss:\t3050.339599609375\n",
      "Epoch 301 mean train loss:\t0.32082411646842957\n",
      "Epoch 301 mean val loss:\t2921.689453125\n",
      "Epoch 302 mean train loss:\t0.28967875242233276\n",
      "Epoch 302 mean val loss:\t1193.8516845703125\n",
      "Epoch 303 mean train loss:\t0.27859634160995483\n",
      "Epoch 303 mean val loss:\t3818.041015625\n",
      "Epoch 304 mean train loss:\t0.2928972542285919\n",
      "Epoch 304 mean val loss:\t3500.408447265625\n",
      "Epoch 305 mean train loss:\t0.284284770488739\n",
      "Epoch 305 mean val loss:\t4837.81005859375\n",
      "Epoch 306 mean train loss:\t0.3181566894054413\n",
      "Epoch 306 mean val loss:\t2061.814453125\n",
      "Epoch 307 mean train loss:\t0.2992873191833496\n",
      "Epoch 307 mean val loss:\t1567.039794921875\n",
      "Epoch 308 mean train loss:\t0.30214276909828186\n",
      "Epoch 308 mean val loss:\t1234.3572998046875\n",
      "Epoch 309 mean train loss:\t0.3109353482723236\n",
      "Epoch 309 mean val loss:\t1236.84130859375\n",
      "Epoch 310 mean train loss:\t0.2999981641769409\n",
      "Epoch 310 mean val loss:\t3362.172119140625\n",
      "Epoch 311 mean train loss:\t0.3137681484222412\n",
      "Epoch 311 mean val loss:\t5037.5234375\n",
      "Epoch 312 mean train loss:\t0.2853817045688629\n",
      "Epoch 312 mean val loss:\t4586.66015625\n",
      "Epoch 313 mean train loss:\t0.281210720539093\n",
      "Epoch 313 mean val loss:\t1431.9256591796875\n",
      "Epoch 314 mean train loss:\t0.3588052988052368\n",
      "Epoch 314 mean val loss:\t2585.7685546875\n",
      "Epoch 315 mean train loss:\t0.2941969931125641\n",
      "Epoch 315 mean val loss:\t5433.12109375\n",
      "Epoch 316 mean train loss:\t0.29880771040916443\n",
      "Epoch 316 mean val loss:\t4304.447265625\n",
      "Epoch 317 mean train loss:\t0.30129900574684143\n",
      "Epoch 317 mean val loss:\t4659.98681640625\n",
      "Epoch 318 mean train loss:\t0.3075651228427887\n",
      "Epoch 318 mean val loss:\t4619.30322265625\n",
      "Epoch 319 mean train loss:\t0.2686169147491455\n",
      "Epoch 319 mean val loss:\t2892.240966796875\n",
      "Epoch 320 mean train loss:\t0.2910315990447998\n",
      "Epoch 320 mean val loss:\t2812.701171875\n",
      "Epoch 321 mean train loss:\t0.28166431188583374\n",
      "Epoch 321 mean val loss:\t2656.20263671875\n",
      "Epoch 322 mean train loss:\t0.27073442935943604\n",
      "Epoch 322 mean val loss:\t1742.08544921875\n",
      "Epoch 323 mean train loss:\t0.3086150884628296\n",
      "Epoch 323 mean val loss:\t4382.42724609375\n",
      "Epoch 324 mean train loss:\t0.3019322454929352\n",
      "Epoch 324 mean val loss:\t6704.265625\n",
      "Epoch 325 mean train loss:\t0.3491930067539215\n",
      "Epoch 325 mean val loss:\t1477.48291015625\n",
      "Epoch 326 mean train loss:\t0.4241042137145996\n",
      "Epoch 326 mean val loss:\t14221.3271484375\n",
      "Epoch 327 mean train loss:\t0.36033183336257935\n",
      "Epoch 327 mean val loss:\t5170.86962890625\n",
      "Epoch 328 mean train loss:\t0.2758011221885681\n",
      "Epoch 328 mean val loss:\t7266.43408203125\n",
      "Epoch 329 mean train loss:\t0.306389182806015\n",
      "Epoch 329 mean val loss:\t2736.066650390625\n",
      "Epoch 330 mean train loss:\t0.3165171444416046\n",
      "Epoch 330 mean val loss:\t3887.1806640625\n",
      "Epoch 331 mean train loss:\t0.27341723442077637\n",
      "Epoch 331 mean val loss:\t1278.119140625\n",
      "Epoch 332 mean train loss:\t0.29194703698158264\n",
      "Epoch 332 mean val loss:\t1365.406494140625\n",
      "Epoch 333 mean train loss:\t0.2794395685195923\n",
      "Epoch 333 mean val loss:\t1048.1962890625\n",
      "Epoch 334 mean train loss:\t0.278780996799469\n",
      "Epoch 334 mean val loss:\t3047.344970703125\n",
      "Epoch 335 mean train loss:\t0.2832263708114624\n",
      "Epoch 335 mean val loss:\t1076.136962890625\n",
      "Epoch 336 mean train loss:\t0.31657660007476807\n",
      "Epoch 336 mean val loss:\t1942.3966064453125\n",
      "Epoch 337 mean train loss:\t0.2876156270503998\n",
      "Epoch 337 mean val loss:\t1861.4344482421875\n",
      "Epoch 338 mean train loss:\t0.26271551847457886\n",
      "Epoch 338 mean val loss:\t2328.649169921875\n",
      "Epoch 339 mean train loss:\t0.26069146394729614\n",
      "Epoch 339 mean val loss:\t3580.986572265625\n",
      "Epoch 340 mean train loss:\t0.2800629138946533\n",
      "Epoch 340 mean val loss:\t2770.428955078125\n",
      "Epoch 341 mean train loss:\t0.31065285205841064\n",
      "Epoch 341 mean val loss:\t275.21807861328125\n",
      "Epoch 342 mean train loss:\t0.2728196680545807\n",
      "Epoch 342 mean val loss:\t3505.54296875\n",
      "Epoch 343 mean train loss:\t0.27387288212776184\n",
      "Epoch 343 mean val loss:\t1657.1456298828125\n",
      "Epoch 344 mean train loss:\t0.2846652567386627\n",
      "Epoch 344 mean val loss:\t2428.67919921875\n",
      "Epoch 345 mean train loss:\t0.2625202536582947\n",
      "Epoch 345 mean val loss:\t1390.8349609375\n",
      "Epoch 346 mean train loss:\t0.2595727741718292\n",
      "Epoch 346 mean val loss:\t1188.9991455078125\n",
      "Epoch 347 mean train loss:\t0.2699701488018036\n",
      "Epoch 347 mean val loss:\t823.1165771484375\n",
      "Epoch 348 mean train loss:\t0.282805860042572\n",
      "Epoch 348 mean val loss:\t1732.7310791015625\n",
      "Epoch 349 mean train loss:\t0.2798432409763336\n",
      "Epoch 349 mean val loss:\t4192.5517578125\n",
      "Epoch 350 mean train loss:\t0.2647962272167206\n",
      "Epoch 350 mean val loss:\t3850.86328125\n",
      "Epoch 351 mean train loss:\t0.2817043960094452\n",
      "Epoch 351 mean val loss:\t3185.362548828125\n",
      "Epoch 352 mean train loss:\t0.26328137516975403\n",
      "Epoch 352 mean val loss:\t3820.24853515625\n",
      "Epoch 353 mean train loss:\t0.2731478810310364\n",
      "Epoch 353 mean val loss:\t2133.1650390625\n",
      "Epoch 354 mean train loss:\t0.2938270568847656\n",
      "Epoch 354 mean val loss:\t1857.259033203125\n",
      "Epoch 355 mean train loss:\t0.27517515420913696\n",
      "Epoch 355 mean val loss:\t3076.24609375\n",
      "Epoch 356 mean train loss:\t0.25827303528785706\n",
      "Epoch 356 mean val loss:\t1815.0048828125\n",
      "Epoch 357 mean train loss:\t0.259165495634079\n",
      "Epoch 357 mean val loss:\t4325.89990234375\n",
      "Epoch 358 mean train loss:\t0.24486446380615234\n",
      "Epoch 358 mean val loss:\t5357.9853515625\n",
      "Epoch 359 mean train loss:\t0.25050726532936096\n",
      "Epoch 359 mean val loss:\t969.7787475585938\n",
      "Epoch 360 mean train loss:\t0.2669212222099304\n",
      "Epoch 360 mean val loss:\t2486.14599609375\n",
      "Epoch 361 mean train loss:\t0.2930624186992645\n",
      "Epoch 361 mean val loss:\t953.0408325195312\n",
      "Epoch 362 mean train loss:\t0.27069270610809326\n",
      "Epoch 362 mean val loss:\t2549.916748046875\n",
      "Epoch 363 mean train loss:\t0.2579537332057953\n",
      "Epoch 363 mean val loss:\t1372.841064453125\n",
      "Epoch 364 mean train loss:\t0.2708517909049988\n",
      "Epoch 364 mean val loss:\t2850.807861328125\n",
      "Epoch 365 mean train loss:\t0.2775169312953949\n",
      "Epoch 365 mean val loss:\t713.9140014648438\n",
      "Epoch 366 mean train loss:\t0.2543881833553314\n",
      "Epoch 366 mean val loss:\t3935.715087890625\n",
      "Epoch 367 mean train loss:\t0.2489180862903595\n",
      "Epoch 367 mean val loss:\t1338.428466796875\n",
      "Epoch 368 mean train loss:\t0.26820018887519836\n",
      "Epoch 368 mean val loss:\t2038.0576171875\n",
      "Epoch 369 mean train loss:\t0.26252126693725586\n",
      "Epoch 369 mean val loss:\t2033.0958251953125\n",
      "Epoch 370 mean train loss:\t0.2584819197654724\n",
      "Epoch 370 mean val loss:\t5755.3671875\n",
      "Epoch 371 mean train loss:\t0.2411162406206131\n",
      "Epoch 371 mean val loss:\t3825.2265625\n",
      "Epoch 372 mean train loss:\t0.2506428360939026\n",
      "Epoch 372 mean val loss:\t744.318115234375\n",
      "Epoch 373 mean train loss:\t0.2578212022781372\n",
      "Epoch 373 mean val loss:\t988.921875\n",
      "Epoch 374 mean train loss:\t0.24910081923007965\n",
      "Epoch 374 mean val loss:\t1632.49951171875\n",
      "Epoch 375 mean train loss:\t0.26638102531433105\n",
      "Epoch 375 mean val loss:\t1438.9603271484375\n",
      "Epoch 376 mean train loss:\t0.24780593812465668\n",
      "Epoch 376 mean val loss:\t2825.4052734375\n",
      "Epoch 377 mean train loss:\t0.2854727804660797\n",
      "Epoch 377 mean val loss:\t4814.27880859375\n",
      "Epoch 378 mean train loss:\t0.27142348885536194\n",
      "Epoch 378 mean val loss:\t4556.9658203125\n",
      "Epoch 379 mean train loss:\t0.26370465755462646\n",
      "Epoch 379 mean val loss:\t4004.30419921875\n",
      "Epoch 380 mean train loss:\t0.24147818982601166\n",
      "Epoch 380 mean val loss:\t2316.13232421875\n",
      "Epoch 381 mean train loss:\t0.24624459445476532\n",
      "Epoch 381 mean val loss:\t4906.802734375\n",
      "Epoch 382 mean train loss:\t0.2543795108795166\n",
      "Epoch 382 mean val loss:\t8524.9365234375\n",
      "Epoch 383 mean train loss:\t0.26668936014175415\n",
      "Epoch 383 mean val loss:\t5002.5068359375\n",
      "Epoch 384 mean train loss:\t0.25525569915771484\n",
      "Epoch 384 mean val loss:\t5507.865234375\n",
      "Epoch 385 mean train loss:\t0.24194304645061493\n",
      "Epoch 385 mean val loss:\t10472.7890625\n",
      "Epoch 386 mean train loss:\t0.223776713013649\n",
      "Epoch 386 mean val loss:\t7605.93017578125\n",
      "Epoch 387 mean train loss:\t0.24928000569343567\n",
      "Epoch 387 mean val loss:\t2241.174072265625\n",
      "Epoch 388 mean train loss:\t0.23604720830917358\n",
      "Epoch 388 mean val loss:\t7034.86474609375\n",
      "Epoch 389 mean train loss:\t0.24853169918060303\n",
      "Epoch 389 mean val loss:\t3692.6376953125\n",
      "Epoch 390 mean train loss:\t0.2824309170246124\n",
      "Epoch 390 mean val loss:\t6365.71630859375\n",
      "Epoch 391 mean train loss:\t0.2942274212837219\n",
      "Epoch 391 mean val loss:\t4668.29931640625\n",
      "Epoch 392 mean train loss:\t0.2720320224761963\n",
      "Epoch 392 mean val loss:\t1635.794189453125\n",
      "Epoch 393 mean train loss:\t0.2662777304649353\n",
      "Epoch 393 mean val loss:\t2745.396728515625\n",
      "Epoch 394 mean train loss:\t0.2437782883644104\n",
      "Epoch 394 mean val loss:\t7818.31689453125\n",
      "Epoch 395 mean train loss:\t0.220359206199646\n",
      "Epoch 395 mean val loss:\t6692.421875\n",
      "Epoch 396 mean train loss:\t0.2371082603931427\n",
      "Epoch 396 mean val loss:\t2968.19091796875\n",
      "Epoch 397 mean train loss:\t0.248490110039711\n",
      "Epoch 397 mean val loss:\t3054.925048828125\n",
      "Epoch 398 mean train loss:\t0.21289826929569244\n",
      "Epoch 398 mean val loss:\t9895.6572265625\n",
      "Epoch 399 mean train loss:\t0.2110574096441269\n",
      "Epoch 399 mean val loss:\t8661.9912109375\n",
      "Epoch 400 mean train loss:\t0.21801899373531342\n",
      "Epoch 400 mean val loss:\t13774.294921875\n",
      "Epoch 401 mean train loss:\t0.277263879776001\n",
      "Epoch 401 mean val loss:\t16182.46484375\n",
      "Epoch 402 mean train loss:\t0.28944432735443115\n",
      "Epoch 402 mean val loss:\t5496.91650390625\n",
      "Epoch 403 mean train loss:\t0.24081090092658997\n",
      "Epoch 403 mean val loss:\t4376.56640625\n",
      "Epoch 404 mean train loss:\t0.23040150105953217\n",
      "Epoch 404 mean val loss:\t6941.13623046875\n",
      "Epoch 405 mean train loss:\t0.24996235966682434\n",
      "Epoch 405 mean val loss:\t6654.16015625\n",
      "Epoch 406 mean train loss:\t0.27284476161003113\n",
      "Epoch 406 mean val loss:\t4141.79248046875\n",
      "Epoch 407 mean train loss:\t0.25060296058654785\n",
      "Epoch 407 mean val loss:\t4309.16162109375\n",
      "Epoch 408 mean train loss:\t0.22782020270824432\n",
      "Epoch 408 mean val loss:\t7331.7890625\n",
      "Epoch 409 mean train loss:\t0.2187233716249466\n",
      "Epoch 409 mean val loss:\t13380.3466796875\n",
      "Epoch 410 mean train loss:\t0.24789832532405853\n",
      "Epoch 410 mean val loss:\t2296.567138671875\n",
      "Epoch 411 mean train loss:\t0.2651536762714386\n",
      "Epoch 411 mean val loss:\t4705.19775390625\n",
      "Epoch 412 mean train loss:\t0.3063613474369049\n",
      "Epoch 412 mean val loss:\t4471.06396484375\n",
      "Epoch 413 mean train loss:\t0.3424271047115326\n",
      "Epoch 413 mean val loss:\t30562.28515625\n",
      "Epoch 414 mean train loss:\t0.3110211193561554\n",
      "Epoch 414 mean val loss:\t5920.029296875\n",
      "Epoch 415 mean train loss:\t0.26117974519729614\n",
      "Epoch 415 mean val loss:\t8632.2373046875\n",
      "Epoch 416 mean train loss:\t0.26631808280944824\n",
      "Epoch 416 mean val loss:\t3595.20654296875\n",
      "Epoch 417 mean train loss:\t0.2412383109331131\n",
      "Epoch 417 mean val loss:\t1914.930908203125\n",
      "Epoch 418 mean train loss:\t0.2228236049413681\n",
      "Epoch 418 mean val loss:\t2979.068603515625\n",
      "Epoch 419 mean train loss:\t0.2564820647239685\n",
      "Epoch 419 mean val loss:\t2325.02001953125\n",
      "Epoch 420 mean train loss:\t0.2334543615579605\n",
      "Epoch 420 mean val loss:\t9412.1875\n",
      "Epoch 421 mean train loss:\t0.22705909609794617\n",
      "Epoch 421 mean val loss:\t4032.94677734375\n",
      "Epoch 422 mean train loss:\t0.2327410876750946\n",
      "Epoch 422 mean val loss:\t5462.86328125\n",
      "Epoch 423 mean train loss:\t0.22363291680812836\n",
      "Epoch 423 mean val loss:\t9068.9521484375\n",
      "Epoch 424 mean train loss:\t0.2273290753364563\n",
      "Epoch 424 mean val loss:\t9987.2275390625\n",
      "Epoch 425 mean train loss:\t0.21807444095611572\n",
      "Epoch 425 mean val loss:\t13860.5078125\n",
      "Epoch 426 mean train loss:\t0.24800121784210205\n",
      "Epoch 426 mean val loss:\t14262.8291015625\n",
      "Epoch 427 mean train loss:\t0.21511350572109222\n",
      "Epoch 427 mean val loss:\t19138.74609375\n",
      "Epoch 428 mean train loss:\t0.22092358767986298\n",
      "Epoch 428 mean val loss:\t16401.796875\n",
      "Epoch 429 mean train loss:\t0.21229645609855652\n",
      "Epoch 429 mean val loss:\t6458.51171875\n",
      "Epoch 430 mean train loss:\t0.21778494119644165\n",
      "Epoch 430 mean val loss:\t3256.329345703125\n",
      "Epoch 431 mean train loss:\t0.25679245591163635\n",
      "Epoch 431 mean val loss:\t7346.68505859375\n",
      "Epoch 432 mean train loss:\t0.25504443049430847\n",
      "Epoch 432 mean val loss:\t3119.15625\n",
      "Epoch 433 mean train loss:\t0.22160419821739197\n",
      "Epoch 433 mean val loss:\t11535.5361328125\n",
      "Epoch 434 mean train loss:\t0.24790707230567932\n",
      "Epoch 434 mean val loss:\t8336.37890625\n",
      "Epoch 435 mean train loss:\t0.2282007336616516\n",
      "Epoch 435 mean val loss:\t8332.62890625\n",
      "Epoch 436 mean train loss:\t0.22761127352714539\n",
      "Epoch 436 mean val loss:\t2337.395751953125\n",
      "Epoch 437 mean train loss:\t0.4037463366985321\n",
      "Epoch 437 mean val loss:\t784.7900390625\n",
      "Epoch 438 mean train loss:\t0.5053224563598633\n",
      "Epoch 438 mean val loss:\t468.265380859375\n",
      "Epoch 439 mean train loss:\t0.33439427614212036\n",
      "Epoch 439 mean val loss:\t997.36083984375\n",
      "Epoch 440 mean train loss:\t0.27075549960136414\n",
      "Epoch 440 mean val loss:\t739.05419921875\n",
      "Epoch 441 mean train loss:\t0.2818034291267395\n",
      "Epoch 441 mean val loss:\t992.8499755859375\n",
      "Epoch 442 mean train loss:\t0.25448206067085266\n",
      "Epoch 442 mean val loss:\t2033.0849609375\n",
      "Epoch 443 mean train loss:\t0.2235369235277176\n",
      "Epoch 443 mean val loss:\t2675.744140625\n",
      "Epoch 444 mean train loss:\t0.24733886122703552\n",
      "Epoch 444 mean val loss:\t827.24951171875\n",
      "Epoch 445 mean train loss:\t0.23334535956382751\n",
      "Epoch 445 mean val loss:\t1465.610595703125\n",
      "Epoch 446 mean train loss:\t0.21712404489517212\n",
      "Epoch 446 mean val loss:\t1019.8193969726562\n",
      "Epoch 447 mean train loss:\t0.22340771555900574\n",
      "Epoch 447 mean val loss:\t1352.312255859375\n",
      "Epoch 448 mean train loss:\t0.2241545170545578\n",
      "Epoch 448 mean val loss:\t3049.416748046875\n",
      "Epoch 449 mean train loss:\t0.20846490561962128\n",
      "Epoch 449 mean val loss:\t1406.3768310546875\n",
      "Epoch 450 mean train loss:\t0.21640758216381073\n",
      "Epoch 450 mean val loss:\t2197.09814453125\n",
      "Epoch 451 mean train loss:\t0.2350115180015564\n",
      "Epoch 451 mean val loss:\t2665.475830078125\n",
      "Epoch 452 mean train loss:\t0.21817311644554138\n",
      "Epoch 452 mean val loss:\t1637.4666748046875\n",
      "Epoch 453 mean train loss:\t0.22621497511863708\n",
      "Epoch 453 mean val loss:\t1209.9520263671875\n",
      "Epoch 454 mean train loss:\t0.35888388752937317\n",
      "Epoch 454 mean val loss:\t224.33267211914062\n",
      "Epoch 455 mean train loss:\t0.2769279181957245\n",
      "Epoch 455 mean val loss:\t662.3329467773438\n",
      "Epoch 456 mean train loss:\t0.2441970556974411\n",
      "Epoch 456 mean val loss:\t526.239013671875\n",
      "Epoch 457 mean train loss:\t0.23149585723876953\n",
      "Epoch 457 mean val loss:\t1009.0023193359375\n",
      "Epoch 458 mean train loss:\t0.22767609357833862\n",
      "Epoch 458 mean val loss:\t622.7635498046875\n",
      "Epoch 459 mean train loss:\t0.21441109478473663\n",
      "Epoch 459 mean val loss:\t535.41357421875\n",
      "Epoch 460 mean train loss:\t0.21402159333229065\n",
      "Epoch 460 mean val loss:\t508.9996032714844\n",
      "Epoch 461 mean train loss:\t0.21310168504714966\n",
      "Epoch 461 mean val loss:\t367.8307189941406\n",
      "Epoch 462 mean train loss:\t0.22202368080615997\n",
      "Epoch 462 mean val loss:\t344.4456787109375\n",
      "Epoch 463 mean train loss:\t0.2143164873123169\n",
      "Epoch 463 mean val loss:\t1018.2565307617188\n",
      "Epoch 464 mean train loss:\t0.22492745518684387\n",
      "Epoch 464 mean val loss:\t1028.34912109375\n",
      "Epoch 465 mean train loss:\t0.20272336900234222\n",
      "Epoch 465 mean val loss:\t1545.44091796875\n",
      "Epoch 466 mean train loss:\t0.21417644619941711\n",
      "Epoch 466 mean val loss:\t1315.5247802734375\n",
      "Epoch 467 mean train loss:\t0.22196249663829803\n",
      "Epoch 467 mean val loss:\t751.9220581054688\n",
      "Epoch 468 mean train loss:\t0.21516892313957214\n",
      "Epoch 468 mean val loss:\t672.62451171875\n",
      "Epoch 469 mean train loss:\t0.22683319449424744\n",
      "Epoch 469 mean val loss:\t985.0059814453125\n",
      "Epoch 470 mean train loss:\t0.2428724318742752\n",
      "Epoch 470 mean val loss:\t1138.8675537109375\n",
      "Epoch 471 mean train loss:\t0.21553528308868408\n",
      "Epoch 471 mean val loss:\t735.49951171875\n",
      "Epoch 472 mean train loss:\t0.20494192838668823\n",
      "Epoch 472 mean val loss:\t1095.2227783203125\n",
      "Epoch 473 mean train loss:\t0.1947445124387741\n",
      "Epoch 473 mean val loss:\t897.5978393554688\n",
      "Epoch 474 mean train loss:\t0.19924145936965942\n",
      "Epoch 474 mean val loss:\t2330.731201171875\n",
      "Epoch 475 mean train loss:\t0.19981712102890015\n",
      "Epoch 475 mean val loss:\t1653.98681640625\n",
      "Epoch 476 mean train loss:\t0.19620931148529053\n",
      "Epoch 476 mean val loss:\t2447.107421875\n",
      "Epoch 477 mean train loss:\t0.22180089354515076\n",
      "Epoch 477 mean val loss:\t1686.302490234375\n",
      "Epoch 478 mean train loss:\t0.21601134538650513\n",
      "Epoch 478 mean val loss:\t1053.8780517578125\n",
      "Epoch 479 mean train loss:\t0.20900070667266846\n",
      "Epoch 479 mean val loss:\t1935.7008056640625\n",
      "Epoch 480 mean train loss:\t0.1975608915090561\n",
      "Epoch 480 mean val loss:\t1262.2293701171875\n",
      "Epoch 481 mean train loss:\t0.20215749740600586\n",
      "Epoch 481 mean val loss:\t3683.13427734375\n",
      "Epoch 482 mean train loss:\t0.22168521583080292\n",
      "Epoch 482 mean val loss:\t2052.1865234375\n",
      "Epoch 483 mean train loss:\t0.24468886852264404\n",
      "Epoch 483 mean val loss:\t724.6486206054688\n",
      "Epoch 484 mean train loss:\t0.2459419220685959\n",
      "Epoch 484 mean val loss:\t3933.30419921875\n",
      "Epoch 485 mean train loss:\t0.21866178512573242\n",
      "Epoch 485 mean val loss:\t3106.256591796875\n",
      "Epoch 486 mean train loss:\t0.22187109291553497\n",
      "Epoch 486 mean val loss:\t3662.593994140625\n",
      "Epoch 487 mean train loss:\t0.1994200199842453\n",
      "Epoch 487 mean val loss:\t3492.613037109375\n",
      "Epoch 488 mean train loss:\t0.24786733090877533\n",
      "Epoch 488 mean val loss:\t6168.552734375\n",
      "Epoch 489 mean train loss:\t0.25945305824279785\n",
      "Epoch 489 mean val loss:\t2175.049560546875\n",
      "Epoch 490 mean train loss:\t0.21601518988609314\n",
      "Epoch 490 mean val loss:\t1005.6726684570312\n",
      "Epoch 491 mean train loss:\t0.20112308859825134\n",
      "Epoch 491 mean val loss:\t1144.3858642578125\n",
      "Epoch 492 mean train loss:\t0.18922825157642365\n",
      "Epoch 492 mean val loss:\t2267.965576171875\n",
      "Epoch 493 mean train loss:\t0.19069691002368927\n",
      "Epoch 493 mean val loss:\t1311.859375\n",
      "Epoch 494 mean train loss:\t0.20815642178058624\n",
      "Epoch 494 mean val loss:\t3540.767578125\n",
      "Epoch 495 mean train loss:\t0.21000976860523224\n",
      "Epoch 495 mean val loss:\t1976.3690185546875\n",
      "Epoch 496 mean train loss:\t0.25952741503715515\n",
      "Epoch 496 mean val loss:\t8443.470703125\n",
      "Epoch 497 mean train loss:\t0.218536376953125\n",
      "Epoch 497 mean val loss:\t6476.2841796875\n",
      "Epoch 498 mean train loss:\t0.3230283260345459\n",
      "Epoch 498 mean val loss:\t7791.77587890625\n",
      "Epoch 499 mean train loss:\t0.3392506539821625\n",
      "Epoch 499 mean val loss:\t3806.689453125\n",
      "Epoch 500 mean train loss:\t0.27755120396614075\n",
      "Epoch 500 mean val loss:\t4369.3671875\n",
      "Epoch 501 mean train loss:\t0.2364879995584488\n",
      "Epoch 501 mean val loss:\t12051.13671875\n",
      "Epoch 502 mean train loss:\t0.24202130734920502\n",
      "Epoch 502 mean val loss:\t4143.49609375\n",
      "Epoch 503 mean train loss:\t0.21445190906524658\n",
      "Epoch 503 mean val loss:\t4391.03515625\n",
      "Epoch 504 mean train loss:\t0.20563411712646484\n",
      "Epoch 504 mean val loss:\t12435.234375\n",
      "Epoch 505 mean train loss:\t0.22414806485176086\n",
      "Epoch 505 mean val loss:\t11306.685546875\n",
      "Epoch 506 mean train loss:\t0.21546870470046997\n",
      "Epoch 506 mean val loss:\t5119.9404296875\n",
      "Epoch 507 mean train loss:\t0.22759979963302612\n",
      "Epoch 507 mean val loss:\t2082.143798828125\n",
      "Epoch 508 mean train loss:\t0.2107951045036316\n",
      "Epoch 508 mean val loss:\t1611.2659912109375\n",
      "Epoch 509 mean train loss:\t0.21099591255187988\n",
      "Epoch 509 mean val loss:\t4666.70947265625\n",
      "Epoch 510 mean train loss:\t0.2249501645565033\n",
      "Epoch 510 mean val loss:\t7006.7646484375\n",
      "Epoch 511 mean train loss:\t0.222268208861351\n",
      "Epoch 511 mean val loss:\t3980.590576171875\n",
      "Epoch 512 mean train loss:\t0.22031241655349731\n",
      "Epoch 512 mean val loss:\t3929.9228515625\n",
      "Epoch 513 mean train loss:\t0.2010210156440735\n",
      "Epoch 513 mean val loss:\t4221.859375\n",
      "Epoch 514 mean train loss:\t0.22433814406394958\n",
      "Epoch 514 mean val loss:\t1472.17919921875\n",
      "Epoch 515 mean train loss:\t0.21274317800998688\n",
      "Epoch 515 mean val loss:\t8950.8046875\n",
      "Epoch 516 mean train loss:\t0.23543789982795715\n",
      "Epoch 516 mean val loss:\t1054.642333984375\n",
      "Epoch 517 mean train loss:\t0.22775018215179443\n",
      "Epoch 517 mean val loss:\t1754.998046875\n",
      "Epoch 518 mean train loss:\t0.23659908771514893\n",
      "Epoch 518 mean val loss:\t1159.9014892578125\n",
      "Epoch 519 mean train loss:\t0.2284805178642273\n",
      "Epoch 519 mean val loss:\t763.5116577148438\n",
      "Epoch 520 mean train loss:\t0.20440855622291565\n",
      "Epoch 520 mean val loss:\t1075.2545166015625\n",
      "Epoch 521 mean train loss:\t0.20629379153251648\n",
      "Epoch 521 mean val loss:\t3517.189208984375\n",
      "Epoch 522 mean train loss:\t0.20419037342071533\n",
      "Epoch 522 mean val loss:\t1851.61962890625\n",
      "Epoch 523 mean train loss:\t0.20874939858913422\n",
      "Epoch 523 mean val loss:\t2207.221923828125\n",
      "Epoch 524 mean train loss:\t0.21081799268722534\n",
      "Epoch 524 mean val loss:\t6812.095703125\n",
      "Epoch 525 mean train loss:\t0.22388184070587158\n",
      "Epoch 525 mean val loss:\t3556.00537109375\n",
      "Epoch 526 mean train loss:\t0.19780074059963226\n",
      "Epoch 526 mean val loss:\t2204.100341796875\n",
      "Epoch 527 mean train loss:\t0.1959359049797058\n",
      "Epoch 527 mean val loss:\t1767.6123046875\n",
      "Epoch 528 mean train loss:\t0.19128239154815674\n",
      "Epoch 528 mean val loss:\t2393.437255859375\n",
      "Epoch 529 mean train loss:\t0.20600220561027527\n",
      "Epoch 529 mean val loss:\t1374.4990234375\n",
      "Epoch 530 mean train loss:\t0.19769085943698883\n",
      "Epoch 530 mean val loss:\t2954.415283203125\n",
      "Epoch 531 mean train loss:\t0.18633639812469482\n",
      "Epoch 531 mean val loss:\t897.2376098632812\n",
      "Epoch 532 mean train loss:\t0.20968343317508698\n",
      "Epoch 532 mean val loss:\t5227.3603515625\n",
      "Epoch 533 mean train loss:\t0.20466509461402893\n",
      "Epoch 533 mean val loss:\t4169.64697265625\n",
      "Epoch 534 mean train loss:\t0.2061273753643036\n",
      "Epoch 534 mean val loss:\t9008.7265625\n",
      "Epoch 535 mean train loss:\t0.19839811325073242\n",
      "Epoch 535 mean val loss:\t2150.6298828125\n",
      "Epoch 536 mean train loss:\t0.19078807532787323\n",
      "Epoch 536 mean val loss:\t2437.98193359375\n",
      "Epoch 537 mean train loss:\t0.20447319746017456\n",
      "Epoch 537 mean val loss:\t3414.43212890625\n",
      "Epoch 538 mean train loss:\t0.2301495522260666\n",
      "Epoch 538 mean val loss:\t2634.64599609375\n",
      "Epoch 539 mean train loss:\t0.21615222096443176\n",
      "Epoch 539 mean val loss:\t2252.574951171875\n",
      "Epoch 540 mean train loss:\t0.2203521579504013\n",
      "Epoch 540 mean val loss:\t3538.05224609375\n",
      "Epoch 541 mean train loss:\t0.21963322162628174\n",
      "Epoch 541 mean val loss:\t5784.607421875\n",
      "Epoch 542 mean train loss:\t0.22235648334026337\n",
      "Epoch 542 mean val loss:\t1752.3922119140625\n",
      "Epoch 543 mean train loss:\t0.18348488211631775\n",
      "Epoch 543 mean val loss:\t3848.470703125\n",
      "Epoch 544 mean train loss:\t0.18994617462158203\n",
      "Epoch 544 mean val loss:\t1843.3001708984375\n",
      "Epoch 545 mean train loss:\t0.19548743963241577\n",
      "Epoch 545 mean val loss:\t4419.40380859375\n",
      "Epoch 546 mean train loss:\t0.18594039976596832\n",
      "Epoch 546 mean val loss:\t3724.431640625\n",
      "Epoch 547 mean train loss:\t0.18531277775764465\n",
      "Epoch 547 mean val loss:\t7635.04296875\n",
      "Epoch 548 mean train loss:\t0.2028006911277771\n",
      "Epoch 548 mean val loss:\t1383.6397705078125\n",
      "Epoch 549 mean train loss:\t0.18306003510951996\n",
      "Epoch 549 mean val loss:\t5061.9970703125\n",
      "Epoch 550 mean train loss:\t0.18526695668697357\n",
      "Epoch 550 mean val loss:\t4101.1552734375\n",
      "Epoch 551 mean train loss:\t0.2667807936668396\n",
      "Epoch 551 mean val loss:\t652.1187133789062\n",
      "Epoch 552 mean train loss:\t0.3910931944847107\n",
      "Epoch 552 mean val loss:\t30578.28515625\n",
      "Epoch 553 mean train loss:\t0.33674609661102295\n",
      "Epoch 553 mean val loss:\t25501.923828125\n",
      "Epoch 554 mean train loss:\t0.259576678276062\n",
      "Epoch 554 mean val loss:\t4345.12890625\n",
      "Epoch 555 mean train loss:\t0.21408770978450775\n",
      "Epoch 555 mean val loss:\t7337.79443359375\n",
      "Epoch 556 mean train loss:\t0.24201619625091553\n",
      "Epoch 556 mean val loss:\t3193.8798828125\n",
      "Epoch 557 mean train loss:\t0.18928737938404083\n",
      "Epoch 557 mean val loss:\t6149.17431640625\n",
      "Epoch 558 mean train loss:\t0.20363309979438782\n",
      "Epoch 558 mean val loss:\t4303.11328125\n",
      "Epoch 559 mean train loss:\t0.20529896020889282\n",
      "Epoch 559 mean val loss:\t11703.513671875\n",
      "Epoch 560 mean train loss:\t0.1929466724395752\n",
      "Epoch 560 mean val loss:\t13974.603515625\n",
      "Epoch 561 mean train loss:\t0.2946665287017822\n",
      "Epoch 561 mean val loss:\t1223.262939453125\n",
      "Epoch 562 mean train loss:\t0.23796749114990234\n",
      "Epoch 562 mean val loss:\t2200.184326171875\n",
      "Epoch 563 mean train loss:\t0.22515824437141418\n",
      "Epoch 563 mean val loss:\t4114.31787109375\n",
      "Epoch 564 mean train loss:\t0.242763489484787\n",
      "Epoch 564 mean val loss:\t4282.95263671875\n",
      "Epoch 565 mean train loss:\t0.21328558027744293\n",
      "Epoch 565 mean val loss:\t4359.9228515625\n",
      "Epoch 566 mean train loss:\t0.199981227517128\n",
      "Epoch 566 mean val loss:\t4049.98388671875\n",
      "Epoch 567 mean train loss:\t0.20530566573143005\n",
      "Epoch 567 mean val loss:\t4044.447509765625\n",
      "Epoch 568 mean train loss:\t0.1933223158121109\n",
      "Epoch 568 mean val loss:\t3163.944580078125\n",
      "Epoch 569 mean train loss:\t0.18904650211334229\n",
      "Epoch 569 mean val loss:\t2325.6806640625\n",
      "Epoch 570 mean train loss:\t0.19627068936824799\n",
      "Epoch 570 mean val loss:\t1195.3681640625\n",
      "Epoch 571 mean train loss:\t0.18471774458885193\n",
      "Epoch 571 mean val loss:\t2445.188232421875\n",
      "Epoch 572 mean train loss:\t0.18000401556491852\n",
      "Epoch 572 mean val loss:\t3341.9267578125\n",
      "Epoch 573 mean train loss:\t0.18134237825870514\n",
      "Epoch 573 mean val loss:\t3676.518310546875\n",
      "Epoch 574 mean train loss:\t0.17641668021678925\n",
      "Epoch 574 mean val loss:\t2925.830078125\n",
      "Epoch 575 mean train loss:\t0.1745610535144806\n",
      "Epoch 575 mean val loss:\t3537.591552734375\n",
      "Epoch 576 mean train loss:\t0.20358923077583313\n",
      "Epoch 576 mean val loss:\t2704.19384765625\n",
      "Epoch 577 mean train loss:\t0.19234904646873474\n",
      "Epoch 577 mean val loss:\t4228.9140625\n",
      "Epoch 578 mean train loss:\t0.18713317811489105\n",
      "Epoch 578 mean val loss:\t7277.50830078125\n",
      "Epoch 579 mean train loss:\t0.20693761110305786\n",
      "Epoch 579 mean val loss:\t2904.176513671875\n",
      "Epoch 580 mean train loss:\t0.2038850635290146\n",
      "Epoch 580 mean val loss:\t4415.27001953125\n",
      "Epoch 581 mean train loss:\t0.19590693712234497\n",
      "Epoch 581 mean val loss:\t3535.302978515625\n",
      "Epoch 582 mean train loss:\t0.20833267271518707\n",
      "Epoch 582 mean val loss:\t695.419189453125\n",
      "Epoch 583 mean train loss:\t0.19145174324512482\n",
      "Epoch 583 mean val loss:\t2510.363525390625\n",
      "Epoch 584 mean train loss:\t0.1721731424331665\n",
      "Epoch 584 mean val loss:\t3819.77587890625\n",
      "Epoch 585 mean train loss:\t0.1967562884092331\n",
      "Epoch 585 mean val loss:\t4470.49853515625\n",
      "Epoch 586 mean train loss:\t0.2060869038105011\n",
      "Epoch 586 mean val loss:\t5915.28271484375\n",
      "Epoch 587 mean train loss:\t0.2014816254377365\n",
      "Epoch 587 mean val loss:\t5155.71484375\n",
      "Epoch 588 mean train loss:\t0.22138382494449615\n",
      "Epoch 588 mean val loss:\t8195.1513671875\n",
      "Epoch 589 mean train loss:\t0.20534422993659973\n",
      "Epoch 589 mean val loss:\t5805.06689453125\n",
      "Epoch 590 mean train loss:\t0.18835553526878357\n",
      "Epoch 590 mean val loss:\t9810.2568359375\n",
      "Epoch 591 mean train loss:\t0.1944890171289444\n",
      "Epoch 591 mean val loss:\t1407.876220703125\n",
      "Epoch 592 mean train loss:\t0.19930684566497803\n",
      "Epoch 592 mean val loss:\t868.700927734375\n",
      "Epoch 593 mean train loss:\t0.19862115383148193\n",
      "Epoch 593 mean val loss:\t2809.97412109375\n",
      "Epoch 594 mean train loss:\t0.18111567199230194\n",
      "Epoch 594 mean val loss:\t9642.7783203125\n",
      "Epoch 595 mean train loss:\t0.19215932488441467\n",
      "Epoch 595 mean val loss:\t5496.828125\n",
      "Epoch 596 mean train loss:\t0.18962326645851135\n",
      "Epoch 596 mean val loss:\t1604.777587890625\n",
      "Epoch 597 mean train loss:\t0.17351004481315613\n",
      "Epoch 597 mean val loss:\t4860.8447265625\n",
      "Epoch 598 mean train loss:\t0.1817837506532669\n",
      "Epoch 598 mean val loss:\t4862.98828125\n",
      "Epoch 599 mean train loss:\t0.200840026140213\n",
      "Epoch 599 mean val loss:\t5322.23828125\n",
      "Epoch 600 mean train loss:\t0.18413691222667694\n",
      "Epoch 600 mean val loss:\t1477.375\n",
      "Epoch 601 mean train loss:\t0.19208568334579468\n",
      "Epoch 601 mean val loss:\t12027.9306640625\n",
      "Epoch 602 mean train loss:\t0.19039207696914673\n",
      "Epoch 602 mean val loss:\t3177.513427734375\n",
      "Epoch 603 mean train loss:\t0.28968819975852966\n",
      "Epoch 603 mean val loss:\t6533.20361328125\n",
      "Epoch 604 mean train loss:\t0.2336939573287964\n",
      "Epoch 604 mean val loss:\t6317.73388671875\n",
      "Epoch 605 mean train loss:\t0.19634543359279633\n",
      "Epoch 605 mean val loss:\t5248.228515625\n",
      "Epoch 606 mean train loss:\t0.20800361037254333\n",
      "Epoch 606 mean val loss:\t4458.15673828125\n",
      "Epoch 607 mean train loss:\t0.1885189563035965\n",
      "Epoch 607 mean val loss:\t4980.35546875\n",
      "Epoch 608 mean train loss:\t0.19655278325080872\n",
      "Epoch 608 mean val loss:\t4918.62255859375\n",
      "Epoch 609 mean train loss:\t0.19691438972949982\n",
      "Epoch 609 mean val loss:\t13810.80078125\n",
      "Epoch 610 mean train loss:\t0.18727727234363556\n",
      "Epoch 610 mean val loss:\t3838.917724609375\n",
      "Epoch 611 mean train loss:\t0.16997742652893066\n",
      "Epoch 611 mean val loss:\t4242.19580078125\n",
      "Epoch 612 mean train loss:\t0.18569019436836243\n",
      "Epoch 612 mean val loss:\t4674.95361328125\n",
      "Epoch 613 mean train loss:\t0.1888275295495987\n",
      "Epoch 613 mean val loss:\t9805.07421875\n",
      "Epoch 614 mean train loss:\t0.17312689125537872\n",
      "Epoch 614 mean val loss:\t3418.35107421875\n",
      "Epoch 615 mean train loss:\t0.18005917966365814\n",
      "Epoch 615 mean val loss:\t2755.529052734375\n",
      "Epoch 616 mean train loss:\t0.17626899480819702\n",
      "Epoch 616 mean val loss:\t4666.04638671875\n",
      "Epoch 617 mean train loss:\t0.20142479240894318\n",
      "Epoch 617 mean val loss:\t4692.52685546875\n",
      "Epoch 618 mean train loss:\t0.19952890276908875\n",
      "Epoch 618 mean val loss:\t4042.661376953125\n",
      "Epoch 619 mean train loss:\t0.22645485401153564\n",
      "Epoch 619 mean val loss:\t1553.0950927734375\n",
      "Epoch 620 mean train loss:\t0.21794307231903076\n",
      "Epoch 620 mean val loss:\t2265.202880859375\n",
      "Epoch 621 mean train loss:\t0.20403747260570526\n",
      "Epoch 621 mean val loss:\t2622.88916015625\n",
      "Epoch 622 mean train loss:\t0.20833611488342285\n",
      "Epoch 622 mean val loss:\t2145.375732421875\n",
      "Epoch 623 mean train loss:\t0.20657260715961456\n",
      "Epoch 623 mean val loss:\t1653.7247314453125\n",
      "Epoch 624 mean train loss:\t0.2002100944519043\n",
      "Epoch 624 mean val loss:\t1953.16015625\n",
      "Epoch 625 mean train loss:\t0.1927177608013153\n",
      "Epoch 625 mean val loss:\t1070.159423828125\n",
      "Epoch 626 mean train loss:\t0.19342300295829773\n",
      "Epoch 626 mean val loss:\t1299.119384765625\n",
      "Epoch 627 mean train loss:\t0.17653009295463562\n",
      "Epoch 627 mean val loss:\t2929.53662109375\n",
      "Epoch 628 mean train loss:\t0.17507261037826538\n",
      "Epoch 628 mean val loss:\t3337.807373046875\n",
      "Epoch 629 mean train loss:\t0.16431477665901184\n",
      "Epoch 629 mean val loss:\t2243.0478515625\n",
      "Epoch 630 mean train loss:\t0.17753992974758148\n",
      "Epoch 630 mean val loss:\t3839.349365234375\n",
      "Epoch 631 mean train loss:\t0.20016644895076752\n",
      "Epoch 631 mean val loss:\t3553.44091796875\n",
      "Epoch 632 mean train loss:\t0.18683063983917236\n",
      "Epoch 632 mean val loss:\t3057.280029296875\n",
      "Epoch 633 mean train loss:\t0.18574026226997375\n",
      "Epoch 633 mean val loss:\t4002.051025390625\n",
      "Epoch 634 mean train loss:\t0.1890793889760971\n",
      "Epoch 634 mean val loss:\t7350.80322265625\n",
      "Epoch 635 mean train loss:\t0.17680780589580536\n",
      "Epoch 635 mean val loss:\t5502.73828125\n",
      "Epoch 636 mean train loss:\t0.26211607456207275\n",
      "Epoch 636 mean val loss:\t112640.5703125\n",
      "Epoch 637 mean train loss:\t0.31953561305999756\n",
      "Epoch 637 mean val loss:\t208.12010192871094\n",
      "Epoch 638 mean train loss:\t0.24092519283294678\n",
      "Epoch 638 mean val loss:\t1943.04638671875\n",
      "Epoch 639 mean train loss:\t0.2069295197725296\n",
      "Epoch 639 mean val loss:\t753.5601806640625\n",
      "Epoch 640 mean train loss:\t0.19691458344459534\n",
      "Epoch 640 mean val loss:\t685.2590942382812\n",
      "Epoch 641 mean train loss:\t0.21529972553253174\n",
      "Epoch 641 mean val loss:\t695.58349609375\n",
      "Epoch 642 mean train loss:\t0.21133701503276825\n",
      "Epoch 642 mean val loss:\t2316.6953125\n",
      "Epoch 643 mean train loss:\t0.1934034675359726\n",
      "Epoch 643 mean val loss:\t601.35498046875\n",
      "Epoch 644 mean train loss:\t0.1894814521074295\n",
      "Epoch 644 mean val loss:\t6775.078125\n",
      "Epoch 645 mean train loss:\t0.19557125866413116\n",
      "Epoch 645 mean val loss:\t5429.43505859375\n",
      "Epoch 646 mean train loss:\t0.2213534712791443\n",
      "Epoch 646 mean val loss:\t3309.77392578125\n",
      "Epoch 647 mean train loss:\t0.19350363314151764\n",
      "Epoch 647 mean val loss:\t3772.993408203125\n",
      "Epoch 648 mean train loss:\t0.20577344298362732\n",
      "Epoch 648 mean val loss:\t6112.28662109375\n",
      "Epoch 649 mean train loss:\t0.19044578075408936\n",
      "Epoch 649 mean val loss:\t12457.3134765625\n",
      "Epoch 650 mean train loss:\t0.18956971168518066\n",
      "Epoch 650 mean val loss:\t13916.361328125\n",
      "Epoch 651 mean train loss:\t0.19074642658233643\n",
      "Epoch 651 mean val loss:\t1873.090087890625\n",
      "Epoch 652 mean train loss:\t0.1867753267288208\n",
      "Epoch 652 mean val loss:\t3899.78076171875\n",
      "Epoch 653 mean train loss:\t0.1829705834388733\n",
      "Epoch 653 mean val loss:\t3679.3291015625\n",
      "Epoch 654 mean train loss:\t0.20461627840995789\n",
      "Epoch 654 mean val loss:\t10083.376953125\n",
      "Epoch 655 mean train loss:\t0.1994653195142746\n",
      "Epoch 655 mean val loss:\t6541.92529296875\n",
      "Epoch 656 mean train loss:\t0.20957502722740173\n",
      "Epoch 656 mean val loss:\t9786.48046875\n",
      "Epoch 657 mean train loss:\t0.20586945116519928\n",
      "Epoch 657 mean val loss:\t588.784912109375\n",
      "Epoch 658 mean train loss:\t0.20138534903526306\n",
      "Epoch 658 mean val loss:\t2078.1015625\n",
      "Epoch 659 mean train loss:\t0.1786361038684845\n",
      "Epoch 659 mean val loss:\t1503.6163330078125\n",
      "Epoch 660 mean train loss:\t0.19523745775222778\n",
      "Epoch 660 mean val loss:\t4470.0\n",
      "Epoch 661 mean train loss:\t0.17667260766029358\n",
      "Epoch 661 mean val loss:\t4298.26171875\n",
      "Epoch 662 mean train loss:\t0.17644241452217102\n",
      "Epoch 662 mean val loss:\t5960.59375\n",
      "Epoch 663 mean train loss:\t0.16832543909549713\n",
      "Epoch 663 mean val loss:\t4961.72998046875\n",
      "Epoch 664 mean train loss:\t0.18589377403259277\n",
      "Epoch 664 mean val loss:\t6915.82275390625\n",
      "Epoch 665 mean train loss:\t0.17634502053260803\n",
      "Epoch 665 mean val loss:\t3985.6416015625\n",
      "Epoch 666 mean train loss:\t0.16613346338272095\n",
      "Epoch 666 mean val loss:\t8775.6201171875\n",
      "Epoch 667 mean train loss:\t0.1785137802362442\n",
      "Epoch 667 mean val loss:\t12225.611328125\n",
      "Epoch 668 mean train loss:\t0.17997512221336365\n",
      "Epoch 668 mean val loss:\t1228.3797607421875\n",
      "Epoch 669 mean train loss:\t0.22284936904907227\n",
      "Epoch 669 mean val loss:\t21666.220703125\n",
      "Epoch 670 mean train loss:\t0.20975922048091888\n",
      "Epoch 670 mean val loss:\t13211.626953125\n",
      "Epoch 671 mean train loss:\t0.20804932713508606\n",
      "Epoch 671 mean val loss:\t20810.634765625\n",
      "Epoch 672 mean train loss:\t0.19517843425273895\n",
      "Epoch 672 mean val loss:\t4862.369140625\n",
      "Epoch 673 mean train loss:\t0.2726345360279083\n",
      "Epoch 673 mean val loss:\t19427.142578125\n",
      "Epoch 674 mean train loss:\t0.23889023065567017\n",
      "Epoch 674 mean val loss:\t25959.455078125\n",
      "Epoch 675 mean train loss:\t0.23195187747478485\n",
      "Epoch 675 mean val loss:\t32591.4453125\n",
      "Epoch 676 mean train loss:\t0.19274461269378662\n",
      "Epoch 676 mean val loss:\t19020.70703125\n",
      "Epoch 677 mean train loss:\t0.18801964819431305\n",
      "Epoch 677 mean val loss:\t14606.5537109375\n",
      "Epoch 678 mean train loss:\t0.18861010670661926\n",
      "Epoch 678 mean val loss:\t11322.4501953125\n",
      "Epoch 679 mean train loss:\t0.1761838048696518\n",
      "Epoch 679 mean val loss:\t23058.4140625\n",
      "Epoch 680 mean train loss:\t0.18324750661849976\n",
      "Epoch 680 mean val loss:\t26916.783203125\n",
      "Epoch 681 mean train loss:\t0.1789170503616333\n",
      "Epoch 681 mean val loss:\t19132.109375\n",
      "Epoch 682 mean train loss:\t0.1934620589017868\n",
      "Epoch 682 mean val loss:\t34564.3125\n",
      "Epoch 683 mean train loss:\t0.2692210078239441\n",
      "Epoch 683 mean val loss:\t7659.72216796875\n",
      "Epoch 684 mean train loss:\t0.251197874546051\n",
      "Epoch 684 mean val loss:\t11829.7529296875\n",
      "Epoch 685 mean train loss:\t0.20964357256889343\n",
      "Epoch 685 mean val loss:\t7959.9814453125\n",
      "Epoch 686 mean train loss:\t0.21161936223506927\n",
      "Epoch 686 mean val loss:\t7100.48828125\n",
      "Epoch 687 mean train loss:\t0.2030140906572342\n",
      "Epoch 687 mean val loss:\t3252.943603515625\n",
      "Epoch 688 mean train loss:\t0.21913614869117737\n",
      "Epoch 688 mean val loss:\t2886.0986328125\n",
      "Epoch 689 mean train loss:\t0.17902415990829468\n",
      "Epoch 689 mean val loss:\t2078.419189453125\n",
      "Epoch 690 mean train loss:\t0.18466731905937195\n",
      "Epoch 690 mean val loss:\t2939.668212890625\n",
      "Epoch 691 mean train loss:\t0.18206919729709625\n",
      "Epoch 691 mean val loss:\t4600.078125\n",
      "Epoch 692 mean train loss:\t0.16392867267131805\n",
      "Epoch 692 mean val loss:\t5201.423828125\n",
      "Epoch 693 mean train loss:\t0.15268434584140778\n",
      "Epoch 693 mean val loss:\t4644.32421875\n",
      "Epoch 694 mean train loss:\t0.18035900592803955\n",
      "Epoch 694 mean val loss:\t9131.6220703125\n",
      "Epoch 695 mean train loss:\t0.17990507185459137\n",
      "Epoch 695 mean val loss:\t10122.662109375\n",
      "Epoch 696 mean train loss:\t0.2203790545463562\n",
      "Epoch 696 mean val loss:\t382.3708801269531\n",
      "Epoch 697 mean train loss:\t0.19094446301460266\n",
      "Epoch 697 mean val loss:\t90.68895721435547\n",
      "Epoch 698 mean train loss:\t0.184418722987175\n",
      "Epoch 698 mean val loss:\t697.7238159179688\n",
      "Epoch 699 mean train loss:\t0.20728613436222076\n",
      "Epoch 699 mean val loss:\t2716.021484375\n",
      "Epoch 700 mean train loss:\t0.20317824184894562\n",
      "Epoch 700 mean val loss:\t6619.8330078125\n",
      "Epoch 701 mean train loss:\t0.20030750334262848\n",
      "Epoch 701 mean val loss:\t4337.51025390625\n",
      "Epoch 702 mean train loss:\t0.18154466152191162\n",
      "Epoch 702 mean val loss:\t1335.44873046875\n",
      "Epoch 703 mean train loss:\t0.21600009500980377\n",
      "Epoch 703 mean val loss:\t7839.6123046875\n",
      "Epoch 704 mean train loss:\t0.1960400491952896\n",
      "Epoch 704 mean val loss:\t49399.2421875\n",
      "Epoch 705 mean train loss:\t0.17907053232192993\n",
      "Epoch 705 mean val loss:\t9999.900390625\n",
      "Epoch 706 mean train loss:\t0.1881963014602661\n",
      "Epoch 706 mean val loss:\t1788.73583984375\n",
      "Epoch 707 mean train loss:\t0.1873883455991745\n",
      "Epoch 707 mean val loss:\t3159.850830078125\n",
      "Epoch 708 mean train loss:\t0.16511067748069763\n",
      "Epoch 708 mean val loss:\t2712.847900390625\n",
      "Epoch 709 mean train loss:\t0.17973564565181732\n",
      "Epoch 709 mean val loss:\t3778.572509765625\n",
      "Epoch 710 mean train loss:\t0.17834609746932983\n",
      "Epoch 710 mean val loss:\t5467.77294921875\n",
      "Epoch 711 mean train loss:\t0.1704336255788803\n",
      "Epoch 711 mean val loss:\t1478.62255859375\n",
      "Epoch 712 mean train loss:\t0.16761915385723114\n",
      "Epoch 712 mean val loss:\t2174.73876953125\n",
      "Epoch 713 mean train loss:\t0.21312862634658813\n",
      "Epoch 713 mean val loss:\t122.73694610595703\n",
      "Epoch 714 mean train loss:\t0.17188546061515808\n",
      "Epoch 714 mean val loss:\t168.50149536132812\n",
      "Epoch 715 mean train loss:\t0.17654535174369812\n",
      "Epoch 715 mean val loss:\t114.50408935546875\n",
      "Epoch 716 mean train loss:\t0.1869228482246399\n",
      "Epoch 716 mean val loss:\t111.48815155029297\n",
      "Epoch 717 mean train loss:\t0.17762769758701324\n",
      "Epoch 717 mean val loss:\t573.8477172851562\n",
      "Epoch 718 mean train loss:\t0.17821276187896729\n",
      "Epoch 718 mean val loss:\t100.84547424316406\n",
      "Epoch 719 mean train loss:\t0.19113482534885406\n",
      "Epoch 719 mean val loss:\t297.0955505371094\n",
      "Epoch 720 mean train loss:\t0.18817158043384552\n",
      "Epoch 720 mean val loss:\t257.09649658203125\n",
      "Epoch 721 mean train loss:\t0.18350428342819214\n",
      "Epoch 721 mean val loss:\t3432.58544921875\n",
      "Epoch 722 mean train loss:\t0.19985419511795044\n",
      "Epoch 722 mean val loss:\t5133.22216796875\n",
      "Epoch 723 mean train loss:\t0.18217457830905914\n",
      "Epoch 723 mean val loss:\t1333.1806640625\n",
      "Epoch 724 mean train loss:\t0.16957464814186096\n",
      "Epoch 724 mean val loss:\t623.503662109375\n",
      "Epoch 725 mean train loss:\t0.19626319408416748\n",
      "Epoch 725 mean val loss:\t102.612548828125\n",
      "Epoch 726 mean train loss:\t0.1972779929637909\n",
      "Epoch 726 mean val loss:\t151.4124755859375\n",
      "Epoch 727 mean train loss:\t0.17664270102977753\n",
      "Epoch 727 mean val loss:\t462.690673828125\n",
      "Epoch 728 mean train loss:\t0.20839159190654755\n",
      "Epoch 728 mean val loss:\t1563.0521240234375\n",
      "Epoch 729 mean train loss:\t0.17666392028331757\n",
      "Epoch 729 mean val loss:\t622.378662109375\n",
      "Epoch 730 mean train loss:\t0.18308484554290771\n",
      "Epoch 730 mean val loss:\t350.7123107910156\n",
      "Epoch 731 mean train loss:\t0.1669013351202011\n",
      "Epoch 731 mean val loss:\t386.8461608886719\n",
      "Epoch 732 mean train loss:\t0.17622941732406616\n",
      "Epoch 732 mean val loss:\t679.8621826171875\n",
      "Epoch 733 mean train loss:\t0.1798926591873169\n",
      "Epoch 733 mean val loss:\t363.87738037109375\n",
      "Epoch 734 mean train loss:\t0.16461016237735748\n",
      "Epoch 734 mean val loss:\t453.3704833984375\n",
      "Epoch 735 mean train loss:\t0.168143168091774\n",
      "Epoch 735 mean val loss:\t748.3352661132812\n",
      "Epoch 736 mean train loss:\t0.18607743084430695\n",
      "Epoch 736 mean val loss:\t128.60272216796875\n",
      "Epoch 737 mean train loss:\t0.19097256660461426\n",
      "Epoch 737 mean val loss:\t935.728271484375\n",
      "Epoch 738 mean train loss:\t0.18914981186389923\n",
      "Epoch 738 mean val loss:\t566.7811279296875\n",
      "Epoch 739 mean train loss:\t0.18066877126693726\n",
      "Epoch 739 mean val loss:\t2330.446044921875\n",
      "Epoch 740 mean train loss:\t0.17264065146446228\n",
      "Epoch 740 mean val loss:\t1653.32080078125\n",
      "Epoch 741 mean train loss:\t0.16900312900543213\n",
      "Epoch 741 mean val loss:\t1227.8284912109375\n",
      "Epoch 742 mean train loss:\t0.17599059641361237\n",
      "Epoch 742 mean val loss:\t1480.373779296875\n",
      "Epoch 743 mean train loss:\t0.16854746639728546\n",
      "Epoch 743 mean val loss:\t282.45361328125\n",
      "Epoch 744 mean train loss:\t0.1606123447418213\n",
      "Epoch 744 mean val loss:\t857.318359375\n",
      "Epoch 745 mean train loss:\t0.16605454683303833\n",
      "Epoch 745 mean val loss:\t471.3855895996094\n",
      "Epoch 746 mean train loss:\t0.17439761757850647\n",
      "Epoch 746 mean val loss:\t2021.5244140625\n",
      "Epoch 747 mean train loss:\t0.168988436460495\n",
      "Epoch 747 mean val loss:\t2396.677734375\n",
      "Epoch 748 mean train loss:\t0.1702585220336914\n",
      "Epoch 748 mean val loss:\t2231.173583984375\n",
      "Epoch 749 mean train loss:\t0.19678516685962677\n",
      "Epoch 749 mean val loss:\t777.9798583984375\n",
      "Epoch 750 mean train loss:\t0.6842470169067383\n",
      "Epoch 750 mean val loss:\t4446.0126953125\n",
      "Epoch 751 mean train loss:\t0.6188405156135559\n",
      "Epoch 751 mean val loss:\t814.0143432617188\n",
      "Epoch 752 mean train loss:\t0.49483418464660645\n",
      "Epoch 752 mean val loss:\t6930.36669921875\n",
      "Epoch 753 mean train loss:\t0.40401291847229004\n",
      "Epoch 753 mean val loss:\t4851.099609375\n",
      "Epoch 754 mean train loss:\t0.3236156105995178\n",
      "Epoch 754 mean val loss:\t7440.5791015625\n",
      "Epoch 755 mean train loss:\t0.2694666385650635\n",
      "Epoch 755 mean val loss:\t7186.6591796875\n",
      "Epoch 756 mean train loss:\t0.30822983384132385\n",
      "Epoch 756 mean val loss:\t4646.1201171875\n",
      "Epoch 757 mean train loss:\t0.28801095485687256\n",
      "Epoch 757 mean val loss:\t13050.1044921875\n",
      "Epoch 758 mean train loss:\t0.25057128071784973\n",
      "Epoch 758 mean val loss:\t105611.921875\n",
      "Epoch 759 mean train loss:\t0.21680320799350739\n",
      "Epoch 759 mean val loss:\t80820.625\n",
      "Epoch 760 mean train loss:\t0.2163359671831131\n",
      "Epoch 760 mean val loss:\t62948.19140625\n",
      "Epoch 761 mean train loss:\t0.20517270267009735\n",
      "Epoch 761 mean val loss:\t64899.69140625\n",
      "Epoch 762 mean train loss:\t0.20854298770427704\n",
      "Epoch 762 mean val loss:\t116440.78125\n",
      "Epoch 763 mean train loss:\t0.18996912240982056\n",
      "Epoch 763 mean val loss:\t72096.2734375\n",
      "Epoch 764 mean train loss:\t0.19538375735282898\n",
      "Epoch 764 mean val loss:\t88917.0234375\n",
      "Epoch 765 mean train loss:\t0.19000500440597534\n",
      "Epoch 765 mean val loss:\t34078.2265625\n",
      "Epoch 766 mean train loss:\t0.17633795738220215\n",
      "Epoch 766 mean val loss:\t22028.369140625\n",
      "Epoch 767 mean train loss:\t0.17234668135643005\n",
      "Epoch 767 mean val loss:\t27121.541015625\n",
      "Epoch 768 mean train loss:\t0.18611299991607666\n",
      "Epoch 768 mean val loss:\t30257.177734375\n",
      "Epoch 769 mean train loss:\t0.18036600947380066\n",
      "Epoch 769 mean val loss:\t4986.861328125\n",
      "Epoch 770 mean train loss:\t0.1820584237575531\n",
      "Epoch 770 mean val loss:\t5030.24072265625\n",
      "Epoch 771 mean train loss:\t0.19215534627437592\n",
      "Epoch 771 mean val loss:\t10046.9775390625\n",
      "Epoch 772 mean train loss:\t0.18447673320770264\n",
      "Epoch 772 mean val loss:\t11635.6435546875\n",
      "Epoch 773 mean train loss:\t0.17564626038074493\n",
      "Epoch 773 mean val loss:\t9081.1015625\n",
      "Epoch 774 mean train loss:\t0.1576649248600006\n",
      "Epoch 774 mean val loss:\t8360.671875\n",
      "Epoch 775 mean train loss:\t0.1739812046289444\n",
      "Epoch 775 mean val loss:\t10920.87890625\n",
      "Epoch 776 mean train loss:\t0.17942607402801514\n",
      "Epoch 776 mean val loss:\t18385.822265625\n",
      "Epoch 777 mean train loss:\t0.17493683099746704\n",
      "Epoch 777 mean val loss:\t28290.939453125\n",
      "Epoch 778 mean train loss:\t0.17340518534183502\n",
      "Epoch 778 mean val loss:\t7904.92724609375\n",
      "Epoch 779 mean train loss:\t0.18535396456718445\n",
      "Epoch 779 mean val loss:\t13050.564453125\n",
      "Epoch 780 mean train loss:\t0.177459716796875\n",
      "Epoch 780 mean val loss:\t18556.501953125\n",
      "Epoch 781 mean train loss:\t0.17307695746421814\n",
      "Epoch 781 mean val loss:\t16404.869140625\n",
      "Epoch 782 mean train loss:\t0.1673988699913025\n",
      "Epoch 782 mean val loss:\t6326.20166015625\n",
      "Epoch 783 mean train loss:\t0.16000768542289734\n",
      "Epoch 783 mean val loss:\t3477.276611328125\n",
      "Epoch 784 mean train loss:\t0.16966421902179718\n",
      "Epoch 784 mean val loss:\t4681.61962890625\n",
      "Epoch 785 mean train loss:\t0.1688930094242096\n",
      "Epoch 785 mean val loss:\t4760.88916015625\n",
      "Epoch 786 mean train loss:\t0.1725977510213852\n",
      "Epoch 786 mean val loss:\t3651.966552734375\n",
      "Epoch 787 mean train loss:\t0.15546873211860657\n",
      "Epoch 787 mean val loss:\t7014.0625\n",
      "Epoch 788 mean train loss:\t0.1803632527589798\n",
      "Epoch 788 mean val loss:\t11917.705078125\n",
      "Epoch 789 mean train loss:\t0.16107769310474396\n",
      "Epoch 789 mean val loss:\t9203.2275390625\n",
      "Epoch 790 mean train loss:\t0.18394434452056885\n",
      "Epoch 790 mean val loss:\t10166.244140625\n",
      "Epoch 791 mean train loss:\t0.176887646317482\n",
      "Epoch 791 mean val loss:\t5939.4697265625\n",
      "Epoch 792 mean train loss:\t0.1750546246767044\n",
      "Epoch 792 mean val loss:\t5706.15185546875\n",
      "Epoch 793 mean train loss:\t0.16655103862285614\n",
      "Epoch 793 mean val loss:\t4171.2587890625\n",
      "Epoch 794 mean train loss:\t0.16048666834831238\n",
      "Epoch 794 mean val loss:\t6180.19140625\n",
      "Epoch 795 mean train loss:\t0.16307976841926575\n",
      "Epoch 795 mean val loss:\t3859.987548828125\n",
      "Epoch 796 mean train loss:\t0.16731801629066467\n",
      "Epoch 796 mean val loss:\t5285.791015625\n",
      "Epoch 797 mean train loss:\t0.19734838604927063\n",
      "Epoch 797 mean val loss:\t8911.048828125\n",
      "Epoch 798 mean train loss:\t0.18778404593467712\n",
      "Epoch 798 mean val loss:\t13697.6474609375\n",
      "Epoch 799 mean train loss:\t0.17180277407169342\n",
      "Epoch 799 mean val loss:\t16526.140625\n",
      "Epoch 800 mean train loss:\t0.1573609560728073\n",
      "Epoch 800 mean val loss:\t13074.7109375\n",
      "Epoch 801 mean train loss:\t0.202259361743927\n",
      "Epoch 801 mean val loss:\t11597.8291015625\n",
      "Epoch 802 mean train loss:\t0.21061217784881592\n",
      "Epoch 802 mean val loss:\t45454.9140625\n",
      "Epoch 803 mean train loss:\t0.19005358219146729\n",
      "Epoch 803 mean val loss:\t3983.558349609375\n",
      "Epoch 804 mean train loss:\t0.21500609815120697\n",
      "Epoch 804 mean val loss:\t8218.6728515625\n",
      "Epoch 805 mean train loss:\t0.18481898307800293\n",
      "Epoch 805 mean val loss:\t2911.103271484375\n",
      "Epoch 806 mean train loss:\t0.18184763193130493\n",
      "Epoch 806 mean val loss:\t2034.1658935546875\n",
      "Epoch 807 mean train loss:\t0.16422139108181\n",
      "Epoch 807 mean val loss:\t1842.4849853515625\n",
      "Epoch 808 mean train loss:\t0.15577901899814606\n",
      "Epoch 808 mean val loss:\t1181.4830322265625\n",
      "Epoch 809 mean train loss:\t0.16499637067317963\n",
      "Epoch 809 mean val loss:\t582.52880859375\n",
      "Epoch 810 mean train loss:\t0.1646599918603897\n",
      "Epoch 810 mean val loss:\t1484.277587890625\n",
      "Epoch 811 mean train loss:\t0.15751220285892487\n",
      "Epoch 811 mean val loss:\t3013.3642578125\n",
      "Patience exhausted in epoch 811. Best val-loss was 1.6835530996322632\n",
      "Using best model from epoch 10 which had loss 1.6835530996322632\n",
      "Saved model as /home/mgauch/runoff-nn/src/../pickle/models/STGCN_simulationTraining_allStations_20190825-153140.pkl\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    train_losses = torch.tensor(0.0)\n",
    "    for i, train_batch in enumerate(train_dataloader):\n",
    "        # in each batch, train on a random subset of connected component to foster generalization\n",
    "        component_idxs = sorted(n for comp in np.random.choice(train_component_indices, size=len(train_component_indices)//3, replace=False) for n in comp)\n",
    "        mask = train_non_constant_subbasin_mask[component_idxs]\n",
    "        adj = train_adjacency[:,component_idxs][:,:,component_idxs]\n",
    "        y_pred = model(train_batch['x'].permute(0,2,1,3)[...,component_idxs].to(device), adj)\n",
    "        train_loss = loss_fn(y_pred[:,mask], train_batch['y_sim'][:,component_idxs][:,mask].to(device), means=y_train_means[component_idxs][mask])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses += train_loss.detach()\n",
    "        \n",
    "    train_loss = (train_losses / len(train_dataloader)).item()\n",
    "    print('Epoch', epoch, 'mean train loss:\\t{}'.format(train_loss))\n",
    "    writer.add_scalar('loss_nse', train_loss, epoch)\n",
    "    \n",
    "    model.eval()\n",
    "    val_losses = torch.tensor(0.0)\n",
    "    for i, val_batch in enumerate(val_dataloader):\n",
    "        y_pred = model(val_batch['x'].permute(0,2,1,3).to(device), val_adjacency).detach()\n",
    "        val_losses += loss_fn(y_pred[:,val_non_constant_subbasin_mask], val_batch['y_sim'][:,val_non_constant_subbasin_mask].to(device), means=y_val_means).detach()\n",
    "        \n",
    "    val_loss = (val_losses / len(val_dataloader)).item()\n",
    "    print('Epoch', epoch, 'mean val loss:\\t{}'.format(val_loss))\n",
    "    writer.add_scalar('loss_nse_val', val_loss, epoch)\n",
    "    \n",
    "    if val_loss < best_loss_model[1] - min_improvement:\n",
    "        best_loss_model = (epoch, val_loss, model.state_dict())  # new best model\n",
    "        load_data.pickle_model('STGCN_simulationTraining', model, 'allStations', time_stamp, model_type='torch.dill')\n",
    "    elif epoch > best_loss_model[0] + patience:\n",
    "        print('Patience exhausted in epoch {}. Best val-loss was {}'.format(epoch, best_loss_model[1]))\n",
    "        break\n",
    "    \n",
    "print('Using best model from epoch', str(best_loss_model[0]), 'which had loss', str(best_loss_model[1]))\n",
    "model.load_state_dict(best_loss_model[2])\n",
    "load_data.save_model_with_state('STGCN_simulationTraining', best_loss_model[0], model, optimizer, time_stamp, use_dill=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "del y_train_means, y_val_means, y_pred, train_non_constant_subbasin_mask, val_non_constant_subbasin_mask\n",
    "if USE_CUDA:\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-25 17:02:35,632 - 20190825-153140 - predicting\n"
     ]
    }
   ],
   "source": [
    "logger.warning('predicting')\n",
    "model.eval()\n",
    "\n",
    "temporal_test_predictions = []  # test on train graph but different time\n",
    "for i, test_batch in enumerate(temporal_test_dataloader):\n",
    "    pred = model(test_batch['x'].permute(0,2,1,3).to(device), train_adjacency).detach().cpu()\n",
    "    temporal_test_predictions.append(pred)\n",
    "predictions = torch.cat(temporal_test_predictions)\n",
    "\n",
    "if spatial_test_dataset is not None:\n",
    "    spatial_test_predictions = []  # test on different graph, different time\n",
    "    for i, test_batch in enumerate(spatial_test_dataloader):\n",
    "        pred = model(test_batch['x'].permute(0,2,1,3).to(device), test_adjacency).detach().cpu()\n",
    "        spatial_test_predictions.append(pred)\n",
    "    predictions = torch.cat([torch.cat(spatial_test_predictions), predictions], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mgauch/miniconda3/envs/gwf/lib/python3.7/site-packages/pandas/plotting/_converter.py:129: FutureWarning: Using an implicitly registered datetime converter for a matplotlib plotting method. The converter was registered by pandas on import. Future versions of pandas will require you to explicitly register matplotlib converters.\n",
      "\n",
      "To register the converters:\n",
      "\t>>> from pandas.plotting import register_matplotlib_converters\n",
      "\t>>> register_matplotlib_converters()\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 \tNSE sim: -1.1728235785168253 \tMSE sim: 2513800.289159647\n",
      "2 \tNSE sim: -1529119.5303730466 \tMSE sim: 129787908.05924524\n",
      "3 \tNSE sim: -4087297.386058091 \tMSE sim: 130065836.90665291\n",
      "4 \tNSE sim: -5743416.219003963 \tMSE sim: 97865045.55232784\n",
      "5 \tNSE sim: -4005407.8437892254 \tMSE sim: 97149003.67167918\n",
      "6 \tNSE sim: -21614876.717993006 \tMSE sim: 125427758.5215138\n",
      "7 \tNSE sim: -45822113.24890118 \tMSE sim: 127047319.11132002\n",
      "8 \tNSE sim: -3283891.4123559785 \tMSE sim: 24998605.640492637\n",
      "9 \tNSE sim: -1397079.8575790976 \tMSE sim: 5670921.495867866\n",
      "10 \tNSE sim: -981142.0882485353 \tMSE sim: 5592353.957787357\n",
      "11 \tNSE sim: -2080855.836507969 \tMSE sim: 25392991.719113197\n",
      "12 \tNSE sim: -1971237.9737317069 \tMSE sim: 5800952.742675789\n",
      "13 \tNSE sim: -33013940.616231926 \tMSE sim: 126701260.04170933\n",
      "14 \tNSE sim: -478497.1137755881 \tMSE sim: 128384383.68505672\n",
      "15 \tNSE sim: -49297.66212299163 \tMSE sim: 5941801.480728056\n",
      "16 \tNSE sim: -404174.1583298157 \tMSE sim: 5913490.086078478\n",
      "17 \tNSE sim: -273595.0949743073 \tMSE sim: 25688809.21668466\n",
      "18 \tNSE sim: -17217.225487291213 \tMSE sim: 123772740.54727738\n",
      "19 \tNSE sim: -4358485.599483517 \tMSE sim: 25689920.082232296\n",
      "20 \tNSE sim: -99884.67614472414 \tMSE sim: 5907672.252128672\n",
      "21 \tNSE sim: -13600.554106220981 \tMSE sim: 95815697.10394502\n",
      "22 \tNSE sim: -1565816.7997223726 \tMSE sim: 7204744.362956206\n",
      "23 \tNSE sim: -769927.7996928185 \tMSE sim: 7161408.319411603\n",
      "24 \tNSE sim: -385963.3244477527 \tMSE sim: 1929723.5155948321\n",
      "25 \tNSE sim: -951.9396387811418 \tMSE sim: 5813300.617418676\n",
      "26 \tNSE sim: -212464.0686303891 \tMSE sim: 6202076.165197448\n",
      "27 \tNSE sim: -4040685.5486025945 \tMSE sim: 25216222.687471606\n",
      "28 \tNSE sim: -3291630.8195672277 \tMSE sim: 25205618.93612627\n",
      "29 \tNSE sim: -5840930.931559939 \tMSE sim: 25923541.651607513\n",
      "30 \tNSE sim: -4385.16128150169 \tMSE sim: 24931517.24373375\n",
      "31 \tNSE sim: -193927.83733979607 \tMSE sim: 6030841.574820828\n",
      "32 \tNSE sim: -1198.3346635760438 \tMSE sim: 5964518.234677473\n",
      "33 \tNSE sim: -19540815.470836718 \tMSE sim: 125621021.81856398\n",
      "34 \tNSE sim: -645487.3323446262 \tMSE sim: 2025970.5453050365\n",
      "35 \tNSE sim: -485864.1752174133 \tMSE sim: 1998604.0433346254\n",
      "36 \tNSE sim: -41603090.16714253 \tMSE sim: 126977600.66573273\n",
      "37 \tNSE sim: -1081483.377169199 \tMSE sim: 129646510.47576328\n",
      "38 \tNSE sim: -67049.68596958487 \tMSE sim: 5567114.455912279\n",
      "39 \tNSE sim: -64920.472198257645 \tMSE sim: 2101643.0739154425\n",
      "40 \tNSE sim: -470.9442220439216 \tMSE sim: 1964344.56365821\n",
      "41 \tNSE sim: -26440437.891093634 \tMSE sim: 126330424.53569679\n",
      "42 \tNSE sim: -7766.227820875523 \tMSE sim: 2164186.8498884737\n",
      "43 \tNSE sim: -603635.2320879454 \tMSE sim: 25860585.767493803\n",
      "44 \tNSE sim: -880.4127031004024 \tMSE sim: 2071081.2355607164\n",
      "45 \tNSE sim: -2830509.7386015495 \tMSE sim: 129374380.81339328\n",
      "46 \tNSE sim: -286307.5486494044 \tMSE sim: 5838594.619128053\n",
      "47 \tNSE sim: -214.9337916102117 \tMSE sim: 49554.746264297035\n",
      "48 \tNSE sim: -198278.48343351597 \tMSE sim: 2142916.0297845486\n",
      "49 \tNSE sim: -165338.73346645647 \tMSE sim: 404907.91003047704\n",
      "50 \tNSE sim: -168.87892705788218 \tMSE sim: 370976.75103164976\n",
      "51 \tNSE sim: -31898.670765431674 \tMSE sim: 735705.9083067374\n",
      "52 \tNSE sim: -916948.8132491602 \tMSE sim: 7260021.594020001\n",
      "53 \tNSE sim: -1349469.2890946213 \tMSE sim: 97894236.78734173\n",
      "54 \tNSE sim: -18412173.907717343 \tMSE sim: 97944195.77238339\n",
      "55 \tNSE sim: -621.7448770385149 \tMSE sim: 60247.52671782311\n",
      "56 \tNSE sim: -27.575616711514904 \tMSE sim: 39848.72819836769\n",
      "57 \tNSE sim: -211129.02890439265 \tMSE sim: 5979397.693256168\n",
      "58 \tNSE sim: -26.415755833215833 \tMSE sim: 5040.2840543070115\n",
      "59 \tNSE sim: -1.5450005895775112 \tMSE sim: 352.5661688795466\n",
      "60 \tNSE sim: -5887997.602436985 \tMSE sim: 97472727.6472017\n",
      "61 \tNSE sim: -1.5806973191844786 \tMSE sim: 3273.5295363359637\n",
      "62 \tNSE sim: -1330319.9096516767 \tMSE sim: 25176245.21469993\n",
      "63 \tNSE sim: -136328.35059851536 \tMSE sim: 5902018.643118345\n",
      "65 \tNSE sim: -59.502436977970795 \tMSE sim: 693.7300913419181\n",
      "67 \tNSE sim: -27.744134544766162 \tMSE sim: 420.26045150945527\n",
      "68 \tNSE sim: -0.21229185136782736 \tMSE sim: 118.18988629392585\n",
      "69 \tNSE sim: -1563777.845633181 \tMSE sim: 5924071.359844228\n",
      "70 \tNSE sim: -1879291.3593526084 \tMSE sim: 5825189.064435122\n",
      "71 \tNSE sim: -296087.782700315 \tMSE sim: 5711510.044993619\n",
      "72 \tNSE sim: -0.9892105630075048 \tMSE sim: 378.16357318342466\n",
      "73 \tNSE sim: 0.05320756679684935 \tMSE sim: 372.41487221814833\n",
      "74 \tNSE sim: -7.336021341870261 \tMSE sim: 23.220291914699285\n",
      "75 \tNSE sim: -2.2018710167169475 \tMSE sim: 33.89711475886459\n",
      "76 \tNSE sim: -0.08252271060570604 \tMSE sim: 257.2544702796075\n",
      "77 \tNSE sim: -1616472.4518888644 \tMSE sim: 7159274.236238733\n",
      "78 \tNSE sim: -0.3110247900118268 \tMSE sim: 83.74591309802707\n",
      "81 \tNSE sim: -0.12030937525951546 \tMSE sim: 143.5411773045419\n",
      "83 \tNSE sim: -0.2714390426223785 \tMSE sim: 158.02035207131377\n",
      "84 \tNSE sim: 0.19807139540281804 \tMSE sim: 7.0231261776451515\n",
      "86 \tNSE sim: 0.018706976627565486 \tMSE sim: 9.22497355544977\n",
      "87 \tNSE sim: 0.058007454426034166 \tMSE sim: 7.71952126451118\n",
      "92 \tNSE sim: 0.20315545078745811 \tMSE sim: 3.722150138195339\n",
      "93 \tNSE sim: 0.27607507934310294 \tMSE sim: 3.5940509747882845\n",
      "94 \tNSE sim: 0.005647294662472713 \tMSE sim: 4.342341822931633\n",
      "95 \tNSE sim: -0.40203871008671 \tMSE sim: 64.23791809158273\n",
      "100 \tNSE sim: -0.29164414055331545 \tMSE sim: 38.14518864075661\n",
      "105 \tNSE sim: -0.06973566795569974 \tMSE sim: 7.462657067809728\n",
      "474 \tNSE sim: -0.562866381976374 \tMSE sim: 1828961.902094879\n",
      "476 \tNSE sim: -0.2778034612495335 \tMSE sim: 0.5604462878124817\n",
      "477 \tNSE sim: -0.4630502231502647 \tMSE sim: 164.38441312909913\n",
      "478 \tNSE sim: -0.20494809622478094 \tMSE sim: 123.25460075118981\n",
      "480 \tNSE sim: -1.4101662766324208 \tMSE sim: 24.179719029669325\n",
      "481 \tNSE sim: -44734.14504236574 \tMSE sim: 262.6001850057003\n",
      "552 \tNSE sim: -62435.83526707268 \tMSE sim: 736891.9110571066\n",
      "553 \tNSE sim: -82.5007046659826 \tMSE sim: 602877.1503093138\n",
      "554 \tNSE sim: -511440.97457782447 \tMSE sim: 737630.6487445973\n",
      "555 \tNSE sim: -3764054.8855356877 \tMSE sim: 746102.1496614375\n",
      "556 \tNSE sim: -3078.5542907609856 \tMSE sim: 714693.7675992774\n",
      "557 \tNSE sim: -186941.80676084498 \tMSE sim: 729387.7054471417\n",
      "558 \tNSE sim: -233159.00560567353 \tMSE sim: 732859.5993159428\n",
      "559 \tNSE sim: -136766.4448550238 \tMSE sim: 725571.7458165893\n",
      "561 \tNSE sim: -383604.34340125776 \tMSE sim: 736595.1543275873\n",
      "563 \tNSE sim: -5100.965182796672 \tMSE sim: 722112.9147934896\n",
      "566 \tNSE sim: -34379.550050930055 \tMSE sim: 735042.4362040889\n",
      "567 \tNSE sim: -6731029.3800920965 \tMSE sim: 747009.5242144989\n",
      "569 \tNSE sim: -178234.99231148855 \tMSE sim: 726821.9001226459\n",
      "571 \tNSE sim: -4284.375846047077 \tMSE sim: 724939.9824947027\n",
      "572 \tNSE sim: -803623.5747442037 \tMSE sim: 739483.0524569611\n",
      "573 \tNSE sim: -391369.1888393081 \tMSE sim: 734829.528974066\n",
      "574 \tNSE sim: -242593.98854132486 \tMSE sim: 730989.8587667173\n",
      "575 \tNSE sim: -1704.3320903757863 \tMSE sim: 692545.7349729398\n",
      "578 \tNSE sim: -2136787.1212089444 \tMSE sim: 744472.0625930075\n",
      "580 \tNSE sim: -319689.26394577464 \tMSE sim: 731876.7673548629\n",
      "582 \tNSE sim: -2247588.8531381106 \tMSE sim: 744009.8517594087\n",
      "584 \tNSE sim: -335203.54791580857 \tMSE sim: 734173.9894594843\n",
      "585 \tNSE sim: -1374901.2317965487 \tMSE sim: 742945.8293525435\n",
      "591 \tNSE sim: -316038.67600828473 \tMSE sim: 731494.7276555217\n",
      "595 \tNSE sim: -51147.459251891894 \tMSE sim: 725937.3856347626\n",
      "598 \tNSE sim: -15971.141660028496 \tMSE sim: 738634.6552512422\n",
      "635 \tNSE sim: -0.438825906770808 \tMSE sim: 146.945185762155\n",
      "636 \tNSE sim: 0.2139656000264254 \tMSE sim: 4.329758407786983\n",
      "637 \tNSE sim: 0.03719150777881708 \tMSE sim: 36.31155470645108\n",
      "638 \tNSE sim: 0.16843611511732492 \tMSE sim: 6.297293384389676\n",
      "02GA038 676 \tNSE: -0.22365184621696432 \tMSE: 201.33778659480404 (clipped to 0)\n",
      "676 \tNSE sim: -0.6367420206977248 \tMSE sim: 62.24615963369899\n",
      "02GA047 677 \tNSE: -52.52320448899378 \tMSE: 4195.6853200236865 (clipped to 0)\n",
      "677 \tNSE sim: -46.54140275400713 \tMSE sim: 4426.212775493696\n",
      "02GA018 684 \tNSE: -0.14530529010393112 \tMSE: 287.64706625541146 (clipped to 0)\n",
      "684 \tNSE sim: -0.2997085419369323 \tMSE sim: 120.64993711754732\n",
      "02GA010 685 \tNSE: -842.430336564334 \tMSE: 402125.2742814511 (clipped to 0)\n",
      "685 \tNSE sim: -1685.4457994020813 \tMSE sim: 405284.3052241962\n",
      "02GB007 686 \tNSE: -246188.06524862873 \tMSE: 7698828.02526766 (clipped to 0)\n",
      "686 \tNSE sim: -261418.83127078562 \tMSE sim: 7697183.98465757\n",
      "02GB001 687 \tNSE: -978.5652591820088 \tMSE: 7356412.8453435935 (clipped to 0)\n",
      "687 \tNSE sim: -1530.418731276135 \tMSE sim: 7461025.559913116\n",
      "04215000 688 \tNSE: -61841.34273463738 \tMSE: 5940130.971028749 (clipped to 0)\n",
      "688 \tNSE sim: -739064.5454010186 \tMSE sim: 5949736.538823589\n",
      "02GC010 701 \tNSE: -427633.07080557843 \tMSE: 25422811.212655064 (clipped to 0)\n",
      "701 \tNSE sim: -624793.5651899276 \tMSE sim: 25422927.45167648\n",
      "02GC026 702 \tNSE: -806702.0657290388 \tMSE: 129744275.1903111 (clipped to 0)\n",
      "702 \tNSE sim: -1076931.2136495505 \tMSE sim: 129773290.0203768\n",
      "02GC007 703 \tNSE: -3243548.0084769516 \tMSE: 97700057.00202256 (clipped to 0)\n",
      "703 \tNSE sim: -847412.6918980822 \tMSE sim: 97682055.37183614\n",
      "04213500 704 \tNSE: -95546.69838723277 \tMSE: 97880671.57756999 (clipped to 0)\n",
      "704 \tNSE sim: -411036.8444853044 \tMSE sim: 98008938.63010204\n",
      "04214500 705 \tNSE: -46953.26899698353 \tMSE: 5659730.80969404 (clipped to 0)\n",
      "705 \tNSE sim: -236601.1651031466 \tMSE sim: 5664340.636846294\n",
      "04215500 706 \tNSE: -542087.9594115166 \tMSE: 97893086.7857982 (clipped to 0)\n",
      "706 \tNSE sim: -3254479.0677895527 \tMSE sim: 97928372.54781964\n",
      "04213000 710 \tNSE: -542664.3204084224 \tMSE: 97391730.66530536 (clipped to 0)\n",
      "710 \tNSE sim: -2343781.685347693 \tMSE sim: 97455615.64229345\n",
      "711 \tNSE sim: -6731255.529204493 \tMSE sim: 129437815.49292777\n",
      "112 \tNSE sim: 0.29621181798814034 \tMSE sim: 1.966921905759996\n",
      "123 \tNSE sim: 0.3682325927070218 \tMSE sim: 18.358310218345675\n",
      "124 \tNSE sim: 0.3869631397653671 \tMSE sim: 18.83316515291561\n",
      "130 \tNSE sim: 0.392830294492606 \tMSE sim: 9.237774961519076\n",
      "131 \tNSE sim: 0.3801413675795454 \tMSE sim: 8.750365654089563\n",
      "132 \tNSE sim: 0.4047373342502403 \tMSE sim: 2.7474993129106027\n",
      "135 \tNSE sim: 0.3833442093748848 \tMSE sim: 1.14518071961541\n",
      "136 \tNSE sim: 0.3795664908830323 \tMSE sim: 18.554358667228705\n",
      "137 \tNSE sim: 0.41825769210822805 \tMSE sim: 34.124049719369246\n",
      "138 \tNSE sim: 0.3743425114620407 \tMSE sim: 6.203997484575263\n",
      "139 \tNSE sim: 0.36952245644345616 \tMSE sim: 2.612932255490598\n",
      "140 \tNSE sim: 0.382690740283144 \tMSE sim: 2.264484402629069\n",
      "141 \tNSE sim: 0.39759389484447816 \tMSE sim: 13.096726583514833\n",
      "142 \tNSE sim: 0.41334190408672733 \tMSE sim: 1.4007857879550623\n",
      "143 \tNSE sim: 0.5123098022383556 \tMSE sim: 3.390573870302116\n",
      "144 \tNSE sim: 0.3613011147893388 \tMSE sim: 9.714080972870304\n",
      "145 \tNSE sim: 0.3890965709631985 \tMSE sim: 3.486124981874252\n",
      "146 \tNSE sim: 0.4954807317594919 \tMSE sim: 108.61109817331388\n",
      "147 \tNSE sim: 0.4768689656028291 \tMSE sim: 4.136326432639096\n",
      "148 \tNSE sim: 0.4150751567151888 \tMSE sim: 2.7976563490961985\n",
      "151 \tNSE sim: 0.5254921492258295 \tMSE sim: 0.9708324495266698\n",
      "152 \tNSE sim: -0.44095572888102064 \tMSE sim: 788.9525412961136\n",
      "153 \tNSE sim: 0.33401574333802475 \tMSE sim: 820.9475431736256\n",
      "154 \tNSE sim: 0.4627704619528058 \tMSE sim: 2.1396065574634253\n",
      "155 \tNSE sim: 0.397277126728934 \tMSE sim: 4.158817231216914\n",
      "156 \tNSE sim: 0.40000064892207166 \tMSE sim: 6.1115987945133226\n",
      "158 \tNSE sim: 0.48386044713368015 \tMSE sim: 3.1259393791389667\n",
      "159 \tNSE sim: 0.48925134690142413 \tMSE sim: 72.43712117125752\n",
      "160 \tNSE sim: 0.48733632015504114 \tMSE sim: 1.5653231004767303\n",
      "161 \tNSE sim: 0.36179256276822813 \tMSE sim: 679.84020991595\n",
      "162 \tNSE sim: 0.455236666311706 \tMSE sim: 2.2522994845246833\n",
      "163 \tNSE sim: 0.49429379675726115 \tMSE sim: 0.5436134777233579\n",
      "164 \tNSE sim: 0.5520355634785516 \tMSE sim: 1.3470321982894051\n",
      "165 \tNSE sim: 0.5311749568092219 \tMSE sim: 0.9130054625723729\n",
      "166 \tNSE sim: 0.4491422762181819 \tMSE sim: 1.326593194484716\n",
      "169 \tNSE sim: 0.44146551397979483 \tMSE sim: 2.14479183335067\n",
      "170 \tNSE sim: -0.08456717861151852 \tMSE sim: 38.1211998054133\n",
      "171 \tNSE sim: -0.601820876867522 \tMSE sim: 51375.51687978894\n",
      "172 \tNSE sim: 0.5175926378937724 \tMSE sim: 3.157840307321105\n",
      "173 \tNSE sim: 0.5502798233315507 \tMSE sim: 90.26485291011474\n",
      "174 \tNSE sim: 0.4369976592539123 \tMSE sim: 1.1089402468734757\n",
      "175 \tNSE sim: 0.3596321915043841 \tMSE sim: 596.1349647585932\n",
      "178 \tNSE sim: 0.6355249500683869 \tMSE sim: 2.0004495691880417\n",
      "179 \tNSE sim: 0.6066326668923414 \tMSE sim: 14.38495878199508\n",
      "180 \tNSE sim: 0.4793067563497715 \tMSE sim: 37.31109559752978\n",
      "181 \tNSE sim: 0.5384938063183071 \tMSE sim: 4.354017398107989\n",
      "182 \tNSE sim: 0.579596347244415 \tMSE sim: 0.7212458578259738\n",
      "183 \tNSE sim: 0.3915311291045137 \tMSE sim: 143.44482514363193\n",
      "184 \tNSE sim: 0.4317403027069914 \tMSE sim: 15.090524697565039\n",
      "185 \tNSE sim: 0.373911374904486 \tMSE sim: 402.51531443304077\n",
      "186 \tNSE sim: 0.6193509003170221 \tMSE sim: 1.4467363024448525\n",
      "187 \tNSE sim: 0.5448545982340477 \tMSE sim: 67.82279764458998\n",
      "188 \tNSE sim: 0.45289105275060215 \tMSE sim: 8.991567783824687\n",
      "189 \tNSE sim: 0.4457782584205766 \tMSE sim: 9.494729351172445\n",
      "191 \tNSE sim: 0.6293471547279983 \tMSE sim: 0.6438681502994529\n",
      "194 \tNSE sim: 0.5571656244563974 \tMSE sim: 1.3489459208505024\n",
      "195 \tNSE sim: 0.5400224126537803 \tMSE sim: 10.528919342888349\n",
      "198 \tNSE sim: -0.6264377041928659 \tMSE sim: 52170.16045901551\n",
      "199 \tNSE sim: 0.3901201901016864 \tMSE sim: 1.704877008112394\n",
      "200 \tNSE sim: 0.4268179425444447 \tMSE sim: 1.1746767567872913\n",
      "201 \tNSE sim: 0.4093309157285261 \tMSE sim: 2.918651163360567\n",
      "202 \tNSE sim: 0.3684831587705336 \tMSE sim: 330.23129464938086\n",
      "205 \tNSE sim: 0.4008664823747896 \tMSE sim: 3.657876023820682\n",
      "206 \tNSE sim: 0.41583373319331096 \tMSE sim: 8.365212765212867\n",
      "207 \tNSE sim: -0.6046973565254818 \tMSE sim: 51222.00953925125\n",
      "208 \tNSE sim: 0.3156790402858114 \tMSE sim: 99.57377528554146\n",
      "209 \tNSE sim: 0.37219952276803725 \tMSE sim: 59.52925143898612\n",
      "210 \tNSE sim: 0.6378871917908648 \tMSE sim: 9.015761955363379\n",
      "212 \tNSE sim: 0.4590489294168488 \tMSE sim: 1.7758095546936825\n",
      "213 \tNSE sim: 0.5884573390894023 \tMSE sim: 1.3787176509590007\n",
      "214 \tNSE sim: 0.564754915373074 \tMSE sim: 0.40837482502290545\n",
      "215 \tNSE sim: 0.505239855030565 \tMSE sim: 0.3968162506855444\n",
      "216 \tNSE sim: 0.5649804077233432 \tMSE sim: 74.34371467941257\n",
      "217 \tNSE sim: 0.5840303399850879 \tMSE sim: 22.999509666393166\n",
      "218 \tNSE sim: 0.5942040544074823 \tMSE sim: 4.108680978366051\n",
      "219 \tNSE sim: 0.3717613773924332 \tMSE sim: 41.006913095346434\n",
      "221 \tNSE sim: 0.6456085070507902 \tMSE sim: 4.836910278775659\n",
      "222 \tNSE sim: 0.39213091159419355 \tMSE sim: 2.012595627830307\n",
      "223 \tNSE sim: 0.5281345769976569 \tMSE sim: 14.773062883801005\n",
      "224 \tNSE sim: 0.3751041502266199 \tMSE sim: 18625.13950908011\n",
      "225 \tNSE sim: 0.5935055937003111 \tMSE sim: 1.6926541362296668\n",
      "226 \tNSE sim: 0.5729348268087929 \tMSE sim: 0.4223462289921307\n",
      "227 \tNSE sim: 0.38620299378237277 \tMSE sim: 65.78945533328354\n",
      "228 \tNSE sim: 0.5042472040905381 \tMSE sim: 5.0751463585321215\n",
      "229 \tNSE sim: 0.5174796186964234 \tMSE sim: 2.411962313528473\n",
      "230 \tNSE sim: 0.5956055763861166 \tMSE sim: 8.71376214324987\n",
      "231 \tNSE sim: 0.5696864114481022 \tMSE sim: 2.4681282463819594\n",
      "232 \tNSE sim: 0.5466662307104169 \tMSE sim: 0.7988676031509208\n",
      "233 \tNSE sim: 0.36941026245750985 \tMSE sim: 18489.55126128112\n",
      "234 \tNSE sim: 0.5164282592461946 \tMSE sim: 5.846617817509565\n",
      "235 \tNSE sim: 0.39004064377122105 \tMSE sim: 17236.76487597077\n",
      "236 \tNSE sim: 0.4013293218078594 \tMSE sim: 23.267909429004394\n",
      "237 \tNSE sim: 0.5917208155557028 \tMSE sim: 2.1733447863926023\n",
      "238 \tNSE sim: 0.5722631983858714 \tMSE sim: 1.5963470826358832\n",
      "239 \tNSE sim: -0.5873498473272991 \tMSE sim: 44002.10126231509\n",
      "240 \tNSE sim: 0.4976236888364247 \tMSE sim: 1.9008432714270254\n",
      "241 \tNSE sim: 0.5880889353868346 \tMSE sim: 0.6385722400002877\n",
      "242 \tNSE sim: 0.5908749118610099 \tMSE sim: 0.4089852557572094\n",
      "243 \tNSE sim: 0.563483471751734 \tMSE sim: 38.36863888312968\n",
      "244 \tNSE sim: 0.5805718172584121 \tMSE sim: 1.9536729240972148\n",
      "245 \tNSE sim: 0.6588436605696079 \tMSE sim: 2.2327933113773013\n",
      "246 \tNSE sim: 0.4049885227360849 \tMSE sim: 13.242134061424709\n",
      "247 \tNSE sim: 0.42714543883759537 \tMSE sim: 8.417351851605764\n",
      "253 \tNSE sim: 0.5424090613190988 \tMSE sim: 0.572356822311573\n",
      "254 \tNSE sim: 0.5789189595552416 \tMSE sim: 28.52901354097423\n",
      "255 \tNSE sim: 0.49612942992450604 \tMSE sim: 2.7056751241521404\n",
      "256 \tNSE sim: 0.40088876959969855 \tMSE sim: 16461.56380919107\n",
      "257 \tNSE sim: 0.43564626215812285 \tMSE sim: 1.6293015097401304\n",
      "262 \tNSE sim: 0.3800242426974215 \tMSE sim: 4.695309066539011\n",
      "263 \tNSE sim: 0.3801495337143367 \tMSE sim: 1.995700817769713\n",
      "266 \tNSE sim: 0.5244193278322782 \tMSE sim: 0.6828865858316109\n",
      "267 \tNSE sim: -0.5891658744904946 \tMSE sim: 42364.17668487866\n",
      "272 \tNSE sim: 0.5936604565514121 \tMSE sim: 6.72896624559738\n",
      "273 \tNSE sim: 0.5704204440980858 \tMSE sim: 0.3702744270366302\n",
      "274 \tNSE sim: 0.6261780885245711 \tMSE sim: 0.7951125125064553\n",
      "279 \tNSE sim: 0.5888356546533746 \tMSE sim: 2.92062404279026\n",
      "280 \tNSE sim: 0.5662038095563482 \tMSE sim: 0.7067466645593836\n",
      "286 \tNSE sim: 0.41239038310008413 \tMSE sim: 2.355504652322102\n",
      "295 \tNSE sim: 0.37747121737079303 \tMSE sim: 3.9091469546429587\n",
      "296 \tNSE sim: 0.44125545395022836 \tMSE sim: 4490.314465941754\n",
      "297 \tNSE sim: 0.43737117033705175 \tMSE sim: 3210.894665311107\n",
      "298 \tNSE sim: -0.47701296631256196 \tMSE sim: 7673.327969515405\n",
      "299 \tNSE sim: 0.6325995220593754 \tMSE sim: 121.25113670828084\n",
      "300 \tNSE sim: 0.41184963858259105 \tMSE sim: 3063.176794649544\n",
      "301 \tNSE sim: 0.5490311246708998 \tMSE sim: 5.653874729005673\n",
      "306 \tNSE sim: 0.6550201812996566 \tMSE sim: 82.93954283814999\n",
      "308 \tNSE sim: 0.6199815128124304 \tMSE sim: 1.9665054820736456\n",
      "310 \tNSE sim: 0.6805262769464893 \tMSE sim: 52.344479554245154\n",
      "311 \tNSE sim: 0.6023169646079449 \tMSE sim: 3.0380567496781286\n",
      "313 \tNSE sim: 0.5214006441254198 \tMSE sim: 2.6955248221805106\n",
      "316 \tNSE sim: 0.5939646319329301 \tMSE sim: 2064.975580661791\n",
      "317 \tNSE sim: 0.37387230699646834 \tMSE sim: 2392.6728509032773\n",
      "318 \tNSE sim: 0.527705781784534 \tMSE sim: 6.0599570097958875\n",
      "319 \tNSE sim: 0.6734275901328408 \tMSE sim: 36.64733650278335\n",
      "320 \tNSE sim: 0.6092790917478919 \tMSE sim: 1.0871406514208344\n",
      "321 \tNSE sim: 0.3781848988826487 \tMSE sim: 1361.4660484612139\n",
      "322 \tNSE sim: 0.43374840396231795 \tMSE sim: 139.25137590289532\n",
      "327 \tNSE sim: 0.6943772005106627 \tMSE sim: 24.201773129090597\n",
      "328 \tNSE sim: 0.637055534298119 \tMSE sim: 0.8667369034372245\n",
      "329 \tNSE sim: 0.6044382871072713 \tMSE sim: 1948.9098658649787\n",
      "330 \tNSE sim: 0.4944506738515342 \tMSE sim: 6.975847436566345\n",
      "331 \tNSE sim: 0.41831876407493473 \tMSE sim: 39.12812421261773\n",
      "332 \tNSE sim: 0.4947941889655847 \tMSE sim: 6.903328254235051\n",
      "334 \tNSE sim: 0.6217976514464186 \tMSE sim: 1788.989917283868\n",
      "336 \tNSE sim: 0.5153337010123997 \tMSE sim: 12.778613089568383\n",
      "339 \tNSE sim: 0.4833930062136259 \tMSE sim: 2.5456163076501603\n",
      "340 \tNSE sim: 0.46609438715732043 \tMSE sim: 18.303044371708804\n",
      "341 \tNSE sim: 0.30826973497760946 \tMSE sim: 392.111403142344\n",
      "342 \tNSE sim: 0.6178150686132814 \tMSE sim: 1741.8331639674236\n",
      "343 \tNSE sim: 0.3686214050086578 \tMSE sim: 317.6658293313903\n",
      "344 \tNSE sim: 0.6711687235814726 \tMSE sim: 20.90758924981105\n",
      "347 \tNSE sim: 0.4900682234721139 \tMSE sim: 1.895463255081154\n",
      "348 \tNSE sim: 0.37542299042648586 \tMSE sim: 100.61957315712921\n",
      "349 \tNSE sim: 0.4157784727516278 \tMSE sim: 53.08537726032512\n",
      "352 \tNSE sim: 0.6620573206654012 \tMSE sim: 14.874383096753176\n",
      "353 \tNSE sim: 0.6069597301144154 \tMSE sim: 1702.5937626309787\n",
      "355 \tNSE sim: 0.4716275188740058 \tMSE sim: 6.275721214266365\n",
      "356 \tNSE sim: 0.4718474620793569 \tMSE sim: 1.4189150868994982\n",
      "357 \tNSE sim: 0.42071136466296377 \tMSE sim: 6.538613386307866\n",
      "358 \tNSE sim: 0.33313017906475284 \tMSE sim: 50.19911771536756\n",
      "359 \tNSE sim: 0.4815137578502353 \tMSE sim: 1.9953375899032324\n",
      "361 \tNSE sim: 0.6635812936349529 \tMSE sim: 10.392878033084157\n",
      "362 \tNSE sim: 0.5938869875436631 \tMSE sim: 1684.572515159946\n",
      "363 \tNSE sim: 0.4993877879223976 \tMSE sim: 1.9440701879008722\n",
      "364 \tNSE sim: 0.3743125308478946 \tMSE sim: 4.004176795716752\n",
      "365 \tNSE sim: 0.2706497220448143 \tMSE sim: 299.91218812545617\n",
      "366 \tNSE sim: 0.5063531576321331 \tMSE sim: 6.301193680248295\n",
      "371 \tNSE sim: 0.6551090560787826 \tMSE sim: 7.010107058854009\n",
      "372 \tNSE sim: 0.5182636399089972 \tMSE sim: 2.0524280227439347\n",
      "373 \tNSE sim: 0.5666027320534051 \tMSE sim: 1712.2172344367978\n",
      "374 \tNSE sim: 0.3467348917744817 \tMSE sim: 43.05502792454404\n",
      "375 \tNSE sim: 0.34695013454594437 \tMSE sim: 34.81160568462885\n",
      "376 \tNSE sim: 0.28096410248864645 \tMSE sim: 192.29361578920373\n",
      "377 \tNSE sim: 0.3167697724669396 \tMSE sim: 8.119854396192613\n",
      "380 \tNSE sim: 0.6468486701872889 \tMSE sim: 5.292497057146362\n",
      "381 \tNSE sim: 0.3280154684835972 \tMSE sim: 2.172242249849366\n",
      "382 \tNSE sim: -0.4451507633995062 \tMSE sim: 274.21429268181345\n",
      "386 \tNSE sim: 0.4871365643466433 \tMSE sim: 413.9629389731553\n",
      "387 \tNSE sim: 0.5563401200656113 \tMSE sim: 536.6479202623821\n",
      "390 \tNSE sim: 0.3530621562921489 \tMSE sim: 21.116388393498287\n",
      "393 \tNSE sim: 0.40268220333235627 \tMSE sim: 26.149229521382402\n",
      "394 \tNSE sim: 0.44299795267982844 \tMSE sim: 413.330706787932\n",
      "395 \tNSE sim: 0.24185456248250625 \tMSE sim: 116.937501712296\n",
      "399 \tNSE sim: 0.6101775963718765 \tMSE sim: 2.3285579806018126\n",
      "402 \tNSE sim: 0.5641606455894738 \tMSE sim: 38.822501661252666\n",
      "403 \tNSE sim: 0.5745287250005168 \tMSE sim: 242.90563444478792\n",
      "404 \tNSE sim: 0.418761847373356 \tMSE sim: 1.8101653088732335\n",
      "405 \tNSE sim: 0.42294357437402297 \tMSE sim: 329.05152076206645\n",
      "407 \tNSE sim: 0.22694048321967153 \tMSE sim: 3.1082013252091345\n",
      "408 \tNSE sim: 0.33393218009804515 \tMSE sim: 9.55529834183204\n",
      "409 \tNSE sim: 0.37262993917927567 \tMSE sim: 10.609047020246887\n",
      "411 \tNSE sim: 0.5678141422236868 \tMSE sim: 0.5520245459977833\n",
      "414 \tNSE sim: 0.6438743049706858 \tMSE sim: 184.31563564373025\n",
      "415 \tNSE sim: 0.5772113793913969 \tMSE sim: 3.58768622945808\n",
      "416 \tNSE sim: 0.5814661788468929 \tMSE sim: 13.699954501272872\n",
      "417 \tNSE sim: 0.26950423391697176 \tMSE sim: 2.4579569261380216\n",
      "418 \tNSE sim: 0.2915687266700574 \tMSE sim: 33.5880744153328\n",
      "419 \tNSE sim: 0.6597320374129471 \tMSE sim: 159.45616563942824\n",
      "420 \tNSE sim: 0.10150899492791399 \tMSE sim: 28.305005383292535\n",
      "421 \tNSE sim: 0.4137660424072619 \tMSE sim: 222.3644859614935\n",
      "422 \tNSE sim: 0.43109874533284365 \tMSE sim: 1.9803069113772536\n",
      "423 \tNSE sim: 0.5589880618388123 \tMSE sim: 1.0245991857010541\n",
      "424 \tNSE sim: 0.5568611356382345 \tMSE sim: 5.693206775287241\n",
      "425 \tNSE sim: 0.37898904919715903 \tMSE sim: 3.031925668369177\n",
      "426 \tNSE sim: 0.32938446057736803 \tMSE sim: 2.9736691245207285\n",
      "427 \tNSE sim: 0.30390595905758655 \tMSE sim: 13.274140742764864\n",
      "428 \tNSE sim: 0.6446964141833307 \tMSE sim: 133.18514664900727\n",
      "429 \tNSE sim: -0.3433884216190861 \tMSE sim: 291.0724263503244\n",
      "430 \tNSE sim: 0.42914276171573174 \tMSE sim: 2.3822791319097556\n",
      "435 \tNSE sim: 0.6521546004193872 \tMSE sim: 64.87591354205814\n",
      "436 \tNSE sim: 0.6284459299106311 \tMSE sim: 6.337924104446477\n",
      "437 \tNSE sim: 0.35645706543679734 \tMSE sim: 109.79170798247756\n",
      "438 \tNSE sim: 0.34410050764878974 \tMSE sim: 2.4552010184236033\n",
      "439 \tNSE sim: 0.32163124201262483 \tMSE sim: 87.50608282167094\n",
      "440 \tNSE sim: 0.5951726674622091 \tMSE sim: 0.8811671057057958\n",
      "441 \tNSE sim: 0.5959334200679387 \tMSE sim: 0.9058695078700604\n",
      "442 \tNSE sim: 0.6183068760877408 \tMSE sim: 1.1365037994910197\n",
      "443 \tNSE sim: 0.6426002309367174 \tMSE sim: 32.237642111460644\n",
      "446 \tNSE sim: 0.31148319568761684 \tMSE sim: 65.4626139663461\n",
      "450 \tNSE sim: 0.6243571437876185 \tMSE sim: 9.212477864555693\n",
      "451 \tNSE sim: 0.6494743295942268 \tMSE sim: 5.615724001460694\n",
      "452 \tNSE sim: 0.3347134946011159 \tMSE sim: 43.98343798357435\n",
      "455 \tNSE sim: 0.6356108521821149 \tMSE sim: 4.248928180742942\n",
      "459 \tNSE sim: 0.3645597545650596 \tMSE sim: 24.45954507970076\n",
      "460 \tNSE sim: 0.6124881263213168 \tMSE sim: 1.1552503542910175\n",
      "461 \tNSE sim: 0.6103321518860976 \tMSE sim: 1.424655742672155\n",
      "466 \tNSE sim: 0.6157904573379478 \tMSE sim: 1.0339708799845508\n",
      "467 \tNSE sim: 0.39401575148628776 \tMSE sim: 3.1158100223548235\n",
      "468 \tNSE sim: 0.36460815261685275 \tMSE sim: 2.019650862729345\n",
      "499 \tNSE sim: nan \tMSE sim: 0.014043948204936032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mgauch/miniconda3/envs/gwf/lib/python3.7/site-packages/hydroeval/objective_functions.py:31: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  np.sum((evaluation - np.mean(evaluation)) ** 2, dtype=np.float64))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "502 \tNSE sim: 0.5413298295390215 \tMSE sim: 0.04781753371505817\n",
      "503 \tNSE sim: 0.553250969769473 \tMSE sim: 0.012056430971058525\n",
      "505 \tNSE sim: 0.5730128220389179 \tMSE sim: 1.2142091433439013\n",
      "506 \tNSE sim: 0.5946320451427964 \tMSE sim: 14.254430905155992\n",
      "507 \tNSE sim: 0.5211405627194388 \tMSE sim: 0.006669892024084307\n",
      "508 \tNSE sim: 0.5399405013482903 \tMSE sim: 1.1989297787604276\n",
      "509 \tNSE sim: 0.5709572436518431 \tMSE sim: 24.688669922929556\n",
      "511 \tNSE sim: 0.5389412834174098 \tMSE sim: 0.33583578152218324\n",
      "512 \tNSE sim: 0.45386319757078786 \tMSE sim: 128.01281721299176\n",
      "513 \tNSE sim: 0.5143465253723432 \tMSE sim: 106.6969519145301\n",
      "514 \tNSE sim: 0.6073784250487682 \tMSE sim: 0.37237193431526583\n",
      "515 \tNSE sim: 0.5980494342954072 \tMSE sim: 0.07889062256689751\n",
      "516 \tNSE sim: 0.5508268066215025 \tMSE sim: 0.17305398784576798\n",
      "517 \tNSE sim: 0.5504261805362999 \tMSE sim: 0.014439148560281553\n",
      "518 \tNSE sim: 0.5324716578904133 \tMSE sim: 0.004263949436433755\n",
      "519 \tNSE sim: 0.46718655359603767 \tMSE sim: 1.1552806270325684\n",
      "520 \tNSE sim: 0.43793287617631027 \tMSE sim: 0.13770053444129762\n",
      "521 \tNSE sim: 0.26494282109036127 \tMSE sim: 3.2136444891281086\n",
      "522 \tNSE sim: 0.5638163030064778 \tMSE sim: 0.7169945069829804\n",
      "523 \tNSE sim: 0.543413558709342 \tMSE sim: 0.026428236993870386\n",
      "524 \tNSE sim: 0.5408484367029769 \tMSE sim: 0.0031008291931162394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mgauch/miniconda3/envs/gwf/lib/python3.7/site-packages/hydroeval/objective_functions.py:31: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  np.sum((evaluation - np.mean(evaluation)) ** 2, dtype=np.float64))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "525 \tNSE sim: nan \tMSE sim: 0.004029344771337211\n",
      "526 \tNSE sim: 0.35695566527265354 \tMSE sim: 0.0002498056348699059\n",
      "527 \tNSE sim: 0.5806772507353647 \tMSE sim: 0.006875483611930098\n",
      "528 \tNSE sim: 0.56106912827 \tMSE sim: 0.029646549329092914\n",
      "529 \tNSE sim: 0.5723614070429559 \tMSE sim: 0.00440795572855666\n",
      "531 \tNSE sim: -0.4852062016201266 \tMSE sim: 1.7752004413570497\n",
      "532 \tNSE sim: -0.06305986515729867 \tMSE sim: 0.34941435052560943\n",
      "533 \tNSE sim: 0.3618272466891308 \tMSE sim: 0.00125370065865796\n",
      "534 \tNSE sim: 0.5905695498383307 \tMSE sim: 0.2503142259726393\n",
      "538 \tNSE sim: -0.3958989195185587 \tMSE sim: 238.95594718301007\n",
      "540 \tNSE sim: 0.4345257277801573 \tMSE sim: 2948.6218029147167\n",
      "541 \tNSE sim: 0.6125305141688075 \tMSE sim: 209.25154248289874\n",
      "542 \tNSE sim: 0.586049662030888 \tMSE sim: 236.17833158635034\n",
      "546 \tNSE sim: 0.22013772646448448 \tMSE sim: 0.7848286528278592\n",
      "548 \tNSE sim: 0.29204850910539004 \tMSE sim: 0.042062338261075125\n",
      "549 \tNSE sim: 0.31932237862713775 \tMSE sim: 0.08910335474859483\n",
      "550 \tNSE sim: 0.4055833685655016 \tMSE sim: 0.004612206588793516\n",
      "551 \tNSE sim: 0.365968917737463 \tMSE sim: 0.011286635587564327\n",
      "583 \tNSE sim: -0.445674399997686 \tMSE sim: 374.20677277664146\n",
      "587 \tNSE sim: 0.23204098280051555 \tMSE sim: 3.4436666150763067\n",
      "588 \tNSE sim: 0.23483728641652413 \tMSE sim: 1.9025361478734446\n",
      "589 \tNSE sim: 0.48208462814818753 \tMSE sim: 0.023808658275461695\n",
      "592 \tNSE sim: 0.34618649703778914 \tMSE sim: 0.3684740967762695\n",
      "593 \tNSE sim: 0.3999675273443275 \tMSE sim: 5.892902820737419\n",
      "596 \tNSE sim: -0.5698190723298355 \tMSE sim: 858.8775690491158\n",
      "599 \tNSE sim: 0.4242953376030659 \tMSE sim: 2.527484116101004\n",
      "601 \tNSE sim: 0.4444756071891618 \tMSE sim: 0.17774521618617792\n",
      "602 \tNSE sim: -0.5744470052100721 \tMSE sim: 5.655001280524758\n",
      "603 \tNSE sim: -0.2538438548565207 \tMSE sim: 10.81129775695882\n",
      "605 \tNSE sim: -0.3061259164224015 \tMSE sim: 13.976803529827842\n",
      "606 \tNSE sim: -0.4611606790615539 \tMSE sim: 341.189114432733\n",
      "607 \tNSE sim: -0.645022691457829 \tMSE sim: 55869.44517837075\n",
      "608 \tNSE sim: -0.5088282836752247 \tMSE sim: 114.85416451672751\n",
      "609 \tNSE sim: 0.4629876080854054 \tMSE sim: 2.1449943188044345\n",
      "611 \tNSE sim: 0.16936931291007196 \tMSE sim: 0.149514156839403\n",
      "612 \tNSE sim: 0.34700020470455817 \tMSE sim: 0.24449175130869472\n",
      "614 \tNSE sim: -0.06203326460002323 \tMSE sim: 8.96277676538962\n",
      "615 \tNSE sim: 0.2101502235175673 \tMSE sim: 11.060867935779788\n",
      "616 \tNSE sim: 0.414451748594601 \tMSE sim: 1.2032111712834637\n",
      "617 \tNSE sim: -0.5464148469206378 \tMSE sim: 219.98390773671468\n",
      "618 \tNSE sim: 0.31059222777981377 \tMSE sim: 0.6247337661578791\n",
      "619 \tNSE sim: -0.25467847258995446 \tMSE sim: 1.5195630781537925\n",
      "620 \tNSE sim: 0.272792994889413 \tMSE sim: 11.162645089097081\n",
      "621 \tNSE sim: -0.5818304416945586 \tMSE sim: 2342.653512427911\n",
      "622 \tNSE sim: 0.3800778339286087 \tMSE sim: 0.3676059211905111\n",
      "647 \tNSE sim: 0.5576126824416214 \tMSE sim: 0.01546253455238858\n",
      "648 \tNSE sim: 0.5297690268325814 \tMSE sim: 0.3414020287316758\n",
      "649 \tNSE sim: 0.5799271834653346 \tMSE sim: 10.705064161478052\n",
      "650 \tNSE sim: 0.5913875027819021 \tMSE sim: 0.5342245779425564\n",
      "651 \tNSE sim: 0.5628356422832546 \tMSE sim: 15.550946090258826\n",
      "653 \tNSE sim: 0.45165474882677603 \tMSE sim: 120.8065457064245\n",
      "654 \tNSE sim: 0.5177287282749649 \tMSE sim: 95.87642469848613\n",
      "655 \tNSE sim: 0.5450484582890707 \tMSE sim: 0.30444648586363354\n",
      "656 \tNSE sim: 0.4447814243988045 \tMSE sim: 0.6851442982848026\n",
      "657 \tNSE sim: 0.48209860451413433 \tMSE sim: 0.020567902509954612\n",
      "658 \tNSE sim: 0.40607437315484174 \tMSE sim: 1.7962555935060882\n",
      "659 \tNSE sim: 0.5645938817261644 \tMSE sim: 0.5872679233294992\n",
      "660 \tNSE sim: 0.4870486357460615 \tMSE sim: 0.540745217898073\n",
      "661 \tNSE sim: 0.5488729365374581 \tMSE sim: 0.10182803990697972\n",
      "664 \tNSE sim: 0.366772476501791 \tMSE sim: 92.3417603959449\n",
      "665 \tNSE sim: 0.5215541329456908 \tMSE sim: 19.85964322402314\n",
      "666 \tNSE sim: 0.39305054283394314 \tMSE sim: 2583.0304093824393\n",
      "668 \tNSE sim: 0.4992818610137434 \tMSE sim: 34.81742498849047\n",
      "671 \tNSE sim: 0.5500780748345935 \tMSE sim: 0.13970271896142936\n",
      "673 \tNSE sim: 0.4246030634414777 \tMSE sim: 81.49683905012235\n",
      "674 \tNSE sim: 0.22542152883700173 \tMSE sim: 62.63464164884786\n",
      "04174500 707 \tNSE: -0.00944503247916173 \tMSE: 95.46749332204695 (clipped to 0)\n",
      "707 \tNSE sim: 0.590258983950935 \tMSE sim: 65.54595525101027\n",
      "04176500 709 \tNSE: 0.2623593933811922 \tMSE: 503.86327933116155 (clipped to 0)\n",
      "709 \tNSE sim: 0.6019721631450898 \tMSE sim: 205.47702361068986\n",
      "04177000 712 \tNSE: 0.20074440932166215 \tMSE: 21.539802144309206 (clipped to 0)\n",
      "712 \tNSE sim: 0.562434438339878 \tMSE sim: 1.843150950618688\n",
      "04193500 713 \tNSE: 0.22198436024359247 \tMSE: 55084.090236849246 (clipped to 0)\n",
      "713 \tNSE sim: 0.39511463715725104 \tMSE sim: 19327.121038039437\n",
      "04195820 714 \tNSE: 0.15364144536032343 \tMSE: 993.4508770413539 (clipped to 0)\n",
      "714 \tNSE sim: 0.5018429163640232 \tMSE sim: 103.35221001753557\n",
      "04198000 715 \tNSE: 0.10840452630375386 \tMSE: 6586.714524427135 (clipped to 0)\n",
      "715 \tNSE sim: 0.35521439200471605 \tMSE sim: 909.0279941854344\n",
      "04199000 716 \tNSE: 0.10468730830330952 \tMSE: 915.5497128691741 (clipped to 0)\n",
      "716 \tNSE sim: 0.3796499627506398 \tMSE sim: 83.47138682574237\n",
      "04199500 717 \tNSE: 0.11476348864131836 \tMSE: 358.20929181182197 (clipped to 0)\n",
      "717 \tNSE sim: 0.3186791799399923 \tMSE sim: 48.430983888350404\n",
      "04200500 718 \tNSE: 0.10799554194455363 \tMSE: 892.6332818678749 (clipped to 0)\n",
      "718 \tNSE sim: 0.30465221182879 \tMSE sim: 84.08418647260933\n",
      "04197100 723 \tNSE: 0.17950594813111043 \tMSE: 84.15348812786301 (clipped to 0)\n",
      "723 \tNSE sim: 0.4305210102305057 \tMSE sim: 10.24556509988318\n",
      "04196800 724 \tNSE: 0.10537655548990865 \tMSE: 267.71471218471595 (clipped to 0)\n",
      "724 \tNSE sim: 0.3314205242670406 \tMSE sim: 44.10300810996984\n"
     ]
    }
   ],
   "source": [
    "actuals = temporal_test_dataset.data_streamflow.copy()\n",
    "if len(actuals['date'].unique()) != len(predictions):\n",
    "    print('Warning: length of prediction {} and actuals {} does not match.'.format(len(predictions), len(actuals['date'].unique())))\n",
    "\n",
    "nse_dict, nse_sim_dict = {}, {}\n",
    "mse_dict, mse_sim_dict = {}, {}\n",
    "predictions_df = pd.DataFrame(columns=actuals.columns)\n",
    "predictions_df['is_test_subbasin'] = False\n",
    "all_test_subbasins = test_subbasins + train_subbasins if train_subbasins != test_subbasins else train_subbasins\n",
    "for i in range(len(all_test_subbasins)):\n",
    "    subbasin = all_test_subbasins[i]\n",
    "    station = None\n",
    "    subbasin_sim = temporal_test_dataset.simulated_streamflow[temporal_test_dataset.simulated_streamflow['subbasin'] == subbasin].set_index('date')\n",
    "    if subbasin in station_subbasins:\n",
    "        station = subbasin_sim['StationID'].values[0]\n",
    "        act = actuals[actuals['station'] == station].set_index('date')['runoff']\n",
    "    if predictions.shape[0] != subbasin_sim.shape[0]:\n",
    "        print('Warning: length of prediction {} and actuals {} does not match for subbasin {}. Ignoring excess actuals.'.format(len(predictions), len(subbasin_sim), subbasin))\n",
    "        subbasin_sim = subbasin_sim.iloc[:predictions.shape[0]]\n",
    "        if station is not None:\n",
    "            act = act.iloc[:predictions.shape[0]]\n",
    "            \n",
    "    pred = pd.DataFrame({'runoff': predictions[:,i]}, index=subbasin_sim.index)\n",
    "    pred['subbasin'] = subbasin\n",
    "    pred['station'] = station\n",
    "    pred['is_test_subbasin'] = subbasin in test_subbasins\n",
    "    predictions_df = predictions_df.append(pred.reset_index(), sort=True)\n",
    "    subbasin_type = 'test' if subbasin in test_subbasins else 'train'\n",
    "    nse_sim, mse_sim = evaluate.evaluate_daily('Sub{}'.format(subbasin), pred['runoff'], subbasin_sim['simulated_streamflow'], writer=writer, group=subbasin_type)\n",
    "    nse_sim_dict[subbasin] = nse_sim\n",
    "    mse_sim_dict[subbasin] = mse_sim\n",
    "\n",
    "    if station is not None:\n",
    "        nse, mse = evaluate.evaluate_daily(station, pred['runoff'], act, writer=writer)\n",
    "        nse_dict[subbasin] = nse\n",
    "        mse_dict[subbasin] = mse\n",
    "        print(station, subbasin, '\\tNSE:', nse, '\\tMSE:', mse, '(clipped to 0)')\n",
    "    print(subbasin, '\\tNSE sim:', nse_sim, '\\tMSE sim:', mse_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal test sim Median NSE (clipped to 0) 0.4571427978642774 / Min -0.645022691457829 / Max 0.6943772005106627\n",
      "                  Median MSE (clipped to 0) 6.0857779021546055 / Min 0.0002498056348699059 / Max 55869.44517837075\n",
      "Stations temporal test Median NSE (clipped to 0) 0.11476348864131836 / Min -0.00944503247916173 / Max 0.2623593933811922\n",
      "                       Median MSE (clipped to 0) 503.86327933116155 / Min 21.539802144309206 / Max 55084.090236849246\n",
      "Spatial test sim Median NSE (clipped to 0) -182588.39953616675 / Min -45822113.24890118 / Max 0.27607507934310294\n",
      "                 Median MSE (clipped to 0) 1879342.7088448554 / Min 0.5604462878124817 / Max 130065836.90665291\n",
      "Stations spatial test Median NSE (clipped to 0) -78694.02056093508 / Min -3243548.0084769516 / Max -0.14530529010393112\n",
      "                      Median MSE (clipped to 0) 7527620.435305627 / Min 201.33778659480404 / Max 129744275.1903111\n"
     ]
    }
   ],
   "source": [
    "def print_nse_mse(name, nse_dict, mse_dict, subbasins):\n",
    "    nses = list(nse_dict[s] for s in subbasins)\n",
    "    mses = list(mse_dict[s] for s in subbasins)\n",
    "    print(name, 'Median NSE (clipped to 0)', np.nanmedian(nses), '/ Min', np.nanmin(nses), '/ Max', np.nanmax(nses))\n",
    "    print(' ' * len(name), 'Median MSE (clipped to 0)', np.nanmedian(mses), '/ Min', np.nanmin(mses), '/ Max', np.nanmax(mses))\n",
    "    \n",
    "    return np.nanmedian(nses)\n",
    "\n",
    "nse_median_sim_temporal = print_nse_mse('Temporal test sim', nse_sim_dict, mse_sim_dict, train_subbasins)\n",
    "nse_median_stations_temporal = print_nse_mse('Stations temporal test', nse_dict, mse_dict, list(s for s in station_subbasins if s in train_subbasins))\n",
    "if spatial_test_dataset is not None:\n",
    "    nse_median_sim_spatial = print_nse_mse('Spatial test sim', nse_sim_dict, mse_sim_dict, test_subbasins)\n",
    "    nse_median_stations_spatial = print_nse_mse('Stations spatial test', nse_dict, mse_dict, list(s for s in station_subbasins if s in test_subbasins))\n",
    "    writer.add_scalar('nse_median_sim', nse_median_sim_spatial)\n",
    "    writer.add_scalar('nse_median_stations_spatial', nse_median_stations_spatial)\n",
    "writer.add_scalar('nse_median_sim_temporal', nse_median_sim_temporal)\n",
    "writer.add_scalar('nse_median_stations_temporal', nse_median_stations_temporal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{676: -0.22365184621696432,\n",
       " 677: -52.52320448899378,\n",
       " 684: -0.14530529010393112,\n",
       " 685: -842.430336564334,\n",
       " 686: -246188.06524862873,\n",
       " 687: -978.5652591820088,\n",
       " 688: -61841.34273463738,\n",
       " 701: -427633.07080557843,\n",
       " 702: -806702.0657290388,\n",
       " 703: -3243548.0084769516,\n",
       " 704: -95546.69838723277,\n",
       " 705: -46953.26899698353,\n",
       " 706: -542087.9594115166,\n",
       " 710: -542664.3204084224,\n",
       " 707: -0.00944503247916173,\n",
       " 709: 0.2623593933811922,\n",
       " 712: 0.20074440932166215,\n",
       " 713: 0.22198436024359247,\n",
       " 714: 0.15364144536032343,\n",
       " 715: 0.10840452630375386,\n",
       " 716: 0.10468730830330952,\n",
       " 717: 0.11476348864131836,\n",
       " 718: 0.10799554194455363,\n",
       " 723: 0.17950594813111043,\n",
       " 724: 0.10537655548990865}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nse_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'STGCN_simulationTraining_20190825-153140.pkl'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_df = pd.merge(predictions_df.rename({'runoff': 'prediction'}, axis=1), \n",
    "                   temporal_test_dataset.simulated_streamflow, on=['date', 'subbasin'])\n",
    "save_df = pd.merge(save_df, actuals.rename({'runoff': 'actual'}, axis=1), how='left', on=['date', 'station'])\\\n",
    "            [['date', 'subbasin', 'station', 'prediction', 'actual', 'simulated_streamflow', 'is_test_subbasin']]\n",
    "load_data.pickle_results('STGCN_simulationTraining', save_df, time_stamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[112, 123, 124, 130, 131, 132, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 151, 152, 153, 154, 155, 156, 158, 159, 160, 161, 162, 163, 164, 165, 166, 169, 170, 171, 172, 173, 174, 175, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 191, 194, 195, 198, 199, 200, 201, 202, 205, 206, 207, 208, 209, 210, 212, 213, 214, 215, 216, 217, 218, 219, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 253, 254, 255, 256, 257, 262, 263, 266, 267, 272, 273, 274, 279, 280, 286, 295, 296, 297, 298, 299, 300, 301, 306, 308, 310, 311, 313, 316, 317, 318, 319, 320, 321, 322, 327, 328, 329, 330, 331, 332, 334, 336, 339, 340, 341, 342, 343, 344, 347, 348, 349, 352, 353, 355, 356, 357, 358, 359, 361, 362, 363, 364, 365, 366, 371, 372, 373, 374, 375, 376, 377, 380, 381, 382, 386, 387, 390, 393, 394, 395, 399, 402, 403, 404, 405, 407, 408, 409, 411, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 435, 436, 437, 438, 439, 440, 441, 442, 443, 446, 450, 451, 452, 455, 459, 460, 461, 466, 467, 468, 499, 502, 503, 505, 506, 507, 508, 509, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 531, 532, 533, 534, 538, 540, 541, 542, 546, 548, 549, 550, 551, 583, 587, 588, 589, 592, 593, 596, 599, 601, 602, 603, 605, 606, 607, 608, 609, 611, 612, 614, 615, 616, 617, 618, 619, 620, 621, 622, 647, 648, 649, 650, 651, 653, 654, 655, 656, 657, 658, 659, 660, 661, 664, 665, 666, 668, 671, 673, 674, 707, 709, 712, 713, 714, 715, 716, 717, 718, 723, 724]\n",
      "[64, 66, 79, 80, 82, 85, 88, 89, 90, 91, 96, 97, 98, 99, 101, 102, 103, 104, 106, 107, 108, 109, 110, 111, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 125, 126, 127, 128, 129, 133, 134, 149, 150, 157, 167, 168, 176, 177, 190, 192, 193, 196, 197, 203, 204, 211, 220, 248, 249, 250, 251, 252, 258, 259, 260, 261, 264, 265, 268, 269, 270, 271, 275, 276, 277, 278, 281, 282, 283, 284, 285, 287, 288, 289, 290, 291, 292, 293, 294, 302, 303, 304, 305, 307, 309, 312, 314, 315, 323, 324, 325, 326, 333, 335, 337, 338, 345, 346, 350, 351, 354, 360, 367, 368, 369, 370, 378, 379, 383, 384, 385, 388, 389, 391, 392, 396, 397, 398, 400, 401, 406, 410, 412, 413, 431, 432, 433, 434, 444, 445, 447, 448, 449, 453, 454, 456, 457, 458, 462, 463, 464, 465, 469, 470, 471, 472, 473, 475, 479, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 500, 501, 504, 510, 530, 535, 536, 537, 539, 543, 544, 545, 547, 560, 562, 564, 565, 568, 570, 576, 577, 579, 581, 586, 590, 594, 597, 600, 604, 610, 613, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 639, 640, 641, 642, 643, 644, 645, 646, 652, 662, 663, 667, 669, 670, 672, 675, 678, 679, 680, 681, 682, 683, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 708, 719, 720, 721, 722]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 65, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 81, 83, 84, 86, 87, 92, 93, 94, 95, 100, 105, 474, 476, 477, 478, 480, 481, 552, 553, 554, 555, 556, 557, 558, 559, 561, 563, 566, 567, 569, 571, 572, 573, 574, 575, 578, 580, 582, 584, 585, 591, 595, 598, 635, 636, 637, 638, 676, 677, 684, 685, 686, 687, 688, 701, 702, 703, 704, 705, 706, 710, 711]\n"
     ]
    }
   ],
   "source": [
    "_ = print(train_subbasins), print(val_subbasins), print(test_subbasins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[676, 677, 684, 685, 686, 687, 688, 701, 702, 703, 704, 705, 706, 710]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(k for k in list(nse_dict.keys()) if k in test_subbasins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20190825-170346'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.now().strftime('%Y%m%d-%H%M%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
