{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConvLSTM trained on simulated streamflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt \n",
    "from datetime import datetime, timedelta\n",
    "import netCDF4 as nc\n",
    "import torch\n",
    "from torch import nn, utils\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from src import load_data, evaluate, conv_lstm, datasets, utils, stgcn\n",
    "import random\n",
    "import pickle\n",
    "import json\n",
    "import networkx as nx\n",
    "\n",
    "time_stamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "time_stamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "fhandler = logging.FileHandler(filename='../log.out', mode='a')\n",
    "chandler = logging.StreamHandler(sys.stdout)\n",
    "formatter = logging.Formatter('%(asctime)s - {} - %(message)s'.format(time_stamp))\n",
    "fhandler.setFormatter(formatter)\n",
    "chandler.setFormatter(formatter)\n",
    "logger.addHandler(fhandler)\n",
    "logger.addHandler(chandler)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = False\n",
    "if torch.cuda.is_available():\n",
    "    print('CUDA Available')\n",
    "    USE_CUDA = True\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "device = torch.device('cuda:0' if USE_CUDA else 'cpu')\n",
    "num_devices = torch.cuda.device_count() if USE_CUDA else 0\n",
    "logger.warning('cuda devices: {}'.format(list(torch.cuda.get_device_name(i) for i in range(num_devices))))\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 8\n",
    "seq_steps = 1\n",
    "train_start = datetime.strptime('2010-01-01', '%Y-%m-%d') + timedelta(days=seq_len * seq_steps)  # first day for which to make a prediction in train set\n",
    "train_end = '2012-12-31'\n",
    "test_start = '2013-01-01'\n",
    "test_end = '2014-12-31'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/simulations_shervan/subbasins.geojson', 'r') as f:\n",
    "     subbasin_shapes = json.loads(f.read())\n",
    "\n",
    "subbasin_graph = utils.create_subbasin_graph()\n",
    "component_graph = subbasin_graph.copy()\n",
    "component_graph.remove_nodes_from(['sub-1', 'sub1', 'sub474'])  # remove Lake Erie and sink to get connected components\n",
    "connected_components = list(nx.connected_components(nx.Graph(component_graph)))\n",
    "\n",
    "# Split into train/test/val regions\n",
    "test_subbasins = [1, 474]\n",
    "train_subbasins = []\n",
    "val_subbasins = []\n",
    "for component in connected_components:\n",
    "    max_x = -999\n",
    "    for node in component:\n",
    "        subbasin = list(s['properties'] for s in subbasin_shapes['features'] if 'sub' + str(s['properties']['SubId']) == node)[0]\n",
    "        max_x = max(max_x, subbasin['INSIDE_X'])\n",
    "    if max_x < -81.9:\n",
    "        train_subbasins += list(int(c[3:]) for c in component)\n",
    "    elif -80.6 > max_x and max_x >= -81.9:\n",
    "        val_subbasins += list(int(c[3:]) for c in component)\n",
    "    else:\n",
    "        test_subbasins += list(int(c[3:]) for c in component)\n",
    "        \n",
    "train_adjacency = torch.unsqueeze(torch.from_numpy(nx.to_numpy_array(subbasin_graph.subgraph(list('sub' + str(t) for t in train_subbasins)))), 0).float().to(device)\n",
    "val_adjacency = torch.unsqueeze(torch.from_numpy(nx.to_numpy_array(subbasin_graph.subgraph(list('sub' + str(t) for t in val_subbasins)))), 0).float().to(device)\n",
    "test_adjacency = torch.unsqueeze(torch.from_numpy(nx.to_numpy_array(subbasin_graph.subgraph(list('sub' + str(t) for t in test_subbasins)))), 0).float().to(device)\n",
    "\n",
    "subbasins = train_subbasins + test_subbasins + val_subbasins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdrs_vars = [4,5]\n",
    "agg = ['sum', 'minmax']\n",
    "include_month = True\n",
    "dem, landcover, soil, groundwater = True, False, False, False\n",
    "landcover_types = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.SubbasinAggregatedDataset(rdrs_vars, train_subbasins, seq_len, seq_steps, train_start, train_end, aggregate_daily=agg, include_months=include_month, \n",
    "                                                   dem=dem, landcover=landcover, soil=soil, groundwater=groundwater, landcover_types=landcover_types)\n",
    "val_dataset = datasets.SubbasinAggregatedDataset(rdrs_vars, val_subbasins, seq_len, seq_steps, train_start, train_end, aggregate_daily=agg, include_months=include_month, \n",
    "                                                 conv_scalers=train_dataset.grid_dataset.conv_scalers, dem=dem, landcover=landcover, soil=soil, groundwater=groundwater, landcover_types=landcover_types)\n",
    "\n",
    "# Two test datasets: one with spatial and temporal validation (i.e., different graph, different time), and one with only temporal validation (i.e. different time period only)\n",
    "spatial_test_dataset = datasets.SubbasinAggregatedDataset(rdrs_vars, test_subbasins, seq_len, seq_steps, test_start, test_end, aggregate_daily=agg, include_months=include_month, \n",
    "                                                          conv_scalers=train_dataset.grid_dataset.conv_scalers, dem=dem, landcover=landcover, soil=soil, groundwater=groundwater, landcover_types=landcover_types)\n",
    "temporal_test_dataset = datasets.SubbasinAggregatedDataset(rdrs_vars, train_subbasins, seq_len, seq_steps, test_start, test_end, aggregate_daily=agg, include_months=include_month, \n",
    "                                                           conv_scalers=train_dataset.grid_dataset.conv_scalers, dem=dem, landcover=landcover, soil=soil, groundwater=groundwater, landcover_types=landcover_types)\n",
    "\n",
    "station_subbasins = train_dataset.grid_dataset.simulated_streamflow[~pd.isna(train_dataset.grid_dataset.simulated_streamflow['StationID'])]['subbasin'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "num_epochs = 300\n",
    "learning_rate = 2e-3\n",
    "patience = 150\n",
    "min_improvement = 0.01\n",
    "best_loss_model = (-1, np.inf, None)\n",
    "weight_decay = 1e-5\n",
    "\n",
    "batch_size = 4\n",
    "model = stgcn.Model(train_dataset.x.shape[2]).to(device)\n",
    "if num_devices > 1:\n",
    "    model = torch.nn.DataParallel(model, device_ids=list(range(num_devices)))\n",
    "loss_fn = evaluate.NSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "writer = SummaryWriter(comment='STGCN_simulationTraining')\n",
    "param_description = {'time_stamp': time_stamp, 'batch_size': batch_size, 'loss': loss_fn, 'include_month': include_month, 'aggregate_daily': agg, 'rdrs_vars': rdrs_vars,\n",
    "                     'optimizer': optimizer, 'lr': learning_rate, 'patience': patience, 'min_improvement': min_improvement, 'x_train_shape': train_dataset.x.shape, 'x_val_shape': val_dataset.x.shape, \n",
    "                     'x_test_shape': test_dataset.x.shape, 'num_epochs': num_epochs, 'seq_len': seq_len, 'seq_steps': seq_steps, 'train_start': train_start, 'train_end': train_end, 'weight_decay': weight_decay, \n",
    "                     'landcover_types': landcover_types, 'test_start': test_start, 'test_end': test_end, 'model': str(model).replace('\\n','').replace(' ', ''),}\n",
    "writer.add_text('Parameter Description', str(param_description))\n",
    "str(param_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=True, pin_memory=True, drop_last=False)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size, shuffle=True, pin_memory=True, drop_last=False)\n",
    "\n",
    "spatial_test_dataloader = torch.utils.data.DataLoader(spatial_test_dataset, batch_size, shuffle=False, pin_memory=True, drop_last=False)\n",
    "temporal_test_dataloader = torch.utils.data.DataLoader(temporal_test_dataset, batch_size, shuffle=False, pin_memory=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subbasins with constant streamflow will divide by zero in loss calculation. Calculate loss without them.\n",
    "train_non_constant_subbasin_mask = ((train_dataset.y_sim.min(dim=0)[0] - train_dataset.y_sim.max(dim=0)[0]) != 0).to(device)\n",
    "val_non_constant_subbasin_mask = ((val_dataset.y_sim.min(dim=0)[0] - val_dataset.y_sim.max(dim=0)[0]) != 0).to(device)\n",
    "\n",
    "y_train_means = train_dataset.y_sim_means[train_non_constant_subbasin_mask].to(device)\n",
    "y_val_means = val_dataset.y_sim_means[val_non_constant_subbasin_mask].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    train_losses = torch.tensor(0.0)\n",
    "    for i, train_batch in enumerate(train_dataloader):\n",
    "        y_pred = model(train_batch['x'].permute(0,2,1,3), train_adjacency)\n",
    "        train_loss = loss_fn(y_pred[:,train_non_constant_subbasin_mask], train_batch['y_sim'][:,train_non_constant_subbasin_mask].to(device), means=y_train_means)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses += train_loss.detach()\n",
    "        \n",
    "    train_loss = (train_losses / len(train_dataloader)).item()\n",
    "    print('Epoch', epoch, 'mean train loss:\\t{}'.format(train_loss))\n",
    "    writer.add_scalar('loss_nse', train_loss, epoch)\n",
    "    \n",
    "    val_losses = torch.tensor(0.0)\n",
    "    for i, val_batch in enumerate(val_dataloader):\n",
    "        y_pred = model(val_batch['x'].permute(0,2,1,3), val_adjacency).detach()\n",
    "        val_losses += loss_fn(y_pred[:,val_non_constant_subbasin_mask], val_batch['y_sim'][:,val_non_constant_subbasin_mask].to(device), means=y_val_means).detach()\n",
    "            \n",
    "    val_loss = (val_losses / len(val_dataloader)).item()\n",
    "    print('Epoch', epoch, 'mean val loss:\\t{}'.format(val_loss))\n",
    "    if val_loss < best_loss_model[1] - min_improvement:\n",
    "        best_loss_model = (epoch, val_loss, model.state_dict())  # new best model\n",
    "        load_data.pickle_model('STGCN_simulationTraining', model, 'allStations', time_stamp, model_type='torch.dill')\n",
    "    elif epoch > best_loss_model[0] + patience:\n",
    "        print('Patience exhausted in epoch {}. Best val-loss was {}'.format(epoch, best_loss_model[1]))\n",
    "        break\n",
    "    \n",
    "print('Using best model from epoch', str(best_loss_model[0]), 'which had loss', str(best_loss_model[1]))\n",
    "model.load_state_dict(best_loss_model[2])\n",
    "load_data.save_model_with_state('STGCN_simulationTraining', best_loss_model[0], model, optimizer, time_stamp, use_dill=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "del y_train_means, y_val_means, y_pred, train_non_constant_subbasin_mask, val_non_constant_subbasin_mask\n",
    "if USE_CUDA:\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logger.warning('predicting')\n",
    "model.eval()\n",
    "\n",
    "spatial_test_predictions = []  # test on different graph, different time\n",
    "for i, test_batch in enumerate(test_dataloader):\n",
    "    pred = model(test_batch['x'].permute(0,2,1,3).to(device), test_adjacency).detach().cpu()\n",
    "    spatial_test_predictions.append(pred)\n",
    "    \n",
    "temporal_test_predictions = []  # test on train graph but different time\n",
    "for i, test_batch in enumerate(temporal_test_dataloader):\n",
    "    pred = model(test_batch['x'].permute(0,2,1,3).to(device), train_adjacency).detach().cpu()\n",
    "    temporal_test_predictions.append(pred)\n",
    "    \n",
    "predictions = torch.cat([torch.cat(spatial_test_predictions), torch.cat(temporal_test_predictions)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "actuals = test_dataset.grid_dataset.data_runoff.copy()\n",
    "if len(actuals['date'].unique()) != len(predictions):\n",
    "    print('Warning: length of prediction {} and actuals {} does not match.'.format(len(predictions), len(actuals['date'].unique())))\n",
    "\n",
    "nse_dict, nse_sim_dict = {}, {}\n",
    "mse_dict, mse_sim_dict = {}, {}\n",
    "predictions_df = pd.DataFrame(columns=actuals.columns)\n",
    "predictions_df['is_test_subbasin'] = False\n",
    "for i in range(len(test_subbasins + train_subbasins)):\n",
    "    subbasin = (test_subbasins + train_subbasins)[i]\n",
    "    station = None\n",
    "    subbasin_sim = test_dataset.grid_dataset.simulated_streamflow[test_dataset.grid_dataset.simulated_streamflow['subbasin'] == subbasin].set_index('date')\n",
    "    if subbasin in station_subbasins:\n",
    "        station = subbasin_sim['StationID'].values[0]\n",
    "        act = actuals[actuals['station'] == station].set_index('date')['runoff']\n",
    "    if predictions.shape[0] != subbasin_sim.shape[0]:\n",
    "        print('Warning: length of prediction {} and actuals {} does not match for subbasin {}. Ignoring excess actuals.'.format(len(predictions), len(subbasin_sim), subbasin))\n",
    "        subbasin_sim = subbasin_sim.iloc[:predictions.shape[0]]\n",
    "        if station is not None:\n",
    "            act = act.iloc[:predictions.shape[0]]\n",
    "            \n",
    "    pred = pd.DataFrame({'runoff': predictions[:,i]}, index=subbasin_sim.index)\n",
    "    pred['subbasin'] = subbasin\n",
    "    pred['station'] = station\n",
    "    pred['is_test_subbasin'] = subbasin in test_subbasins\n",
    "    predictions_df = predictions_df.append(pred.reset_index(), sort=True)\n",
    "    subbasin_type = 'test' if subbasin in test_subbasins else 'train'\n",
    "    nse_sim, mse_sim = evaluate.evaluate_daily('Sub{}'.format(subbasin), pred['runoff'], subbasin_sim['simulated_streamflow'], writer=writer, group=subbasin_type)\n",
    "    nse_sim_dict[subbasin] = nse_sim\n",
    "    mse_sim_dict[subbasin] = mse_sim\n",
    "\n",
    "    if station is not None:\n",
    "        nse, mse = evaluate.evaluate_daily(station, pred['runoff'], act, writer=writer)\n",
    "        nse_dict[subbasin] = nse\n",
    "        mse_dict[subbasin] = mse\n",
    "        print(station, subbasin, '\\tNSE:', nse, '\\tMSE:', mse, '(clipped to 0)')\n",
    "    print(subbasin, '\\tNSE sim:', nse_sim, '\\tMSE sim:', mse_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_nse_mse(name, nse_dict, mse_dict, subbasins):\n",
    "    nses = list(nse_dict[s] for s in subbasins)\n",
    "    mses = list(mse_dict[s] for s in subbasins)\n",
    "    print(name, 'Median NSE (clipped to 0)', np.median(nses), '/ Min', np.min(nses), '/ Max', np.max(nses))\n",
    "    print(name, 'Median MSE (clipped to 0)', np.median(mses), '/ Min', np.min(mses), '/ Max', np.max(mses))\n",
    "    \n",
    "    return np.median(nses)\n",
    "\n",
    "nse_median_sim_temporal = print_nse_mse('Temporal test sim', nse_sim_dict, mse_sim_dict, train_subbasins)\n",
    "nse_median_sim_spatial = print_nse_mse('Spatial test sim', nse_sim_dict, mse_sim_dict, test_subbasins)\n",
    "nse_median_stations_temporal = print_nse_mse('Stations temporal test', nse_dict, mse_dict, list(s for s in station_subbasins if s in train_subbasins))\n",
    "nse_median_stations_spatial = print_nse_mse('Stations spatial test', nse_dict, mse_dict, list(s for s in station_subbasins if s in test_subbasins))\n",
    "\n",
    "writer.add_scalar('nse_median_sim_temporal', nse_median_sim_temporal)\n",
    "writer.add_scalar('nse_median_sim', nse_median_sim_spatial)\n",
    "writer.add_scalar('nse_median_stations_temporal', nse_median_stations_temporal)\n",
    "writer.add_scalar('nse_median_stations_spatial', nse_median_stations_spatial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nse_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df = pd.merge(predictions_df.rename({'runoff': 'prediction'}, axis=1), \n",
    "                   test_dataset.grid_dataset.simulated_streamflow, on=['date', 'subbasin'])\n",
    "save_df = pd.merge(save_df, actuals.rename({'runoff': 'actual'}, axis=1), how='left', on=['date', 'station'])\\\n",
    "            [['date', 'subbasin', 'station', 'prediction', 'actual', 'simulated_streamflow', 'is_test_subbasin']]\n",
    "load_data.pickle_results('STGCN_simulationTraining', save_df, time_stamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = print(train_subbasins), print(val_subbasins), print(test_subbasins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(k for k in list(nse_dict.keys()) if k in test_subbasins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.now().strftime('%Y%m%d-%H%M%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
