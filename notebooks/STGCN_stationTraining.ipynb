{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConvLSTM trained on measured streamflow at gauge stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:This caffe2 python run does not have GPU support. Will run in CPU only mode.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'20190823-074543'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt \n",
    "from datetime import datetime, timedelta\n",
    "import netCDF4 as nc\n",
    "import torch\n",
    "from torch import nn, utils\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from src import load_data, evaluate, conv_lstm, datasets, utils, stgcn\n",
    "import random\n",
    "import pickle\n",
    "import json\n",
    "import networkx as nx\n",
    "\n",
    "time_stamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "time_stamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "fhandler = logging.FileHandler(filename='../log.out', mode='a')\n",
    "chandler = logging.StreamHandler(sys.stdout)\n",
    "formatter = logging.Formatter('%(asctime)s - {} - %(message)s'.format(time_stamp))\n",
    "fhandler.setFormatter(formatter)\n",
    "chandler.setFormatter(formatter)\n",
    "logger.addHandler(fhandler)\n",
    "logger.addHandler(chandler)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:cuda devices: []\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-23 07:45:43,666 - 20190823-074543 - cuda devices: []\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = False\n",
    "if torch.cuda.is_available():\n",
    "    print('CUDA Available')\n",
    "    USE_CUDA = True\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "device = torch.device('cuda:0' if USE_CUDA else 'cpu')\n",
    "num_devices = torch.cuda.device_count() if USE_CUDA else 0\n",
    "logger.warning('cuda devices: {}'.format(list(torch.cuda.get_device_name(i) for i in range(num_devices))))\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioning_strategy = 'distance'  # 'distance' or 'unilabel', see https://arxiv.org/abs/1801.07455\n",
    "max_hops = 10 if partitioning_strategy == 'distance' else None\n",
    "rdrs_vars = [4, 5]\n",
    "agg = ['sum', 'minmax']\n",
    "include_month = True\n",
    "dem, landcover, soil, groundwater = True, False, False, False\n",
    "landcover_types = []\n",
    "seq_len = 8\n",
    "seq_steps = 1\n",
    "\n",
    "train_start = datetime.strptime('2010-01-01', '%Y-%m-%d') + timedelta(days=seq_len * seq_steps)  # first day for which to make a prediction in train set\n",
    "train_end = '2012-12-31'\n",
    "test_start = '2013-01-01'\n",
    "test_end = '2014-12-31'\n",
    "val_fraction = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/simulations_shervan/subbasins.geojson', 'r') as f:\n",
    "     subbasin_shapes = json.loads(f.read())\n",
    "\n",
    "subbasin_graph = utils.create_subbasin_graph()\n",
    "subbasin_graph.remove_nodes_from(['sub-1'])\n",
    "subbasins = list(int(n[3:]) for n in subbasin_graph.nodes)\n",
    "\n",
    "if partitioning_strategy == 'unilabel':\n",
    "    adjacency = torch.unsqueeze(torch.from_numpy(nx.to_numpy_array(subbasin_graph)), 0).float().to(device)\n",
    "elif partitioning_strategy == 'distance':  # use distances in upstream-graph, i.e. in reversed downstream-graph\n",
    "    adjacency = utils.create_hop_matrix(subbasin_graph.reverse(), max_hops).float().to(device)\n",
    "else:\n",
    "    raise Exception('Unsupported partitioning strategy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mgauch/miniconda3/envs/gwf/lib/python3.7/site-packages/ipykernel_launcher.py:2: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  \n",
      "/home/mgauch/miniconda3/envs/gwf/lib/python3.7/site-packages/ipykernel_launcher.py:4: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.SubbasinAggregatedDataset(rdrs_vars, subbasins, seq_len, seq_steps, train_start, train_end, aggregate_daily=agg, include_months=include_month, \n",
    "                                                   dem=dem, landcover=landcover, soil=soil, groundwater=groundwater, landcover_types=landcover_types)\n",
    "test_dataset = datasets.SubbasinAggregatedDataset(rdrs_vars, subbasins, seq_len, seq_steps, test_start, test_end, aggregate_daily=agg, include_months=include_month, \n",
    "                                                  conv_scalers=train_dataset.grid_dataset.conv_scalers, dem=dem, landcover=landcover, soil=soil, groundwater=groundwater, landcover_types=landcover_types)\n",
    "\n",
    "station_subbasins = list(train_dataset.grid_dataset.subbasin_to_station.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'time_stamp': '20190822-222133', 'batch_size': 32, 'loss': NSELoss(), 'include_month': True, 'aggregate_daily': ['sum', 'minmax'], 'rdrs_vars': [4, 5], 'dropout': 0.2, 'val_fraction': 0.1, 'optimizer': Adam (\\nParameter Group 0\\n    amsgrad: False\\n    betas: (0.9, 0.999)\\n    eps: 1e-08\\n    lr: 0.002\\n    weight_decay: 1e-05\\n), 'lr': 0.002, 'patience': 300, 'min_improvement': 0.01, 'x_train_shape': torch.Size([1088, 8, 48, 724]), 'partitioning_strategy': 'distance', 'max_hops': 10, 'x_test_shape': torch.Size([730, 8, 48, 724]), 'num_epochs': 1000, 'seq_len': 8, 'seq_steps': 1, 'train_start': datetime.datetime(2010, 1, 9, 0, 0), 'train_end': '2012-12-31', 'weight_decay': 1e-05, 'landcover_types': [], 'test_start': '2013-01-01', 'test_end': '2014-12-31', 'model': 'Model((st_gcn_networks):ModuleList((0):st_gcn((gcn):ConvTemporalGraphical((conv):Conv2d(48,640,kernel_size=(1,1),stride=(1,1)))(tcn):Sequential((0):BatchNorm2d(64,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)(1):ReLU(inplace=True)(2):Conv2d(64,64,kernel_size=(9,1),stride=(1,1),padding=(4,0))(3):BatchNorm2d(64,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)(4):Dropout(p=0,inplace=True))(relu):ReLU(inplace=True))(1):st_gcn((gcn):ConvTemporalGraphical((conv):Conv2d(64,640,kernel_size=(1,1),stride=(1,1)))(tcn):Sequential((0):BatchNorm2d(64,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)(1):ReLU(inplace=True)(2):Conv2d(64,64,kernel_size=(9,1),stride=(1,1),padding=(4,0))(3):BatchNorm2d(64,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)(4):Dropout(p=0,inplace=True))(relu):ReLU(inplace=True))(2):st_gcn((gcn):ConvTemporalGraphical((conv):Conv2d(64,640,kernel_size=(1,1),stride=(1,1)))(tcn):Sequential((0):BatchNorm2d(64,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)(1):ReLU(inplace=True)(2):Conv2d(64,64,kernel_size=(9,1),stride=(1,1),padding=(4,0))(3):BatchNorm2d(64,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)(4):Dropout(p=0,inplace=True))(relu):ReLU(inplace=True))(3):st_gcn((gcn):ConvTemporalGraphical((conv):Conv2d(64,640,kernel_size=(1,1),stride=(1,1)))(tcn):Sequential((0):BatchNorm2d(64,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)(1):ReLU(inplace=True)(2):Conv2d(64,64,kernel_size=(9,1),stride=(1,1),padding=(4,0))(3):BatchNorm2d(64,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)(4):Dropout(p=0,inplace=True))(relu):ReLU(inplace=True))(4):st_gcn((gcn):ConvTemporalGraphical((conv):Conv2d(64,1280,kernel_size=(1,1),stride=(1,1)))(tcn):Sequential((0):BatchNorm2d(128,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)(1):ReLU(inplace=True)(2):Conv2d(128,128,kernel_size=(9,1),stride=(2,1),padding=(4,0))(3):BatchNorm2d(128,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)(4):Dropout(p=0,inplace=True))(residual):Sequential((0):Conv2d(64,128,kernel_size=(1,1),stride=(2,1))(1):BatchNorm2d(128,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True))(relu):ReLU(inplace=True))(5):st_gcn((gcn):ConvTemporalGraphical((conv):Conv2d(128,1280,kernel_size=(1,1),stride=(1,1)))(tcn):Sequential((0):BatchNorm2d(128,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)(1):ReLU(inplace=True)(2):Conv2d(128,128,kernel_size=(9,1),stride=(1,1),padding=(4,0))(3):BatchNorm2d(128,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)(4):Dropout(p=0,inplace=True))(relu):ReLU(inplace=True))(6):st_gcn((gcn):ConvTemporalGraphical((conv):Conv2d(128,1280,kernel_size=(1,1),stride=(1,1)))(tcn):Sequential((0):BatchNorm2d(128,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)(1):ReLU(inplace=True)(2):Conv2d(128,128,kernel_size=(9,1),stride=(1,1),padding=(4,0))(3):BatchNorm2d(128,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)(4):Dropout(p=0,inplace=True))(relu):ReLU(inplace=True))(7):st_gcn((gcn):ConvTemporalGraphical((conv):Conv2d(128,2560,kernel_size=(1,1),stride=(1,1)))(tcn):Sequential((0):BatchNorm2d(256,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)(1):ReLU(inplace=True)(2):Conv2d(256,256,kernel_size=(9,1),stride=(2,1),padding=(4,0))(3):BatchNorm2d(256,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)(4):Dropout(p=0,inplace=True))(residual):Sequential((0):Conv2d(128,256,kernel_size=(1,1),stride=(2,1))(1):BatchNorm2d(256,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True))(relu):ReLU(inplace=True))(8):st_gcn((gcn):ConvTemporalGraphical((conv):Conv2d(256,2560,kernel_size=(1,1),stride=(1,1)))(tcn):Sequential((0):BatchNorm2d(256,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)(1):ReLU(inplace=True)(2):Conv2d(256,256,kernel_size=(9,1),stride=(1,1),padding=(4,0))(3):BatchNorm2d(256,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)(4):Dropout(p=0,inplace=True))(relu):ReLU(inplace=True))(9):st_gcn((gcn):ConvTemporalGraphical((conv):Conv2d(256,2560,kernel_size=(1,1),stride=(1,1)))(tcn):Sequential((0):BatchNorm2d(256,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)(1):ReLU(inplace=True)(2):Conv2d(256,256,kernel_size=(9,1),stride=(1,1),padding=(4,0))(3):BatchNorm2d(256,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)(4):Dropout(p=0,inplace=True))(relu):ReLU(inplace=True)))(fcn):Conv2d(256,1,kernel_size=(1,1),stride=(1,1)))'}\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "num_epochs = 1000\n",
    "learning_rate = 2e-3\n",
    "patience = 300\n",
    "min_improvement = 0.01\n",
    "best_loss_model = (-1, np.inf, None)\n",
    "dropout = 0.2\n",
    "weight_decay = 1e-5\n",
    "\n",
    "batch_size = 32\n",
    "model = stgcn.Model(train_dataset.x.shape[2], adjacency.shape[0]).to(device)\n",
    "if num_devices > 1:\n",
    "    model = torch.nn.DataParallel(model, device_ids=list(range(num_devices)))\n",
    "loss_fn = evaluate.NSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "writer = SummaryWriter(comment='STGCN_stationTraining')\n",
    "param_description = {'time_stamp': time_stamp, 'batch_size': batch_size, 'loss': loss_fn, 'include_month': include_month, 'aggregate_daily': agg, 'rdrs_vars': rdrs_vars, 'dropout': dropout, 'val_fraction': val_fraction,\n",
    "                     'optimizer': optimizer, 'lr': learning_rate, 'patience': patience, 'min_improvement': min_improvement, 'x_train_shape': train_dataset.x.shape, 'partitioning_strategy': partitioning_strategy, 'max_hops': max_hops,\n",
    "                     'x_test_shape': test_dataset.x.shape, 'num_epochs': num_epochs, 'seq_len': seq_len, 'seq_steps': seq_steps, 'train_start': train_start, 'train_end': train_end, 'weight_decay': weight_decay, \n",
    "                     'landcover_types': landcover_types, 'test_start': test_start, 'test_end': test_end, 'model': str(model).replace('\\n','').replace(' ', ''),}\n",
    "writer.add_text('Parameter Description', str(param_description))\n",
    "str(param_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_indices = np.random.choice(len(train_dataset), size=int(val_fraction * len(train_dataset)), replace=False)\n",
    "train_indices = list(i for i in range(len(train_dataset)) if i not in val_indices)\n",
    "train_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\n",
    "val_sampler = torch.utils.data.SubsetRandomSampler(val_indices)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size, sampler=train_sampler, pin_memory=True, drop_last=False)\n",
    "val_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size, sampler=val_sampler, pin_memory=True, drop_last=False)\n",
    "    \n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size, shuffle=False, pin_memory=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_means = train_dataset.y_means.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    train_losses = torch.tensor(0.0)\n",
    "    for i, train_batch in enumerate(train_dataloader):\n",
    "        y_mask = train_batch['y_mask'].all(dim=0).to(device)  # only backprop for stations where we have non-NA values for the whole batch\n",
    "        if not torch.any(y_mask):\n",
    "            continue\n",
    "            \n",
    "        y_pred = model(train_batch['x'].permute(0,2,1,3).to(device), adjacency)\n",
    "        train_loss = loss_fn(y_pred[:,y_mask], train_batch['y'][:,y_mask].to(device), means=y_train_means[y_mask])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses += train_loss.detach()\n",
    "        \n",
    "    train_loss = (train_losses / len(train_dataloader)).item()\n",
    "    print('Epoch', epoch, 'mean train loss:\\t{}'.format(train_loss))\n",
    "    writer.add_scalar('loss_nse', train_loss, epoch)\n",
    "    \n",
    "    model.eval()\n",
    "    val_losses = torch.tensor(0.0)\n",
    "    for i, val_batch in enumerate(val_dataloader):\n",
    "        y_mask = val_batch['y_mask'].all(dim=0).to(device)\n",
    "        if not torch.any(y_mask):\n",
    "            continue\n",
    "            \n",
    "        y_pred = model(val_batch['x'].permute(0,2,1,3).to(device), adjacency).detach()\n",
    "        val_losses += loss_fn(y_pred[:,y_mask], val_batch['y'][:,y_mask].to(device), means=y_train_means[y_mask]).detach()\n",
    "            \n",
    "    val_loss = (val_losses / len(val_dataloader)).item()\n",
    "    print('Epoch', epoch, 'mean val loss:\\t{}'.format(val_loss))\n",
    "    writer.add_scalar('loss_nse_val', val_loss, epoch)\n",
    "    \n",
    "    if val_loss < best_loss_model[1] - min_improvement:\n",
    "        best_loss_model = (epoch, val_loss, model.state_dict())  # new best model\n",
    "        load_data.pickle_model('STGCN_stationTraining', model, 'allStations', time_stamp, model_type='torch.dill')\n",
    "    elif epoch > best_loss_model[0] + patience:\n",
    "        print('Patience exhausted in epoch {}. Best val-loss was {}'.format(epoch, best_loss_model[1]))\n",
    "        break\n",
    "    \n",
    "print('Using best model from epoch', str(best_loss_model[0]), 'which had loss', str(best_loss_model[1]))\n",
    "model.load_state_dict(best_loss_model[2])\n",
    "load_data.save_model_with_state('STGCN_stationTraining', best_loss_model[0], model, optimizer, time_stamp, use_dill=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "del y_train_means, y_pred, y_mask\n",
    "if USE_CUDA:\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logger.warning('predicting')\n",
    "model.eval()\n",
    "\n",
    "predictions = []  # test on same graph but different time\n",
    "for i, test_batch in enumerate(test_dataloader):\n",
    "    pred = model(test_batch['x'].permute(0,2,1,3).to(device), adjacency).detach().cpu()\n",
    "    predictions.append(pred)\n",
    "    \n",
    "predictions = torch.cat(predictions, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "actuals = test_dataset.grid_dataset.data_runoff.copy()\n",
    "if len(actuals['date'].unique()) != len(predictions):\n",
    "    print('Warning: length of prediction {} and actuals {} does not match.'.format(len(predictions), len(actuals['date'].unique())))\n",
    "\n",
    "nse_dict = {}\n",
    "mse_dict = {}\n",
    "predictions_df = pd.DataFrame(columns=actuals.columns)\n",
    "for i in range(len(subbasins)):\n",
    "    subbasin = subbasins[i]\n",
    "    if subbasin not in station_subbasins:\n",
    "        continue\n",
    "    station = test_dataset.grid_dataset.subbasin_to_station[subbasin]\n",
    "    act = actuals[actuals['station'] == station].set_index('date')['runoff']\n",
    "    if predictions.shape[0] != act.shape[0]:\n",
    "        print('Warning: length of prediction {} and actuals {} does not match for subbasin {}. Ignoring excess actuals.'.format(len(predictions), len(act), subbasin))\n",
    "        act = act.iloc[:predictions.shape[0]]\n",
    "            \n",
    "    pred = pd.DataFrame({'runoff': predictions[:,i]}, index=act.index)\n",
    "    pred['subbasin'] = subbasin\n",
    "    pred['station'] = station\n",
    "    predictions_df = predictions_df.append(pred.reset_index(), sort=True)\n",
    "    \n",
    "    nse, mse = evaluate.evaluate_daily(station, pred['runoff'], act, writer=writer)\n",
    "    nse_dict[subbasin] = nse\n",
    "    mse_dict[subbasin] = mse\n",
    "    print(station, subbasin, '\\tNSE:', nse, '\\tMSE:', mse, '(clipped to 0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_nse_mse(name, nse_dict, mse_dict, subbasins):\n",
    "    nses = list(nse_dict[s] for s in subbasins)\n",
    "    mses = list(mse_dict[s] for s in subbasins)\n",
    "    print(name, 'Median NSE (clipped to 0)', np.nanmedian(nses), '/ Min', np.nanmin(nses), '/ Max', np.nanmax(nses))\n",
    "    print(' ' * len(name), 'Median MSE (clipped to 0)', np.nanmedian(mses), '/ Min', np.nanmin(mses), '/ Max', np.nanmax(mses))\n",
    "    \n",
    "    return np.nanmedian(nses)\n",
    "\n",
    "nse_median_stations = print_nse_mse('Stations test', nse_dict, mse_dict, list(s for s in station_subbasins))\n",
    "\n",
    "writer.add_scalar('nse_median_stations_temporal', nse_median_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nse_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'STGCN_stationTraining_20190822-205559.pkl'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_df = pd.merge(predictions_df.rename({'runoff': 'prediction'}, axis=1), \n",
    "                   test_dataset.grid_dataset.simulated_streamflow, on=['date', 'subbasin'])\n",
    "save_df = pd.merge(save_df, actuals.rename({'runoff': 'actual'}, axis=1), how='left', on=['date', 'station'])\\\n",
    "            [['date', 'subbasin', 'station', 'prediction', 'actual', 'simulated_streamflow']]\n",
    "load_data.pickle_results('STGCN_stationTraining', save_df, time_stamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20190822-221943'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.now().strftime('%Y%m%d-%H%M%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
