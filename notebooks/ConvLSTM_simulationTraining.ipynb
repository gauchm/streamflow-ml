{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConvLSTM trained on simulated streamflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'20190814-105500'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt \n",
    "from datetime import datetime, timedelta\n",
    "from sklearn import preprocessing\n",
    "import netCDF4 as nc\n",
    "import torch\n",
    "from torch import nn, utils\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from src import load_data, evaluate, conv_lstm, datasets\n",
    "import torch.autograd as autograd\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms.functional as TF\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "time_stamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "time_stamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "fhandler = logging.FileHandler(filename='../log.out', mode='a')\n",
    "chandler = logging.StreamHandler(sys.stdout)\n",
    "formatter = logging.Formatter('%(asctime)s - {} - %(message)s'.format(time_stamp))\n",
    "fhandler.setFormatter(formatter)\n",
    "chandler.setFormatter(formatter)\n",
    "logger.addHandler(fhandler)\n",
    "logger.addHandler(chandler)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = False\n",
    "if torch.cuda.is_available():\n",
    "    print('CUDA Available')\n",
    "    USE_CUDA = True\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "device = torch.device('cuda:0' if USE_CUDA else 'cpu')\n",
    "num_devices = torch.cuda.device_count() if USE_CUDA else 0\n",
    "logger.warning('cuda devices: {}'.format(list(torch.cuda.get_device_name(i) for i in range(num_devices))))\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landcover_nc = nc.Dataset('../data/NA_NALCMS_LC_30m_LAEA_mmu12_urb05_n40-45w75-90_erie.nc', 'r')\n",
    "landcover_nc.set_auto_mask(False)\n",
    "erie_lats = landcover_nc['lat'][:][::-1]\n",
    "erie_lons = landcover_nc['lon'][:]\n",
    "landcover_nc.close()\n",
    "erie_lat_min, erie_lat_max, erie_lon_min, erie_lon_max = erie_lats.min(), erie_lats.max(), erie_lons.min(), erie_lons.max()\n",
    "del erie_lats, erie_lons\n",
    "\n",
    "out_lats, out_lons = load_data.load_dem_lats_lons()\n",
    "out_lats = out_lats[(erie_lat_min <= out_lats) & (out_lats <= erie_lat_max)].copy()\n",
    "out_lons = out_lons[(erie_lon_min <= out_lons) &  (out_lons <= erie_lon_max)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 8\n",
    "seq_steps = 1\n",
    "stateful_lstm = False\n",
    "validation_fraction, val_start, val_end = None, None, None\n",
    "random_transform = True\n",
    "\n",
    "if stateful_lstm:\n",
    "    val_start = datetime.strptime('2010-01-01', '%Y-%m-%d') + timedelta(days=seq_len * seq_steps)  # first day for which to make a prediction in train set\n",
    "    val_end = '2010-09-30'\n",
    "    train_start = '2010-10-01'\n",
    "    train_end = '2012-12-31'\n",
    "else:\n",
    "    validation_fraction = 0.1\n",
    "    train_start = datetime.strptime('2010-01-01', '%Y-%m-%d') + timedelta(days=seq_len * seq_steps)  # first day for which to make a prediction in train set\n",
    "    train_end = '2012-12-31'\n",
    "test_start = '2013-01-01'\n",
    "test_end = '2014-12-31'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rdrs_vars = [4, 5]\n",
    "agg = ['sum', 'minmax']\n",
    "include_month = True\n",
    "train_dataset = datasets.RdrsGridDataset(rdrs_vars, seq_len, seq_steps, train_start, train_end, aggregate_daily=agg, include_months=True, include_simulated_streamflow=True, resample_rdrs=True, out_lats=out_lats, out_lons=out_lons)\n",
    "if stateful_lstm:\n",
    "    val_dataset = datasets.RdrsGridDataset(rdrs_vars, seq_len, seq_steps, val_start, val_end, conv_scalers=train_dataset.conv_scalers, aggregate_daily=agg,  include_months=True, include_simulated_streamflow=True, resample_rdrs=True, out_lats=out_lats, out_lons=out_lons)\n",
    "test_dataset = datasets.RdrsGridDataset(rdrs_vars, seq_len, seq_steps, test_start, test_end, conv_scalers=train_dataset.conv_scalers, aggregate_daily=agg, include_months=True, include_simulated_streamflow=True, resample_rdrs=True, out_lats=out_lats, out_lons=out_lons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "landcover_types = []\n",
    "geophysical_dataset = datasets.GeophysicalGridDataset(dem=True, landcover=False, soil=False, groundwater=False, min_lat=erie_lat_min, max_lat=erie_lat_max, min_lon=erie_lon_min, max_lon=erie_lon_max, landcover_types=landcover_types)\n",
    "geophysical_data = next(geophysical_dataset.__iter__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "subbasins = train_dataset.simulated_streamflow['subbasin'].unique()\n",
    "np.random.seed(0)\n",
    "test_subbasins = np.random.choice(subbasins, size=int(0.2 * len(subbasins)), replace=False)\n",
    "val_subbasins = np.random.choice(list(s for s in subbasins if s not in test_subbasins), size=int(validation_fraction * len(subbasins)), replace=False)\n",
    "train_subbasins = list(s for s in subbasins if s not in test_subbasins and s not in val_subbasins)\n",
    "station_subbasins = train_dataset.simulated_streamflow[~pd.isna(train_dataset.simulated_streamflow['StationID'])]['subbasin'].unique()\n",
    "\n",
    "train_subbasin_indices = list(train_dataset.outlet_to_row_col[s] for s in train_subbasins)\n",
    "val_subbasin_indices = list(train_dataset.outlet_to_row_col[s] for s in val_subbasins)\n",
    "test_subbasin_indices = list(test_dataset.outlet_to_row_col[s] for s in test_subbasins)\n",
    "\n",
    "train_mask = torch.zeros((train_dataset.out_lats.shape[0], train_dataset.out_lats.shape[1]), dtype=torch.bool)\n",
    "val_mask = torch.zeros((train_dataset.out_lats.shape[0], train_dataset.out_lats.shape[1]), dtype=torch.bool)\n",
    "for row in range(train_mask.shape[0]):\n",
    "    for col in range(train_mask.shape[1]):\n",
    "        train_mask[row, col] = True if (row, col) in train_subbasin_indices else False\n",
    "        val_mask[row, col] = True if (row, col) in val_subbasin_indices else False\n",
    "train_mask = train_mask.to(device)\n",
    "val_mask = val_mask.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "num_epochs = 600\n",
    "learning_rate = 2e-3\n",
    "patience = 250\n",
    "min_improvement = 0.01\n",
    "best_loss_model = (-1, np.inf, None)\n",
    "\n",
    "# Prepare model\n",
    "batch_size = 8\n",
    "num_convlstm_layers = 3\n",
    "num_conv_layers = 6\n",
    "convlstm_hidden_dims = [16,8,8]\n",
    "conv_hidden_dims = [128,64,64,32,16]\n",
    "convlstm_kernel_size = [(3,3)] * num_convlstm_layers\n",
    "conv_kernel_size = [(3,3)] * num_conv_layers\n",
    "conv_activation = nn.LeakyReLU\n",
    "dropout = 0.1\n",
    "weight_decay = 1e-6\n",
    "\n",
    "model = conv_lstm.ConvLSTMGridWithGeophysicalInput((train_dataset.conv_height, train_dataset.conv_width), train_dataset.n_conv_vars, \n",
    "                                                   geophysical_dataset.shape[0], convlstm_hidden_dims, conv_hidden_dims, convlstm_kernel_size, \n",
    "                                                   conv_kernel_size, num_convlstm_layers, num_conv_layers, conv_activation, dropout=dropout, \n",
    "                                                   geophysical_size=geophysical_dataset.shape[1:], feed_timesteps=seq_len).to(device)\n",
    "if num_devices > 1:\n",
    "    model = torch.nn.DataParallel(model, device_ids=list(range(num_devices)))\n",
    "loss_fn = evaluate.NSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "writer = SummaryWriter(comment='ConvLSTM_simulationTraining')\n",
    "param_description = {'time_stamp': time_stamp, 'H_convlstm': convlstm_hidden_dims, 'H_conv': conv_hidden_dims, 'batch_size': batch_size, 'num_convlstm_layers': num_convlstm_layers, 'num_conv_layers': num_conv_layers, 'convlstm_kernel_size': convlstm_kernel_size, 'conv_kernel_size': conv_kernel_size, 'loss': loss_fn, \n",
    "                     'optimizer': optimizer, 'lr': learning_rate, 'patience': patience, 'min_improvement': min_improvement, 'stateful_lstm': stateful_lstm, 'dropout': dropout, 'geophys_shape': geophysical_dataset.shape, 'conv_activation': conv_activation,\n",
    "                     'num_epochs': num_epochs, 'seq_len': seq_len, 'seq_steps': seq_steps, 'train_start': train_start, 'train_end': train_end, 'weight_decay': weight_decay, 'validation_fraction': validation_fraction, 'landcover_types': landcover_types,\n",
    "                     'test_start': test_start, 'test_end': test_end, 'n_conv_vars': train_dataset.n_conv_vars, 'model': str(model).replace('\\n','').replace(' ', ''), 'val_start': val_start, 'val_end': val_end,\n",
    "                     'train len': len(train_dataset), 'conv_height': train_dataset.conv_height, 'conv_width': train_dataset.conv_width, 'test len': len(test_dataset), 'random_transform': random_transform}\n",
    "writer.add_text('Parameter Description', str(param_description))\n",
    "str(param_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "if stateful_lstm:\n",
    "    train_sampler = datasets.StatefulBatchSampler(train_dataset, batch_size)\n",
    "    val_sampler = datasets.StatefulBatchSampler(val_dataset, batch_size)\n",
    "    test_sampler = datasets.StatefulBatchSampler(test_dataset, batch_size)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_sampler=train_sampler, pin_memory=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_sampler=val_sampler, pin_memory=True)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_sampler=test_sampler, pin_memory=True)\n",
    "else:\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=True, pin_memory=True, drop_last=False)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size, shuffle=False, pin_memory=True, drop_last=False)\n",
    "    \n",
    "geophysical_batch = geophysical_data.repeat(batch_size,1,1,1).to(device, non_blocking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_transform(rdrs_batch, geophysical_batch, y_batch, train_mask, val_mask, rdrs_contains_month=False, border_masking=0):\n",
    "    if random_transform:# and random.random() > 0.5:\n",
    "        angle = random.randint(-180, 180)\n",
    "        horizontal_flip = random.choice([True, False])\n",
    "        vertical_flip = random.choice([True, False])\n",
    "        transformed_tensors = []\n",
    "        for tensor in [rdrs_batch, geophysical_batch, y_batch, train_mask.float(), val_mask.float()]:\n",
    "            images = [TF.to_pil_image(image, mode='F') for image in tensor.reshape((-1,*tensor.shape[-2:]))]\n",
    "            images = [TF.rotate(image, angle) for image in images]\n",
    "            images = [TF.hflip(image) for image in images] if horizontal_flip else images\n",
    "            images = [TF.vflip(image) for image in images] if vertical_flip else images\n",
    "            \n",
    "            transformed_tensors.append(torch.cat([TF.to_tensor(image) for image in images]).reshape(tensor.shape))\n",
    "        \n",
    "        rdrs_transformed, geophysical_transformed, y_transformed, train_mask_transformed, val_mask_transformed = transformed_tensors\n",
    "        \n",
    "        # Fix \"month\" features that are all-0/all-1 images\n",
    "        if rdrs_contains_month:\n",
    "            rdrs_transformed[:,:,-12:] = rdrs_batch[:,:,-12:]\n",
    "            \n",
    "        # Do not consider subbasins that are rotated almost out of the image for training\n",
    "        border_mask = torch.zeros(train_mask.shape, device=device, dtype=torch.bool)\n",
    "        border_mask[border_masking:-border_masking,border_masking:-border_masking] = True\n",
    "        return rdrs_transformed, geophysical_transformed, y_transformed, train_mask_transformed.bool() & border_mask, val_mask_transformed.bool() & border_mask\n",
    "    return rdrs_batch, geophysical_batch, y_batch, train_mask, val_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_mean = train_dataset.simulated_streamflow[train_dataset.simulated_streamflow['subbasin'].isin(train_subbasins)]['simulated_streamflow'].mean()\n",
    "y_mean_val = train_dataset.simulated_streamflow[train_dataset.simulated_streamflow['subbasin'].isin(val_subbasins)]['simulated_streamflow'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    train_losses = torch.tensor(0.0)\n",
    "    val_losses = torch.tensor(0.0)\n",
    "    conv_hidden_states = None\n",
    "    for i, train_batch in enumerate(train_dataloader):\n",
    "        geophysical_input = geophysical_batch[:train_batch['y_sim'].shape[0]]\n",
    "        x_conv, geophysical_input, y_train, train_mask_transformed, val_mask_transformed = random_transform(train_batch['x_conv'], geophysical_input, train_batch['y_sim'], \n",
    "                                                                                                            train_mask, val_mask, rdrs_contains_month=include_month, border_masking=20)\n",
    "        y_train = y_train.reshape((y_train.shape[0],-1)).to(device, non_blocking=True)\n",
    "        train_mask_transformed = train_mask_transformed.reshape(-1)\n",
    "        val_mask_transformed = val_mask_transformed.reshape(-1)\n",
    "        \n",
    "        if not train_mask_transformed.any():\n",
    "            print('Batch {} has no target values. skipping.'.format(i))\n",
    "            continue\n",
    "        if not stateful_lstm:\n",
    "            conv_hidden_states = None\n",
    "        \n",
    "        y_pred, conv_hidden_states = model(x_conv.to(device), geophysical_input, hidden_state=conv_hidden_states)\n",
    "        y_pred = y_pred.reshape((y_train.shape[0], -1))\n",
    "        train_loss = loss_fn(y_pred[:,train_mask_transformed], y_train[:,train_mask_transformed], mean=y_mean)\n",
    "        val_losses += loss_fn(y_pred[:,val_mask_transformed], y_train[:,val_mask_transformed], mean=y_mean_val).detach()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses += train_loss.detach()\n",
    "        if i==2:\n",
    "            break\n",
    "    train_loss = (train_losses / len(train_dataloader)).item()\n",
    "    val_loss = (val_losses / len(train_dataloader)).item()\n",
    "    print('Epoch', epoch, 'mean train loss:\\t{}'.format(train_loss))\n",
    "    print('Epoch', epoch, 'mean val loss:\\t{}'.format(val_loss))\n",
    "    writer.add_scalar('loss_nse', train_loss, epoch)\n",
    "    writer.add_scalar('loss_nse_val', val_loss, epoch)\n",
    "    \n",
    "    if val_loss < best_loss_model[1] - min_improvement:\n",
    "        best_loss_model = (epoch, val_loss, model.state_dict())  # new best model\n",
    "        load_data.pickle_model('ConvLSTM_simulationTraining', model, 'allStations', time_stamp)\n",
    "    elif epoch > best_loss_model[0] + patience:\n",
    "        print('Patience exhausted in epoch {}. Best val-loss was {}'.format(epoch, best_loss_model[1]))\n",
    "        break\n",
    "    break\n",
    "print('Using best model from epoch', str(best_loss_model[0]), 'which had loss', str(best_loss_model[1]))\n",
    "model.load_state_dict(best_loss_model[2])\n",
    "load_data.save_model_with_state('ConvLSTM_simulationTraining', best_loss_model[0], model, optimizer, time_stamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "logger.warning('predicting')\n",
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "conv_hidden_states = None\n",
    "for i, test_batch in enumerate(test_dataloader):\n",
    "    if not stateful_lstm:\n",
    "        conv_hidden_states = None\n",
    "        \n",
    "    geophysical_input = geophysical_batch[:test_batch['y_sim'].shape[0]]\n",
    "    pred, conv_hidden_states = model(test_batch['x_conv'].to(device), geophysical_input, hidden_state=conv_hidden_states)\n",
    "    predictions.append(pred.detach().cpu())\n",
    "    \n",
    "predictions = torch.cat(predictions).cpu()\n",
    "\n",
    "if stateful_lstm:\n",
    "    # reorder time series\n",
    "    pred_indices = np.array(list(test_sampler.__iter__())).reshape(-1)\n",
    "    predictions = predictions[pred_indices.argsort()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "actuals = test_dataset.data_runoff.copy()\n",
    "if len(actuals['date'].unique()) != len(predictions):\n",
    "    print('Warning: length of prediction {} and actuals {} does not match.'.format(len(predictions), len(actuals['date'].unique())))\n",
    "\n",
    "nse_dict, nse_sim_dict = {}, {}\n",
    "mse_dict, mse_sim_dict = {}, {}\n",
    "predictions_df = pd.DataFrame(columns=actuals.columns)\n",
    "predictions_df['is_test_subbasin'] = False\n",
    "predictions_df['is_val_subbasin'] = False\n",
    "for subbasin in test_dataset.simulated_streamflow['subbasin'].unique():\n",
    "    row, col = test_dataset.outlet_to_row_col[subbasin]\n",
    "    \n",
    "    station = None\n",
    "    subbasin_sim = test_dataset.simulated_streamflow[test_dataset.simulated_streamflow['subbasin'] == subbasin].set_index('date')\n",
    "    if subbasin in station_subbasins:\n",
    "        station = subbasin_sim['StationID'].values[0]\n",
    "        act = actuals[actuals['station'] == station].set_index('date')['runoff']\n",
    "    if predictions.shape[0] != subbasin_sim.shape[0]:\n",
    "        print('Warning: length of prediction {} and actuals {} does not match for subbasin {}. Ignoring excess actuals.'.format(len(predictions), len(subbasin_sim), subbasin))\n",
    "        subbasin_sim = subbasin_sim.iloc[:predictions.shape[0]]\n",
    "        if station is not None:\n",
    "            act = act.iloc[:predictions.shape[0]]\n",
    "    pred = pd.DataFrame({'runoff': predictions[:,row,col]}, index=subbasin_sim.index)\n",
    "    pred['subbasin'] = subbasin\n",
    "    pred['station'] = station\n",
    "    pred['is_test_subbasin'] = subbasin in test_subbasins\n",
    "    pred['is_val_subbasin'] = subbasin in val_subbasins\n",
    "    predictions_df = predictions_df.append(pred.reset_index(), sort=True)\n",
    "    nse_sim, mse_sim = evaluate.evaluate_daily('Sub{}'.format(subbasin), pred['runoff'], subbasin_sim['simulated_streamflow'], writer=writer)\n",
    "    nse_sim_dict[subbasin] = nse_sim\n",
    "    mse_sim_dict[subbasin] = mse_sim\n",
    "\n",
    "    if station is not None:\n",
    "        nse, mse = evaluate.evaluate_daily(station, pred['runoff'], act, writer=writer)\n",
    "        nse_dict[subbasin] = nse\n",
    "        mse_dict[subbasin] = mse\n",
    "        print(station, subbasin, '\\tNSE:', nse, '\\tMSE:', mse, '(clipped to 0)')\n",
    "    print(subbasin, '\\tNSE sim:', nse_sim, '\\tMSE sim:', mse_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_nse_mse(name, nse_dict, mse_dict, subbasins):\n",
    "    nses = list(nse_dict[s] for s in subbasins)\n",
    "    mses = list(mse_dict[s] for s in subbasins)\n",
    "    print(name, 'Median NSE (clipped to 0)', np.median(nses), '/ Min', np.min(nses), '/ Max', np.max(nses))\n",
    "    print(name, 'Median MSE (clipped to 0)', np.median(mses), '/ Min', np.min(mses), '/ Max', np.max(mses))\n",
    "    \n",
    "    return np.median(nses)\n",
    "\n",
    "print_nse_mse('Train sim', nse_sim_dict, mse_sim_dict, train_subbasins)\n",
    "print_nse_mse('Val sim', nse_sim_dict, mse_sim_dict, val_subbasins)\n",
    "nse_median_sim_test = print_nse_mse('Test sim', nse_sim_dict, mse_sim_dict, train_subbasins)\n",
    "nse_median_stations_train_val = print_nse_mse('Stations (Train/Val)', nse_dict, mse_dict, list(s for s in station_subbasins if s not in test_subbasins))\n",
    "nse_median_stations_test = print_nse_mse('Stations (Test)', nse_dict, mse_dict, list(s for s in station_subbasins if s in test_subbasins))\n",
    "nse_median_stations = print_nse_mse('Stations (Train/Val/Test)', nse_dict, mse_dict, station_subbasins)\n",
    "\n",
    "writer.add_scalar('nse_median_sim', nse_median_sim_test)\n",
    "writer.add_scalar('nse_median_stations_test', nse_median_stations_test)\n",
    "writer.add_scalar('nse_median_stations_all', nse_median_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nse_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df = pd.merge(predictions_df.rename({'runoff': 'prediction'}, axis=1), \n",
    "                   train_dataset.simulated_streamflow, on=['date', 'subbasin'])\n",
    "save_df = pd.merge(save_df, actuals.rename({'runoff': 'actual'}, axis=1), how='left', on=['date', 'station'])\\\n",
    "            [['date', 'subbasin', 'station', 'prediction', 'actual', 'simulated_streamflow', 'is_test_subbasin', 'is_val_subbasin']]\n",
    "load_data.pickle_results('ConvLSTM_simulationTraining', save_df, time_stamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = print(train_subbasins), print(val_subbasins), print(test_subbasins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(k for k in list(nse_dict.keys()) if k in test_subbasins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.now().strftime('%Y%m%d-%H%M%S')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
