{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConvLSTM trained on gridded forcings for all stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt \n",
    "from datetime import datetime, timedelta\n",
    "from sklearn import preprocessing\n",
    "import netCDF4 as nc\n",
    "import torch\n",
    "from torch import nn, utils\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from src import load_data, evaluate, conv_lstm, datasets\n",
    "import torch.autograd as autograd\n",
    "import pickle\n",
    "\n",
    "time_stamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "time_stamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "fhandler = logging.FileHandler(filename='../log.out', mode='a')\n",
    "formatter = logging.Formatter('%(asctime)s - {} - %(message)s'.format(time_stamp))\n",
    "fhandler.setFormatter(formatter)\n",
    "logger.addHandler(fhandler)\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = False\n",
    "if torch.cuda.is_available():\n",
    "    print('CUDA Available')\n",
    "    USE_CUDA = True\n",
    "device = torch.device('cuda' if USE_CUDA else 'cpu')\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 50\n",
    "seq_steps = 4\n",
    "validation_fraction = 0.2\n",
    "\n",
    "train_start = datetime.strptime('2010-01-01', '%Y-%m-%d') + timedelta(hours=seq_len * seq_steps)  # first day for which to make a prediction in train set\n",
    "train_end = '2010-12-31'\n",
    "test_start = '2013-01-01'\n",
    "test_end = '2014-12-31'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdrs_vars = [4,5]\n",
    "train_dataset = datasets.RdrsDataset(rdrs_vars, seq_len, seq_steps, train_start, train_end)\n",
    "test_dataset = datasets.RdrsDataset(rdrs_vars, seq_len, seq_steps, test_start, test_end, \n",
    "                                    conv_scalers=train_dataset.conv_scalers, fc_scalers=train_dataset.fc_scalers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "num_epochs = 150\n",
    "learning_rate = 2e-3\n",
    "patience = 100\n",
    "min_improvement = 0.05\n",
    "best_loss_model = (-1, np.inf, None)\n",
    "\n",
    "# Prepare model\n",
    "lstm_layers = 4\n",
    "conv_hidden_dims = [16] * (lstm_layers - 1) + [1]\n",
    "H_fc = 8\n",
    "batch_size = 16\n",
    "fc_layers = 1\n",
    "kernel_size = (3,3)\n",
    "dropout = 0.0\n",
    "pooling = [True] * lstm_layers\n",
    "model = conv_lstm.ConvLSTMRegression((train_dataset.conv_height, train_dataset.conv_width), train_dataset.n_fc_vars, train_dataset.n_conv_vars, conv_hidden_dims, \n",
    "                                     kernel_size, lstm_layers, dropout, fc_layers, H_fc, pooling, batch_first=True).to(device)\n",
    "loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "writer = SummaryWriter()\n",
    "param_description = {'H_conv': conv_hidden_dims, 'H_fc': H_fc, 'batch_size': batch_size, 'lstm_layers': lstm_layers, 'fc_layers': fc_layers, 'kernel_size': kernel_size, 'loss': loss_fn, \n",
    "                     'optimizer': optimizer, 'lr': learning_rate, 'patience': patience, 'min_improvement': min_improvement, 'pooling': pooling,\n",
    "                     'num_epochs': num_epochs, 'seq_len': seq_len, 'seq_steps': seq_steps, 'validation_fraction': validation_fraction, 'dropout': dropout,\n",
    "                     'train_start': train_start, 'train_end': train_end, 'test_start': test_start, 'test_end': test_end, 'x_conv_train': x_conv_train.shape,\n",
    "                     'x_conv_val': x_conv_val.shape, 'x_fc_train': x_fc_train.shape, 'x_fc_val': x_fc_val.shape, 'y_train': y_train.shape, 'y_val': y_val.shape}\n",
    "writer.add_text('Parameter Description', str(param_description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = list(range(len(train_dataset)))\n",
    "n_val_samples = int(validation_fraction * len(train_dataset))\n",
    "validation_indices = np.random.choice(indices, size=n_val_samples, replace=False)\n",
    "train_indices = list(set(indices) - set(validation_indices))\n",
    "\n",
    "train_sampler = utils.data.SubsetRandomSampler(train_indices)\n",
    "validation_sampler = utils.data.SubsetRandomSampler(validation_indices)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size, sampler=train_sampler)\n",
    "validation_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size, sampler=validation_sampler)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    epoch_losses = []\n",
    "    for i, train_batch in enumerate(train_dataloader):\n",
    "        y_pred = model(train_batch['x_conv'].to(device), train_batch['x_fc'].to(device))\n",
    "        loss = loss_fn(y_pred, train_batch['y'].reshape((batch_size, 1)).to(device))\n",
    "        epoch_losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if USE_CUDA & (i % 100 == 0):\n",
    "            logger.warning('epoch {} i {} cuda memory: {}'.format(epoch, i, torch.cuda.max_memory_allocated(device=device)))\n",
    "            torch.cuda.reset_max_memory_allocated(device=device)\n",
    "    \n",
    "    epoch_loss = np.array(epoch_losses).mean()\n",
    "    print('Epoch', epoch, 'mean train loss:\\t{}'.format(epoch_loss))\n",
    "    writer.add_scalar('loss', epoch_loss, epoch)\n",
    "    \n",
    "    # eval on validation split\n",
    "    model.eval()\n",
    "    val_pred = np.array([])\n",
    "    val_y = np.array([])\n",
    "    for i, val_batch in enumerate(validation_dataloader):\n",
    "        batch_pred = model(val_batch['x_conv'].to(device), val_batch['x_fc'].to(device)).detach().cpu().numpy().reshape(batch_size)\n",
    "        val_pred = np.concatenate([val_pred, batch_pred])\n",
    "        val_y = np.concatenate([val_y, val_batch['y'].numpy()])\n",
    "    \n",
    "    val_nse, val_mse = evaluate.evaluate_daily('All Stations', pd.Series(val_pred), pd.Series(val_y))\n",
    "    print('Epoch {} mean val mse:    \\t{},\\tnse: {}'.format(epoch, val_mse, val_nse))\n",
    "    writer.add_scalar('loss_eval', val_mse, epoch)\n",
    "\n",
    "    if val_mse < best_loss_model[1] - min_improvement:\n",
    "        best_loss_model = (epoch, val_mse, model.state_dict())  # new best model\n",
    "    elif epoch > best_loss_model[0] + patience:\n",
    "        print('Patience exhausted in epoch {}. Best val-loss was {}'.format(epoch, best_loss_model[1]))\n",
    "        break\n",
    "\n",
    "print('Using best model from epoch', str(best_loss_model[0]), 'which had loss', str(best_loss_model[1]))\n",
    "model.load_state_dict(best_loss_model[2])\n",
    "load_data.pickle_model('ConvLSTM+LinearLayer_VIC', model, 'allStations', time_stamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "predict = test_dataset.data_runoff.copy()\n",
    "predict['runoff'] = np.nan\n",
    "pred_array = np.array([])\n",
    "for i, test_batch in enumerate(test_dataloader):\n",
    "    pred_array = np.concatenate([pred_array, model(test_batch['x_conv'].to(device), test_batch['x_fc'].to(device)).detach().cpu().numpy().reshape(batch_size)])\n",
    "\n",
    "predict['runoff'] = pred_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "actuals = test_dataset.data_runoff.copy()\n",
    "\n",
    "nse_list = []\n",
    "mse_list = []\n",
    "for station in predict['station'].unique():\n",
    "    nse, mse = evaluate.evaluate_daily(station, predict[predict['station'] == station]['runoff'], actuals[actuals['station'] == station]['runoff'], writer=writer)\n",
    "    nse_list.append(nse)\n",
    "    mse_list.append(mse)\n",
    "    \n",
    "    print(station, '\\tNSE:', nse, '\\tMSE:', mse, '(clipped to 0)')\n",
    "\n",
    "print('Median NSE (clipped to 0)', np.median(nse_list), '/ Min', np.min(nse_list), '/ Max', np.max(nse_list))\n",
    "print('Median MSE (clipped to 0)', np.median(mse_list), '/ Min', np.min(mse_list), '/ Max', np.max(mse_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df = pd.merge(predict.rename({'runoff': 'prediction'}, axis=1), actuals.rename({'runoff': 'actual'}, axis=1), \n",
    "                   on=['date', 'station'])[['date', 'station', 'prediction', 'actual']]\n",
    "load_data.pickle_results('ConvLSTM+LinearLayer_VIC', save_df, time_stamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.now().strftime('%Y%m%d-%H%M%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
