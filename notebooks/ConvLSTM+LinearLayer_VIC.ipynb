{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConvLSTM trained on gridded forcings for all stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'20190717-093145'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt \n",
    "from datetime import datetime, timedelta\n",
    "from sklearn import preprocessing\n",
    "import netCDF4 as nc\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from src import load_data, evaluate, conv_lstm\n",
    "import torch.autograd as autograd\n",
    "import pickle\n",
    "\n",
    "time_stamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "time_stamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = False\n",
    "if torch.cuda.is_available():\n",
    "    print('CUDA Available')\n",
    "    USE_CUDA = True\n",
    "device = torch.device('cuda' if USE_CUDA else 'cpu')\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../src/load_data.py:43: RuntimeWarning: invalid value encountered in greater\n",
      "  rdrs_data[:,i,:,:] = rdrs_nc[forcing_variables[i]][:]\n",
      "../src/load_data.py:20: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support skipfooter; you can avoid this warning by specifying engine='python'.\n",
      "  data = pd.read_csv(os.path.join(dir, f), skiprows=2, skipfooter=1, index_col=False, header=None, names=['runoff'], na_values='-1.2345')\n",
      "/home/mgauch/runoff-nn/gwf/lib/python3.6/site-packages/pandas/core/frame.py:6692: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    }
   ],
   "source": [
    "rdrs_data, rdrs_var_names, rdrs_time_index = load_data.load_rdrs_forcings(as_grid=True)\n",
    "data_runoff = load_data.load_discharge_gr4j_vic()\n",
    "data_runoff = data_runoff.pivot(index='date', columns='station', values='runoff')\n",
    "data_runoff = data_runoff.loc[:,data_runoff.columns != '04214500']  # This station has >1K nan target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdrs_data = rdrs_data[:,[4,5],:,:]  # only keep precipitation and temperature\n",
    "rdrs_var_names = rdrs_var_names[4:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTMRegression(nn.Module):\n",
    "    def __init__(self, input_size, output_size, batch_size, input_dim, hidden_dim, kernel_size, num_layers):\n",
    "        super(ConvLSTMRegression, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.conv_lstm = conv_lstm.ConvLSTM(input_size, input_dim, hidden_dim, kernel_size, num_layers)\n",
    "        self.linear = nn.Linear(hidden_dim * input_size[0] * input_size[1], output_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        lstm_out, hidden = self.conv_lstm(input)\n",
    "        linear_in = lstm_out[-1][:,-1,:,:,:].reshape((batch_size, -1))\n",
    "        \n",
    "        return self.linear(linear_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vars, n_rows, n_cols = rdrs_data.shape[1], rdrs_data.shape[2], rdrs_data.shape[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {}\n",
    "actuals = {}\n",
    "seq_len = 5 * 24\n",
    "validation_fraction = 0.1\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "train_start = datetime.strptime('2010-01-01', '%Y-%m-%d') + timedelta(hours=seq_len)  # first day for which to make a prediction in train set\n",
    "train_end = '2012-12-31'\n",
    "test_start = '2013-01-01'\n",
    "test_end = '2014-12-31'\n",
    "\n",
    "# For training, only include dates where no station has an nan target value.\n",
    "data_runoff = data_runoff.loc[train_start:test_end]\n",
    "non_nan_train_dates = pd.Series(data_runoff[(pd.isna(data_runoff).sum(axis=1)==0) | (data_runoff.index >= test_start)].index)\n",
    "data_runoff = data_runoff.loc[non_nan_train_dates,:]\n",
    "\n",
    "num_train_days = non_nan_train_dates[(non_nan_train_dates >= train_start) & (non_nan_train_dates <= train_end)].shape[0]\n",
    "num_total_days = num_train_days + len(pd.date_range(test_start, test_end, freq='D'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: (seq_len, samples, variables, height, width)\n",
    "x = np.zeros((seq_len, num_total_days, n_vars, n_rows, n_cols))\n",
    "i = 0\n",
    "for day in range(len(pd.date_range(train_start, test_end, freq='D'))):\n",
    "    # For each day that is to be predicted, cut out a sequence that ends with that day's 23:00 and is seq_len long\n",
    "    day_date = train_start + timedelta(days=day)\n",
    "    if len(non_nan_train_dates[non_nan_train_dates == day_date]) == 0:\n",
    "        continue\n",
    "    end_of_day_index = rdrs_time_index[rdrs_time_index == day_date].index.values[0] + 23\n",
    "    x[:,i,:,:,:] = rdrs_data[end_of_day_index - seq_len : end_of_day_index]\n",
    "    i += 1\n",
    "    \n",
    "# Scale training data\n",
    "scalers = []  # save scalers to apply them to test data later\n",
    "x_train = x[:,:num_train_days,:,:,:].copy()\n",
    "for i in range(x.shape[2]):\n",
    "    scalers.append(preprocessing.StandardScaler())\n",
    "    x_train[:,:,i,:,:] = np.nan_to_num(scalers[i].fit_transform(x_train[:,:,i,:,:].reshape((-1, 1))).reshape(x_train[:,:,i,:,:].shape))\n",
    "x_train = torch.from_numpy(x_train).float().to(device)    \n",
    "y_train = torch.from_numpy(data_runoff.loc[train_start:train_end].to_numpy()).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes: x_train torch.Size([120, 915, 2, 34, 39]), y_train torch.Size([915, 45]), x_val torch.Size([120, 101, 2, 34, 39]), y_val torch.Size([101, 45])\n"
     ]
    }
   ],
   "source": [
    "# Get validation split\n",
    "num_validation_samples = int(x_train.shape[1] * validation_fraction)\n",
    "validation_indices = np.random.choice(range(x_train.shape[1]), size=num_validation_samples)\n",
    "shuffle_indices = np.arange(x_train.shape[1])\n",
    "np.random.shuffle(shuffle_indices)\n",
    "x_train = x_train[:,shuffle_indices,:,:,:]\n",
    "y_train = y_train[shuffle_indices,:]\n",
    "\n",
    "x_val, x_train = x_train[:,-num_validation_samples:,:], x_train[:,:-num_validation_samples,:]\n",
    "y_val, y_train = y_train[-num_validation_samples:,:], y_train[:-num_validation_samples,:]\n",
    "print('Shapes: x_train {}, y_train {}, x_val {}, y_val {}'.format(x_train.shape, y_train.shape, x_val.shape, y_val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "learning_rate = 2e-3\n",
    "patience = 100\n",
    "min_improvement = 0.05\n",
    "best_loss_model = (-1, np.inf, None)\n",
    "\n",
    "# Prepare model\n",
    "H = 20\n",
    "batch_size = 5\n",
    "lstm_layers = 3\n",
    "kernel_size = (3,3)\n",
    "model = ConvLSTMRegression(x_train.shape[3:], data_runoff.shape[1], batch_size, x_train.shape[2], H, kernel_size, lstm_layers).to(device)\n",
    "loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 mean train loss:\t2982.042225405167\n",
      "Epoch 0 mean val mse:    \t1324.1949462890625,\tnse: 0.10675069917400437\n",
      "Epoch 1 mean train loss:\t2674.297116138896\n",
      "Epoch 1 mean val mse:    \t1245.1566162109375,\tnse: 0.16006687071943715\n",
      "Epoch 2 mean train loss:\t2656.3398365375124\n",
      "Epoch 2 mean val mse:    \t1155.8607177734375,\tnse: 0.22030229188112127\n",
      "Epoch 3 mean train loss:\t2640.0161976631875\n",
      "Epoch 3 mean val mse:    \t1301.391357421875,\tnse: 0.12213312753731209\n",
      "Epoch 4 mean train loss:\t2640.1683865323093\n",
      "Epoch 4 mean val mse:    \t1270.649169921875,\tnse: 0.14287044748433575\n",
      "Epoch 5 mean train loss:\t2634.376356948269\n",
      "Epoch 5 mean val mse:    \t1227.1207275390625,\tnse: 0.17223316002478906\n",
      "Epoch 6 mean train loss:\t2607.8315393833514\n",
      "Epoch 6 mean val mse:    \t1118.9278564453125,\tnse: 0.24521565642289023\n",
      "Epoch 7 mean train loss:\t2731.156963056554\n",
      "Epoch 7 mean val mse:    \t1094.3780517578125,\tnse: 0.26177606457245506\n",
      "Epoch 8 mean train loss:\t2754.1451489391225\n",
      "Epoch 8 mean val mse:    \t1148.49755859375,\tnse: 0.2252691504319727\n",
      "Epoch 9 mean train loss:\t2678.0390369853035\n",
      "Epoch 9 mean val mse:    \t1129.3658447265625,\tnse: 0.23817460220323206\n",
      "Epoch 10 mean train loss:\t2617.758762964134\n",
      "Epoch 10 mean val mse:    \t1146.7147216796875,\tnse: 0.22647173887053051\n",
      "Epoch 11 mean train loss:\t2607.770506103182\n",
      "Epoch 11 mean val mse:    \t1357.903564453125,\tnse: 0.08401207699317159\n",
      "Epoch 12 mean train loss:\t2498.6098797073782\n",
      "Epoch 12 mean val mse:    \t1040.0115966796875,\tnse: 0.2984495048484548\n",
      "Epoch 13 mean train loss:\t2405.994518259184\n",
      "Epoch 13 mean val mse:    \t1026.4395751953125,\tnse: 0.30760465115902913\n",
      "Epoch 14 mean train loss:\t2548.4615609841267\n",
      "Epoch 14 mean val mse:    \t1637.9251708984375,\tnse: -0.10487923369197172\n",
      "Epoch 15 mean train loss:\t2529.5980621504655\n",
      "Epoch 15 mean val mse:    \t1374.0506591796875,\tnse: 0.07312001539346402\n",
      "Epoch 16 mean train loss:\t2383.894163788342\n",
      "Epoch 16 mean val mse:    \t1319.1278076171875,\tnse: 0.11016884127109972\n",
      "Epoch 17 mean train loss:\t2223.155874992329\n",
      "Epoch 17 mean val mse:    \t1117.5706787109375,\tnse: 0.24613112567124373\n",
      "Epoch 18 mean train loss:\t2262.9591878672113\n",
      "Epoch 18 mean val mse:    \t1002.1126708984375,\tnse: 0.32401458972216457\n",
      "Epoch 19 mean train loss:\t2342.6324961511164\n",
      "Epoch 19 mean val mse:    \t1095.465087890625,\tnse: 0.2610427207991206\n",
      "Epoch 20 mean train loss:\t2197.7941206635023\n",
      "Epoch 20 mean val mse:    \t1300.1036376953125,\tnse: 0.12300166474265406\n",
      "Epoch 21 mean train loss:\t2322.080143245843\n",
      "Epoch 21 mean val mse:    \t1080.85400390625,\tnse: 0.27089885706198136\n",
      "Epoch 22 mean train loss:\t2077.1086971929167\n",
      "Epoch 22 mean val mse:    \t1125.3577880859375,\tnse: 0.2408783695497988\n",
      "Epoch 23 mean train loss:\t1910.4265547371954\n",
      "Epoch 23 mean val mse:    \t689.7516479492188,\tnse: 0.5347209007281051\n",
      "Epoch 24 mean train loss:\t1836.8080477688482\n",
      "Epoch 24 mean val mse:    \t757.5087280273438,\tnse: 0.4890147070691878\n",
      "Epoch 25 mean train loss:\t1849.5130261072045\n",
      "Epoch 25 mean val mse:    \t723.6403198242188,\tnse: 0.5118609914090856\n",
      "Epoch 26 mean train loss:\t1691.5527829654882\n",
      "Epoch 26 mean val mse:    \t1130.823486328125,\tnse: 0.23719141623758278\n",
      "Epoch 27 mean train loss:\t1700.1712794903196\n",
      "Epoch 27 mean val mse:    \t806.174072265625,\tnse: 0.4561870053416601\n",
      "Epoch 28 mean train loss:\t1561.5057104141986\n",
      "Epoch 28 mean val mse:    \t1016.4042358398438,\tnse: 0.31437407869022505\n",
      "Epoch 29 mean train loss:\t1535.4256166135026\n",
      "Epoch 29 mean val mse:    \t918.845458984375,\tnse: 0.38018334438914625\n",
      "Epoch 30 mean train loss:\t1441.8242187291546\n",
      "Epoch 30 mean val mse:    \t847.980224609375,\tnse: 0.42798619058927234\n",
      "Epoch 31 mean train loss:\t1390.0880906151945\n",
      "Epoch 31 mean val mse:    \t795.8248291015625,\tnse: 0.4631681766205875\n",
      "Epoch 32 mean train loss:\t1450.4696285060195\n",
      "Epoch 32 mean val mse:    \t1167.8631591796875,\tnse: 0.21220589203695162\n",
      "Epoch 33 mean train loss:\t1637.2809650858894\n",
      "Epoch 33 mean val mse:    \t757.0875854492188,\tnse: 0.48929874979993704\n",
      "Epoch 34 mean train loss:\t2231.397128496014\n",
      "Epoch 34 mean val mse:    \t1058.5167236328125,\tnse: 0.2859666017846526\n",
      "Epoch 35 mean train loss:\t2230.1018439120935\n",
      "Epoch 35 mean val mse:    \t1475.4678955078125,\tnse: 0.004707938084689611\n",
      "Epoch 36 mean train loss:\t2813.7710902313065\n",
      "Epoch 36 mean val mse:    \t1466.267578125,\tnse: 0.010914130894822072\n",
      "Epoch 37 mean train loss:\t2422.611198550365\n",
      "Epoch 37 mean val mse:    \t952.1326904296875,\tnse: 0.35772914418607193\n",
      "Epoch 38 mean train loss:\t2196.111376277736\n",
      "Epoch 38 mean val mse:    \t1060.5203857421875,\tnse: 0.28461496785332974\n",
      "Epoch 39 mean train loss:\t2147.3302325055897\n",
      "Epoch 39 mean val mse:    \t803.078125,\tnse: 0.45827544077442184\n",
      "Epoch 40 mean train loss:\t1922.1650334551034\n",
      "Epoch 40 mean val mse:    \t826.5175170898438,\tnse: 0.4424641400318875\n",
      "Epoch 41 mean train loss:\t1708.8573547321591\n",
      "Epoch 41 mean val mse:    \t729.0999145507812,\tnse: 0.5081782137000379\n",
      "Epoch 42 mean train loss:\t1823.010436719884\n",
      "Epoch 42 mean val mse:    \t861.4657592773438,\tnse: 0.41888944955588536\n",
      "Epoch 43 mean train loss:\t1899.6222540016383\n",
      "Epoch 43 mean val mse:    \t909.163818359375,\tnse: 0.38671417552170084\n",
      "Epoch 44 mean train loss:\t1681.7163252908676\n",
      "Epoch 44 mean val mse:    \t899.8165283203125,\tnse: 0.39301948302489176\n",
      "Epoch 45 mean train loss:\t1493.25246346062\n",
      "Epoch 45 mean val mse:    \t752.759033203125,\tnse: 0.49221861926865185\n",
      "Epoch 46 mean train loss:\t1482.5218714312778\n",
      "Epoch 46 mean val mse:    \t1050.1873779296875,\tnse: 0.2915853225965386\n",
      "Epoch 47 mean train loss:\t1397.2007639316914\n",
      "Epoch 47 mean val mse:    \t793.86767578125,\tnse: 0.4644883837164989\n",
      "Epoch 48 mean train loss:\t1325.3463753872231\n",
      "Epoch 48 mean val mse:    \t980.8133544921875,\tnse: 0.3383822310053105\n",
      "Epoch 49 mean train loss:\t1360.118704123575\n",
      "Epoch 49 mean val mse:    \t744.8192138671875,\tnse: 0.4975744599753289\n",
      "Epoch 50 mean train loss:\t1241.7988315019452\n",
      "Epoch 50 mean val mse:    \t1267.470947265625,\tnse: 0.14501439553282325\n",
      "Epoch 51 mean train loss:\t1293.7640665606723\n",
      "Epoch 51 mean val mse:    \t731.0233764648438,\tnse: 0.5068806432182328\n",
      "Epoch 52 mean train loss:\t1059.0979868570964\n",
      "Epoch 52 mean val mse:    \t688.898193359375,\tnse: 0.5352966466907036\n",
      "Epoch 53 mean train loss:\t1064.8921484608468\n",
      "Epoch 53 mean val mse:    \t842.0509033203125,\tnse: 0.4319859400152942\n",
      "Epoch 54 mean train loss:\t1204.4162496764802\n",
      "Epoch 54 mean val mse:    \t686.1488037109375,\tnse: 0.537151270264415\n",
      "Epoch 55 mean train loss:\t1151.6665588962576\n",
      "Epoch 55 mean val mse:    \t812.05078125,\tnse: 0.45222281659583863\n",
      "Epoch 56 mean train loss:\t1064.5096598974342\n",
      "Epoch 56 mean val mse:    \t1133.7816162109375,\tnse: 0.2351959796262708\n",
      "Epoch 57 mean train loss:\t1009.3263811830615\n",
      "Epoch 57 mean val mse:    \t686.9653930664062,\tnse: 0.53660040011442\n",
      "Epoch 58 mean train loss:\t944.2979577069725\n",
      "Epoch 58 mean val mse:    \t714.1837158203125,\tnse: 0.5182399979807278\n",
      "Epoch 59 mean train loss:\t934.9391491582485\n",
      "Epoch 59 mean val mse:    \t654.9859008789062,\tnse: 0.5581724877690872\n",
      "Epoch 60 mean train loss:\t1291.3912409381137\n",
      "Epoch 60 mean val mse:    \t682.9996948242188,\tnse: 0.5392755427951534\n",
      "Epoch 61 mean train loss:\t1151.2861611413175\n",
      "Epoch 61 mean val mse:    \t767.8160400390625,\tnse: 0.48206176194343453\n",
      "Epoch 62 mean train loss:\t941.9037835584963\n",
      "Epoch 62 mean val mse:    \t888.3184204101562,\tnse: 0.4007756021363136\n",
      "Epoch 63 mean train loss:\t857.2442553160621\n",
      "Epoch 63 mean val mse:    \t760.8819580078125,\tnse: 0.486739237138368\n",
      "Epoch 64 mean train loss:\t831.1434359315966\n",
      "Epoch 64 mean val mse:    \t649.06201171875,\tnse: 0.5621685324270501\n",
      "Epoch 65 mean train loss:\t936.4095200919063\n",
      "Epoch 65 mean val mse:    \t713.3648071289062,\tnse: 0.5187924552873259\n",
      "Epoch 66 mean train loss:\t1262.817438010961\n",
      "Epoch 66 mean val mse:    \t740.3663940429688,\tnse: 0.5005782202656508\n",
      "Epoch 67 mean train loss:\t1043.4706019208732\n",
      "Epoch 67 mean val mse:    \t652.13134765625,\tnse: 0.560098103576548\n",
      "Epoch 68 mean train loss:\t955.2579092223788\n",
      "Epoch 68 mean val mse:    \t641.5269775390625,\tnse: 0.5672513601083214\n",
      "Epoch 69 mean train loss:\t917.2665246640398\n",
      "Epoch 69 mean val mse:    \t653.2160034179688,\tnse: 0.5593664561486168\n",
      "Epoch 70 mean train loss:\t763.9138969879984\n",
      "Epoch 70 mean val mse:    \t857.2923583984375,\tnse: 0.4217045874943408\n",
      "Epoch 71 mean train loss:\t758.9457725566593\n",
      "Epoch 71 mean val mse:    \t665.0524291992188,\tnse: 0.5513820448331701\n",
      "Epoch 72 mean train loss:\t699.3054979668289\n",
      "Epoch 72 mean val mse:    \t621.1774291992188,\tnse: 0.5809783665286027\n",
      "Epoch 73 mean train loss:\t660.9339980245288\n",
      "Epoch 73 mean val mse:    \t719.6396484375,\tnse: 0.514559654430351\n",
      "Epoch 74 mean train loss:\t660.4291437909902\n",
      "Epoch 74 mean val mse:    \t595.7532348632812,\tnse: 0.5981285300197363\n",
      "Epoch 75 mean train loss:\t689.7897072255286\n",
      "Epoch 75 mean val mse:    \t613.9011840820312,\tnse: 0.5858866738090485\n",
      "Epoch 76 mean train loss:\t664.3977083925341\n",
      "Epoch 76 mean val mse:    \t622.5524291992188,\tnse: 0.5800508534083451\n",
      "Epoch 77 mean train loss:\t721.2848319381964\n",
      "Epoch 77 mean val mse:    \t588.59130859375,\tnse: 0.6029596503300172\n",
      "Epoch 78 mean train loss:\t701.8781928390753\n",
      "Epoch 78 mean val mse:    \t694.682861328125,\tnse: 0.531394493696731\n",
      "Epoch 79 mean train loss:\t938.4244225298772\n",
      "Epoch 79 mean val mse:    \t531.0399169921875,\tnse: 0.6417815968281749\n",
      "Epoch 80 mean train loss:\t642.8547290739466\n",
      "Epoch 80 mean val mse:    \t511.6445617675781,\tnse: 0.6548648713271492\n",
      "Epoch 81 mean train loss:\t593.8922365324094\n",
      "Epoch 81 mean val mse:    \t549.4232177734375,\tnse: 0.6293809225977958\n",
      "Epoch 82 mean train loss:\t504.5961493820441\n",
      "Epoch 82 mean val mse:    \t609.1559448242188,\tnse: 0.5890876110054091\n",
      "Epoch 83 mean train loss:\t518.8546157378316\n",
      "Epoch 83 mean val mse:    \t660.5983276367188,\tnse: 0.5543866026070863\n",
      "Epoch 84 mean train loss:\t633.3543810401458\n",
      "Epoch 84 mean val mse:    \t596.1968994140625,\tnse: 0.597829232466514\n",
      "Epoch 85 mean train loss:\t687.7530486518568\n",
      "Epoch 85 mean val mse:    \t637.4360961914062,\tnse: 0.5700108934970052\n",
      "Epoch 86 mean train loss:\t526.5204559909841\n",
      "Epoch 86 mean val mse:    \t589.4131469726562,\tnse: 0.6024052690549728\n",
      "Epoch 87 mean train loss:\t472.27526203009603\n",
      "Epoch 87 mean val mse:    \t602.1053466796875,\tnse: 0.5938436459824001\n",
      "Epoch 88 mean train loss:\t469.9390265251118\n",
      "Epoch 88 mean val mse:    \t594.2666015625,\tnse: 0.5991313261795307\n",
      "Epoch 89 mean train loss:\t435.2541952914879\n",
      "Epoch 89 mean val mse:    \t681.2432250976562,\tnse: 0.5404603706331375\n",
      "Epoch 90 mean train loss:\t487.9665037895161\n",
      "Epoch 90 mean val mse:    \t582.8577880859375,\tnse: 0.6068272860083979\n",
      "Epoch 91 mean train loss:\t447.72450137529216\n",
      "Epoch 91 mean val mse:    \t537.0552368164062,\tnse: 0.6377238861520881\n",
      "Epoch 92 mean train loss:\t481.56647162098704\n",
      "Epoch 92 mean val mse:    \t626.9559936523438,\tnse: 0.5770803530355579\n",
      "Epoch 93 mean train loss:\t446.3020651040833\n",
      "Epoch 93 mean val mse:    \t579.0934448242188,\tnse: 0.6093665617388542\n",
      "Epoch 94 mean train loss:\t432.29368332826374\n",
      "Epoch 94 mean val mse:    \t478.0296630859375,\tnse: 0.6775401576464173\n",
      "Epoch 95 mean train loss:\t378.515444020756\n",
      "Epoch 95 mean val mse:    \t666.9207763671875,\tnse: 0.5501216947955894\n",
      "Epoch 96 mean train loss:\t388.2692021646135\n",
      "Epoch 96 mean val mse:    \t585.1417846679688,\tnse: 0.6052865752499349\n",
      "Epoch 97 mean train loss:\t462.9207933978305\n",
      "Epoch 97 mean val mse:    \t644.4868774414062,\tnse: 0.5652546925174258\n",
      "Epoch 98 mean train loss:\t791.7110257591706\n",
      "Epoch 98 mean val mse:    \t824.9806518554688,\tnse: 0.4435007850684384\n",
      "Epoch 99 mean train loss:\t1138.9329801882552\n",
      "Epoch 99 mean val mse:    \t756.3589477539062,\tnse: 0.48979029309829003\n",
      "Epoch 100 mean train loss:\t869.2107848704187\n",
      "Epoch 100 mean val mse:    \t631.0645751953125,\tnse: 0.5743089240219875\n",
      "Epoch 101 mean train loss:\t825.263620657999\n",
      "Epoch 101 mean val mse:    \t945.8545532226562,\tnse: 0.3619640804251971\n",
      "Epoch 102 mean train loss:\t814.8785223413686\n",
      "Epoch 102 mean val mse:    \t571.7952270507812,\tnse: 0.6142896592620757\n",
      "Epoch 103 mean train loss:\t1097.9402890022986\n",
      "Epoch 103 mean val mse:    \t909.7335815429688,\tnse: 0.386329809071255\n",
      "Epoch 104 mean train loss:\t697.7696080442335\n",
      "Epoch 104 mean val mse:    \t691.0387573242188,\tnse: 0.5338526831993009\n",
      "Epoch 105 mean train loss:\t666.919773122652\n",
      "Epoch 105 mean val mse:    \t699.3401489257812,\tnse: 0.5282528630842289\n",
      "Epoch 106 mean train loss:\t747.5460586339398\n",
      "Epoch 106 mean val mse:    \t943.4868774414062,\tnse: 0.3635612202346471\n",
      "Epoch 107 mean train loss:\t1877.578320195766\n",
      "Epoch 107 mean val mse:    \t1249.7769775390625,\tnse: 0.15695004212956387\n",
      "Epoch 108 mean train loss:\t1361.6350712593787\n",
      "Epoch 108 mean val mse:    \t743.727294921875,\tnse: 0.4983111165273112\n",
      "Epoch 109 mean train loss:\t1078.76186395864\n",
      "Epoch 109 mean val mse:    \t770.1426391601562,\tnse: 0.48049226308287785\n",
      "Epoch 110 mean train loss:\t818.2748875435584\n",
      "Epoch 110 mean val mse:    \t621.001220703125,\tnse: 0.5810972332321045\n",
      "Epoch 111 mean train loss:\t732.4818791457212\n",
      "Epoch 111 mean val mse:    \t778.78369140625,\tnse: 0.4746634501721322\n",
      "Epoch 112 mean train loss:\t731.6493104820044\n",
      "Epoch 112 mean val mse:    \t980.0018920898438,\tnse: 0.33892961785343\n",
      "Epoch 113 mean train loss:\t758.9617010231226\n",
      "Epoch 113 mean val mse:    \t698.4882202148438,\tnse: 0.5288275500442292\n",
      "Epoch 114 mean train loss:\t747.1682195194433\n",
      "Epoch 114 mean val mse:    \t633.3829345703125,\tnse: 0.5727450032256276\n",
      "Epoch 115 mean train loss:\t672.1313928072569\n",
      "Epoch 115 mean val mse:    \t688.7149047851562,\tnse: 0.5354202863490367\n",
      "Epoch 116 mean train loss:\t637.509179078816\n",
      "Epoch 116 mean val mse:    \t591.4407958984375,\tnse: 0.6010375446852942\n",
      "Epoch 117 mean train loss:\t550.1477617982958\n",
      "Epoch 117 mean val mse:    \t536.0575561523438,\tnse: 0.6383968778838199\n",
      "Epoch 118 mean train loss:\t532.9275044092063\n",
      "Epoch 118 mean val mse:    \t522.3408203125,\tnse: 0.6476496185508125\n",
      "Epoch 119 mean train loss:\t479.169163354759\n",
      "Epoch 119 mean val mse:    \t521.1776733398438,\tnse: 0.648434265492722\n",
      "Epoch 120 mean train loss:\t457.27672399718904\n",
      "Epoch 120 mean val mse:    \t607.01708984375,\tnse: 0.5905303384939395\n",
      "Epoch 121 mean train loss:\t460.65282371395926\n",
      "Epoch 121 mean val mse:    \t498.306884765625,\tnse: 0.6638619678994452\n",
      "Epoch 122 mean train loss:\t497.97063971347495\n",
      "Epoch 122 mean val mse:    \t548.6629638671875,\tnse: 0.6298937141664271\n",
      "Epoch 123 mean train loss:\t702.5667695634352\n",
      "Epoch 123 mean val mse:    \t932.824462890625,\tnse: 0.3707536717992237\n",
      "Epoch 124 mean train loss:\t601.5636384682577\n",
      "Epoch 124 mean val mse:    \t499.931884765625,\tnse: 0.6627658035694486\n",
      "Epoch 125 mean train loss:\t450.3959330053277\n",
      "Epoch 125 mean val mse:    \t500.3407897949219,\tnse: 0.6624899692259089\n",
      "Epoch 126 mean train loss:\t399.2994162971205\n",
      "Epoch 126 mean val mse:    \t508.9397888183594,\tnse: 0.6566894491398316\n",
      "Epoch 127 mean train loss:\t377.9644230284977\n",
      "Epoch 127 mean val mse:    \t549.31298828125,\tnse: 0.629455233199527\n",
      "Epoch 128 mean train loss:\t398.58613115321094\n",
      "Epoch 128 mean val mse:    \t517.412353515625,\tnse: 0.650974153820392\n",
      "Epoch 129 mean train loss:\t373.50010733265697\n",
      "Epoch 129 mean val mse:    \t497.968994140625,\tnse: 0.6640898718193692\n",
      "Epoch 130 mean train loss:\t364.9666354486851\n",
      "Epoch 130 mean val mse:    \t526.4849853515625,\tnse: 0.64485413001229\n",
      "Epoch 131 mean train loss:\t358.42489522923535\n",
      "Epoch 131 mean val mse:    \t481.7137145996094,\tnse: 0.6750550401972898\n",
      "Epoch 132 mean train loss:\t382.73389463997927\n",
      "Epoch 132 mean val mse:    \t524.66357421875,\tnse: 0.6460828152285729\n",
      "Epoch 133 mean train loss:\t335.310340995997\n",
      "Epoch 133 mean val mse:    \t581.6215209960938,\tnse: 0.6076612062080993\n",
      "Epoch 134 mean train loss:\t733.1686033030026\n",
      "Epoch 134 mean val mse:    \t743.355712890625,\tnse: 0.49856172764167805\n",
      "Epoch 135 mean train loss:\t676.7455925654843\n",
      "Epoch 135 mean val mse:    \t833.6009521484375,\tnse: 0.4376858940259283\n",
      "Epoch 136 mean train loss:\t784.8349926641079\n",
      "Epoch 136 mean val mse:    \t611.6140747070312,\tnse: 0.5874294536740604\n",
      "Epoch 137 mean train loss:\t487.78030370493406\n",
      "Epoch 137 mean val mse:    \t566.5709228515625,\tnse: 0.6178137189983864\n",
      "Epoch 138 mean train loss:\t435.01574571536537\n",
      "Epoch 138 mean val mse:    \t553.0817260742188,\tnse: 0.6269130148337144\n",
      "Epoch 139 mean train loss:\t414.3455820344185\n",
      "Epoch 139 mean val mse:    \t509.22467041015625,\tnse: 0.6564972480260031\n",
      "Epoch 140 mean train loss:\t379.97269475134345\n",
      "Epoch 140 mean val mse:    \t499.603271484375,\tnse: 0.6629874818482444\n",
      "Epoch 141 mean train loss:\t346.0086422279233\n",
      "Epoch 141 mean val mse:    \t457.79498291015625,\tnse: 0.6911896840677123\n",
      "Epoch 142 mean train loss:\t346.6686783462274\n",
      "Epoch 142 mean val mse:    \t570.9335327148438,\tnse: 0.6148708671029208\n",
      "Epoch 143 mean train loss:\t313.04648823555704\n",
      "Epoch 143 mean val mse:    \t487.81549072265625,\tnse: 0.6709390203043317\n",
      "Epoch 144 mean train loss:\t310.9400153967852\n",
      "Epoch 144 mean val mse:    \t450.1856689453125,\tnse: 0.6963226135736545\n",
      "Epoch 145 mean train loss:\t309.31350557921365\n",
      "Epoch 145 mean val mse:    \t459.47027587890625,\tnse: 0.6900595907804814\n",
      "Epoch 146 mean train loss:\t510.9656330588085\n",
      "Epoch 146 mean val mse:    \t512.604248046875,\tnse: 0.6542175005697859\n",
      "Epoch 147 mean train loss:\t432.05021302687015\n",
      "Epoch 147 mean val mse:    \t484.8752746582031,\tnse: 0.6729223988638082\n",
      "Epoch 148 mean train loss:\t334.53432124820563\n",
      "Epoch 148 mean val mse:    \t425.0301818847656,\tnse: 0.7132915258616992\n",
      "Epoch 149 mean train loss:\t317.1133524576823\n",
      "Epoch 149 mean val mse:    \t421.167724609375,\tnse: 0.7158969806537175\n",
      "Epoch 150 mean train loss:\t292.2595360969585\n",
      "Epoch 150 mean val mse:    \t413.4825134277344,\tnse: 0.721081117521186\n",
      "Epoch 151 mean train loss:\t267.93493996291863\n",
      "Epoch 151 mean val mse:    \t430.8953857421875,\tnse: 0.709335070392968\n",
      "Epoch 152 mean train loss:\t275.418381737881\n",
      "Epoch 152 mean val mse:    \t442.31439208984375,\tnse: 0.7016322725440503\n",
      "Epoch 153 mean train loss:\t341.7052717156749\n",
      "Epoch 153 mean val mse:    \t510.0082092285156,\tnse: 0.6559686913431221\n",
      "Epoch 154 mean train loss:\t323.91075185348427\n",
      "Epoch 154 mean val mse:    \t479.04638671875,\tnse: 0.6768543035514553\n",
      "Epoch 155 mean train loss:\t267.5625865915434\n",
      "Epoch 155 mean val mse:    \t476.13848876953125,\tnse: 0.6788158798108275\n",
      "Epoch 156 mean train loss:\t230.01953175028817\n",
      "Epoch 156 mean val mse:    \t469.66900634765625,\tnse: 0.6831799661368352\n",
      "Epoch 157 mean train loss:\t221.5883300239271\n",
      "Epoch 157 mean val mse:    \t459.3002624511719,\tnse: 0.6901742723435464\n",
      "Epoch 158 mean train loss:\t216.9747203555915\n",
      "Epoch 158 mean val mse:    \t429.4875183105469,\tnse: 0.710284766998942\n",
      "Epoch 159 mean train loss:\t218.14152252739245\n",
      "Epoch 159 mean val mse:    \t474.1514587402344,\tnse: 0.6801562707969173\n",
      "Epoch 160 mean train loss:\t232.81628157401997\n",
      "Epoch 160 mean val mse:    \t461.9266052246094,\tnse: 0.6884026504762011\n",
      "Epoch 161 mean train loss:\t228.79520821701632\n",
      "Epoch 161 mean val mse:    \t413.9171142578125,\tnse: 0.7207879410774074\n",
      "Epoch 162 mean train loss:\t264.7461264250708\n",
      "Epoch 162 mean val mse:    \t525.2838745117188,\tnse: 0.6456643936725708\n",
      "Epoch 163 mean train loss:\t266.09675531439444\n",
      "Epoch 163 mean val mse:    \t410.3636779785156,\tnse: 0.723184962880721\n",
      "Epoch 164 mean train loss:\t251.60314301454304\n",
      "Epoch 164 mean val mse:    \t578.2088012695312,\tnse: 0.6099633118230441\n",
      "Epoch 165 mean train loss:\t278.8120848650489\n",
      "Epoch 165 mean val mse:    \t382.4096374511719,\tnse: 0.7420416359398445\n",
      "Epoch 166 mean train loss:\t395.16808700561523\n",
      "Epoch 166 mean val mse:    \t464.5382385253906,\tnse: 0.6866409295146689\n",
      "Epoch 167 mean train loss:\t443.2043146227227\n",
      "Epoch 167 mean val mse:    \t490.45111083984375,\tnse: 0.6691611370001165\n",
      "Epoch 168 mean train loss:\t420.17982070172417\n",
      "Epoch 168 mean val mse:    \t514.8549194335938,\tnse: 0.6526993047219294\n",
      "Epoch 169 mean train loss:\t332.15125783284503\n",
      "Epoch 169 mean val mse:    \t437.6864929199219,\tnse: 0.7047540768755858\n",
      "Epoch 170 mean train loss:\t292.35395725437854\n",
      "Epoch 170 mean val mse:    \t439.9959411621094,\tnse: 0.7031962017047249\n",
      "Epoch 171 mean train loss:\t309.39692382604045\n",
      "Epoch 171 mean val mse:    \t764.2822265625,\tnse: 0.4844455202397091\n",
      "Epoch 172 mean train loss:\t341.8927314633229\n",
      "Epoch 172 mean val mse:    \t423.97601318359375,\tnse: 0.7140026089449669\n",
      "Epoch 173 mean train loss:\t223.42375702154442\n",
      "Epoch 173 mean val mse:    \t462.2452087402344,\tnse: 0.688187741657206\n",
      "Epoch 174 mean train loss:\t237.28763117555712\n",
      "Epoch 174 mean val mse:    \t411.34918212890625,\tnse: 0.7225201632533181\n",
      "Epoch 175 mean train loss:\t213.49784909441172\n",
      "Epoch 175 mean val mse:    \t446.17291259765625,\tnse: 0.6990294390837075\n",
      "Epoch 176 mean train loss:\t205.46344267214582\n",
      "Epoch 176 mean val mse:    \t442.4627685546875,\tnse: 0.7015321716011691\n",
      "Epoch 177 mean train loss:\t262.6820500379052\n",
      "Epoch 177 mean val mse:    \t571.1690063476562,\tnse: 0.6147121050769104\n",
      "Epoch 178 mean train loss:\t317.6047447809105\n",
      "Epoch 178 mean val mse:    \t533.90869140625,\tnse: 0.6398464254696006\n",
      "Epoch 179 mean train loss:\t380.0299337418353\n",
      "Epoch 179 mean val mse:    \t477.1433410644531,\tnse: 0.6781380416537621\n",
      "Epoch 180 mean train loss:\t269.60533011806467\n",
      "Epoch 180 mean val mse:    \t444.1839599609375,\tnse: 0.7003711199817724\n",
      "Epoch 181 mean train loss:\t244.44002123348048\n",
      "Epoch 181 mean val mse:    \t466.2171630859375,\tnse: 0.6855083977994703\n",
      "Epoch 182 mean train loss:\t229.46431552907808\n",
      "Epoch 182 mean val mse:    \t398.0237731933594,\tnse: 0.7315089639340597\n",
      "Epoch 183 mean train loss:\t244.98124448849202\n",
      "Epoch 183 mean val mse:    \t423.789306640625,\tnse: 0.7141285529010308\n",
      "Epoch 184 mean train loss:\t701.5472624731846\n",
      "Epoch 184 mean val mse:    \t561.3060913085938,\tnse: 0.6213651973246375\n",
      "Epoch 185 mean train loss:\t460.46662640180745\n",
      "Epoch 185 mean val mse:    \t479.9984436035156,\tnse: 0.6762121160187576\n",
      "Epoch 186 mean train loss:\t320.9325106886567\n",
      "Epoch 186 mean val mse:    \t552.4730834960938,\tnse: 0.6273235883311761\n",
      "Epoch 187 mean train loss:\t256.71677338230154\n",
      "Epoch 187 mean val mse:    \t456.1120300292969,\tnse: 0.6923249248170256\n",
      "Epoch 188 mean train loss:\t238.58030378623087\n",
      "Epoch 188 mean val mse:    \t490.9774475097656,\tnse: 0.6688060886242666\n",
      "Epoch 189 mean train loss:\t215.078542917804\n",
      "Epoch 189 mean val mse:    \t420.8021240234375,\tnse: 0.7161435890865959\n",
      "Epoch 190 mean train loss:\t185.68349012781363\n",
      "Epoch 190 mean val mse:    \t429.46734619140625,\tnse: 0.7102983849728225\n",
      "Epoch 191 mean train loss:\t191.070200998275\n",
      "Epoch 191 mean val mse:    \t511.1854553222656,\tnse: 0.6551745870848793\n",
      "Epoch 192 mean train loss:\t186.5143546849652\n",
      "Epoch 192 mean val mse:    \t441.1927185058594,\tnse: 0.7023889193510302\n",
      "Epoch 193 mean train loss:\t186.90513264807197\n",
      "Epoch 193 mean val mse:    \t465.20452880859375,\tnse: 0.6861914991420359\n",
      "Epoch 194 mean train loss:\t224.34674322409708\n",
      "Epoch 194 mean val mse:    \t404.8034362792969,\tnse: 0.7269356804467246\n",
      "Epoch 195 mean train loss:\t197.46964739189772\n",
      "Epoch 195 mean val mse:    \t419.82415771484375,\tnse: 0.7168032767452799\n",
      "Epoch 196 mean train loss:\t201.6516969086694\n",
      "Epoch 196 mean val mse:    \t444.6551818847656,\tnse: 0.7000532882224778\n",
      "Epoch 197 mean train loss:\t212.2442627265805\n",
      "Epoch 197 mean val mse:    \t477.19500732421875,\tnse: 0.6781031748169881\n",
      "Epoch 198 mean train loss:\t240.07147427334812\n",
      "Epoch 198 mean val mse:    \t415.40985107421875,\tnse: 0.719780990667412\n",
      "Epoch 199 mean train loss:\t210.5821666092169\n",
      "Epoch 199 mean val mse:    \t370.18328857421875,\tnse: 0.7502890528939903\n",
      "Epoch 200 mean train loss:\t184.5028959493168\n",
      "Epoch 200 mean val mse:    \t404.1374816894531,\tnse: 0.7273849015152478\n",
      "Epoch 201 mean train loss:\t341.45790581625016\n",
      "Epoch 201 mean val mse:    \t477.2414855957031,\tnse: 0.678071804279742\n",
      "Epoch 202 mean train loss:\t220.97798623673904\n",
      "Epoch 202 mean val mse:    \t406.90484619140625,\tnse: 0.7255181460755524\n",
      "Epoch 203 mean train loss:\t197.84092324679014\n",
      "Epoch 203 mean val mse:    \t415.4828796386719,\tnse: 0.7197317384612311\n",
      "Epoch 204 mean train loss:\t165.30889150484012\n",
      "Epoch 204 mean val mse:    \t364.6631774902344,\tnse: 0.7540127128223977\n",
      "Epoch 205 mean train loss:\t157.0692548595491\n",
      "Epoch 205 mean val mse:    \t393.25689697265625,\tnse: 0.7347245179868439\n",
      "Epoch 206 mean train loss:\t173.67571377363362\n",
      "Epoch 206 mean val mse:    \t402.6625061035156,\tnse: 0.7283798781925247\n",
      "Epoch 207 mean train loss:\t156.02651200007872\n",
      "Epoch 207 mean val mse:    \t390.13531494140625,\tnse: 0.7368302108238489\n",
      "Epoch 208 mean train loss:\t152.43815436649842\n",
      "Epoch 208 mean val mse:    \t389.6669006347656,\tnse: 0.7371462025346252\n",
      "Epoch 209 mean train loss:\t154.859988874425\n",
      "Epoch 209 mean val mse:    \t395.2459411621094,\tnse: 0.7333827638308492\n",
      "Epoch 210 mean train loss:\t148.40828578719675\n",
      "Epoch 210 mean val mse:    \t389.01068115234375,\tnse: 0.7375888407632213\n",
      "Epoch 211 mean train loss:\t159.82511168099492\n",
      "Epoch 211 mean val mse:    \t489.5634460449219,\tnse: 0.6697599539162182\n",
      "Epoch 212 mean train loss:\t154.16900778598472\n",
      "Epoch 212 mean val mse:    \t371.38507080078125,\tnse: 0.7494783969458465\n",
      "Epoch 213 mean train loss:\t140.29584044576342\n",
      "Epoch 213 mean val mse:    \t355.5780029296875,\tnse: 0.7601411971780432\n",
      "Epoch 214 mean train loss:\t145.4543935744489\n",
      "Epoch 214 mean val mse:    \t356.8363952636719,\tnse: 0.7592923377278049\n",
      "Epoch 215 mean train loss:\t137.58110773889092\n",
      "Epoch 215 mean val mse:    \t335.0751953125,\tnse: 0.7739715827248314\n",
      "Epoch 216 mean train loss:\t141.89919020439106\n",
      "Epoch 216 mean val mse:    \t347.7039489746094,\tnse: 0.7654527221516157\n",
      "Epoch 217 mean train loss:\t137.75282489797456\n",
      "Epoch 217 mean val mse:    \t387.7241516113281,\tnse: 0.7384566840686115\n",
      "Epoch 218 mean train loss:\t137.1080079104731\n",
      "Epoch 218 mean val mse:    \t369.0540771484375,\tnse: 0.7510507670369365\n",
      "Epoch 219 mean train loss:\t141.37897606104448\n",
      "Epoch 219 mean val mse:    \t377.8138122558594,\tnse: 0.7451418183748549\n",
      "Epoch 220 mean train loss:\t158.0816304983337\n",
      "Epoch 220 mean val mse:    \t417.42742919921875,\tnse: 0.718420022909563\n",
      "Epoch 221 mean train loss:\t177.17496487351715\n",
      "Epoch 221 mean val mse:    \t358.3463439941406,\tnse: 0.7582738031308122\n",
      "Epoch 222 mean train loss:\t177.884770617459\n",
      "Epoch 222 mean val mse:    \t324.0787353515625,\tnse: 0.7813893496899982\n",
      "Epoch 223 mean train loss:\t138.06949711367082\n",
      "Epoch 223 mean val mse:    \t343.93572998046875,\tnse: 0.7679946027067913\n",
      "Epoch 224 mean train loss:\t148.95932708281637\n",
      "Epoch 224 mean val mse:    \t374.33648681640625,\tnse: 0.7474874738596621\n",
      "Epoch 225 mean train loss:\t137.29762237840663\n",
      "Epoch 225 mean val mse:    \t366.0306701660156,\tnse: 0.7530902278886393\n",
      "Epoch 226 mean train loss:\t122.83515751948121\n",
      "Epoch 226 mean val mse:    \t374.0140380859375,\tnse: 0.7477049832923655\n",
      "Epoch 227 mean train loss:\t126.68798754124042\n",
      "Epoch 227 mean val mse:    \t362.8244323730469,\tnse: 0.7552530459321452\n",
      "Epoch 228 mean train loss:\t112.77817704247647\n",
      "Epoch 228 mean val mse:    \t363.8573303222656,\tnse: 0.7545562758611031\n",
      "Epoch 229 mean train loss:\t122.77820532960318\n",
      "Epoch 229 mean val mse:    \t336.5332946777344,\tnse: 0.7729880013564641\n",
      "Epoch 230 mean train loss:\t127.26540062075756\n",
      "Epoch 230 mean val mse:    \t349.985595703125,\tnse: 0.7639136258814924\n",
      "Epoch 231 mean train loss:\t108.69090211586874\n",
      "Epoch 231 mean val mse:    \t370.73760986328125,\tnse: 0.7499151363727253\n",
      "Epoch 232 mean train loss:\t111.48845049331749\n",
      "Epoch 232 mean val mse:    \t360.3079528808594,\tnse: 0.756950552263347\n",
      "Epoch 233 mean train loss:\t135.7794809393544\n",
      "Epoch 233 mean val mse:    \t326.9271240234375,\tnse: 0.7794679412349517\n",
      "Epoch 234 mean train loss:\t114.91315149609508\n",
      "Epoch 234 mean val mse:    \t357.0528259277344,\tnse: 0.7591463301552269\n",
      "Epoch 235 mean train loss:\t128.4714252596996\n",
      "Epoch 235 mean val mse:    \t337.5406799316406,\tnse: 0.772308460137919\n",
      "Epoch 236 mean train loss:\t128.13661477344283\n",
      "Epoch 236 mean val mse:    \t362.5046081542969,\tnse: 0.7554688004078429\n",
      "Epoch 237 mean train loss:\t112.17016496293532\n",
      "Epoch 237 mean val mse:    \t320.47216796875,\tnse: 0.783822198759379\n",
      "Epoch 238 mean train loss:\t97.54615906678914\n",
      "Epoch 238 mean val mse:    \t318.0843811035156,\tnse: 0.7854328961032857\n",
      "Epoch 239 mean train loss:\t94.12868096398526\n",
      "Epoch 239 mean val mse:    \t335.81976318359375,\tnse: 0.7734693171079308\n",
      "Epoch 240 mean train loss:\t108.59538977784537\n",
      "Epoch 240 mean val mse:    \t396.82415771484375,\tnse: 0.7323181733257028\n",
      "Epoch 241 mean train loss:\t176.67008788728975\n",
      "Epoch 241 mean val mse:    \t392.1141052246094,\tnse: 0.735495413321491\n",
      "Epoch 242 mean train loss:\t218.04369116611167\n",
      "Epoch 242 mean val mse:    \t335.1109313964844,\tnse: 0.7739474544152285\n",
      "Epoch 243 mean train loss:\t120.93469045461853\n",
      "Epoch 243 mean val mse:    \t324.8795471191406,\tnse: 0.7808491558199008\n",
      "Epoch 244 mean train loss:\t101.15133002025834\n",
      "Epoch 244 mean val mse:    \t355.6321716308594,\tnse: 0.7601046609085667\n",
      "Epoch 245 mean train loss:\t102.22868225222729\n",
      "Epoch 245 mean val mse:    \t306.7043762207031,\tnse: 0.7931094178473709\n",
      "Epoch 246 mean train loss:\t104.90762520878692\n",
      "Epoch 246 mean val mse:    \t315.9217834472656,\tnse: 0.7868917041344882\n",
      "Epoch 247 mean train loss:\t96.45927626187684\n",
      "Epoch 247 mean val mse:    \t331.5406799316406,\tnse: 0.7763558387651984\n",
      "Epoch 248 mean train loss:\t96.1320302421278\n",
      "Epoch 248 mean val mse:    \t367.06451416015625,\tnse: 0.7523928517120617\n",
      "Epoch 249 mean train loss:\t97.09301016760654\n",
      "Epoch 249 mean val mse:    \t384.3885498046875,\tnse: 0.7407067493156548\n",
      "Epoch 250 mean train loss:\t110.11496464671984\n",
      "Epoch 250 mean val mse:    \t368.9266662597656,\tnse: 0.7511367141537341\n",
      "Epoch 251 mean train loss:\t107.464232908572\n",
      "Epoch 251 mean val mse:    \t326.51934814453125,\tnse: 0.7797430201373562\n",
      "Epoch 252 mean train loss:\t90.41079334092271\n",
      "Epoch 252 mean val mse:    \t352.6015625,\tnse: 0.7621489809361324\n",
      "Epoch 253 mean train loss:\t82.1906143303126\n",
      "Epoch 253 mean val mse:    \t313.4119873046875,\tnse: 0.7885847192196025\n",
      "Epoch 254 mean train loss:\t85.45863858207328\n",
      "Epoch 254 mean val mse:    \t315.39605712890625,\tnse: 0.7872463346594106\n",
      "Epoch 255 mean train loss:\t100.98052971219757\n",
      "Epoch 255 mean val mse:    \t399.2740478515625,\tnse: 0.7306655494875078\n",
      "Epoch 256 mean train loss:\t77.55955694021424\n",
      "Epoch 256 mean val mse:    \t353.37750244140625,\tnse: 0.7616255608693722\n",
      "Epoch 257 mean train loss:\t84.96779837894961\n",
      "Epoch 257 mean val mse:    \t332.3585205078125,\tnse: 0.7758041237411557\n",
      "Epoch 258 mean train loss:\t84.67395946627758\n",
      "Epoch 258 mean val mse:    \t342.71795654296875,\tnse: 0.768816087687193\n",
      "Epoch 259 mean train loss:\t73.21346257944576\n",
      "Epoch 259 mean val mse:    \t345.538330078125,\tnse: 0.7669135619127185\n",
      "Epoch 260 mean train loss:\t101.38968810097116\n",
      "Epoch 260 mean val mse:    \t371.6959228515625,\tnse: 0.7492686878913315\n",
      "Epoch 261 mean train loss:\t107.46831083037162\n",
      "Epoch 261 mean val mse:    \t357.0365295410156,\tnse: 0.7591573407587958\n",
      "Epoch 262 mean train loss:\t85.95386699509751\n",
      "Epoch 262 mean val mse:    \t331.7652282714844,\tnse: 0.7762043613725705\n",
      "Epoch 263 mean train loss:\t86.09497349118926\n",
      "Epoch 263 mean val mse:    \t369.86572265625,\tnse: 0.7505032717748985\n",
      "Epoch 264 mean train loss:\t77.34802005851203\n",
      "Epoch 264 mean val mse:    \t361.1036071777344,\tnse: 0.7564138487094794\n",
      "Epoch 265 mean train loss:\t103.65130873716593\n",
      "Epoch 265 mean val mse:    \t361.5744934082031,\tnse: 0.7560961903238104\n",
      "Epoch 266 mean train loss:\t77.13235830088131\n",
      "Epoch 266 mean val mse:    \t339.02154541015625,\tnse: 0.7713095212678287\n",
      "Epoch 267 mean train loss:\t73.93878845569215\n",
      "Epoch 267 mean val mse:    \t333.9420471191406,\tnse: 0.7747359567286541\n",
      "Epoch 268 mean train loss:\t64.95938075435618\n",
      "Epoch 268 mean val mse:    \t378.91644287109375,\tnse: 0.744398019349127\n",
      "Epoch 269 mean train loss:\t96.2774545846741\n",
      "Epoch 269 mean val mse:    \t318.12420654296875,\tnse: 0.7854060499520396\n",
      "Epoch 270 mean train loss:\t76.82406119049573\n",
      "Epoch 270 mean val mse:    \t343.408203125,\tnse: 0.7683504636112886\n",
      "Epoch 271 mean train loss:\t96.73365436616491\n",
      "Epoch 271 mean val mse:    \t351.8372802734375,\tnse: 0.7626645365667089\n",
      "Epoch 272 mean train loss:\t100.88989917567518\n",
      "Epoch 272 mean val mse:    \t364.7358703613281,\tnse: 0.7539636562635585\n",
      "Epoch 273 mean train loss:\t103.19044948015056\n",
      "Epoch 273 mean val mse:    \t349.6240539550781,\tnse: 0.7641574760830023\n",
      "Epoch 274 mean train loss:\t81.13095587329136\n",
      "Epoch 274 mean val mse:    \t337.0591125488281,\tnse: 0.7726333140832174\n",
      "Epoch 275 mean train loss:\t64.59340941319701\n",
      "Epoch 275 mean val mse:    \t334.5504455566406,\tnse: 0.7743255542166451\n",
      "Epoch 276 mean train loss:\t59.346102818765274\n",
      "Epoch 276 mean val mse:    \t369.5704650878906,\tnse: 0.7507024334940344\n",
      "Epoch 277 mean train loss:\t80.99650888495107\n",
      "Epoch 277 mean val mse:    \t350.95098876953125,\tnse: 0.7632623718873073\n",
      "Epoch 278 mean train loss:\t75.64432746595372\n",
      "Epoch 278 mean val mse:    \t322.2447814941406,\tnse: 0.7826264682172778\n",
      "Epoch 279 mean train loss:\t63.1024467332767\n",
      "Epoch 279 mean val mse:    \t333.9322814941406,\tnse: 0.7747425291061538\n",
      "Epoch 280 mean train loss:\t56.345847093342435\n",
      "Epoch 280 mean val mse:    \t326.7091369628906,\tnse: 0.7796149835078127\n",
      "Epoch 281 mean train loss:\t58.35456433843394\n",
      "Epoch 281 mean val mse:    \t348.1266784667969,\tnse: 0.765167575572848\n",
      "Epoch 282 mean train loss:\t76.46812640122377\n",
      "Epoch 282 mean val mse:    \t417.9740295410156,\tnse: 0.7180513192478855\n",
      "Epoch 283 mean train loss:\t90.28765808689138\n",
      "Epoch 283 mean val mse:    \t332.6954040527344,\tnse: 0.7755769031931418\n",
      "Epoch 284 mean train loss:\t88.60493270165281\n",
      "Epoch 284 mean val mse:    \t348.906982421875,\tnse: 0.7646412027778274\n",
      "Epoch 285 mean train loss:\t354.9108498786968\n",
      "Epoch 285 mean val mse:    \t513.4779052734375,\tnse: 0.6536282180924735\n",
      "Epoch 286 mean train loss:\t267.84459588306197\n",
      "Epoch 286 mean val mse:    \t452.77288818359375,\tnse: 0.6945773969879854\n",
      "Epoch 287 mean train loss:\t168.25204799214347\n",
      "Epoch 287 mean val mse:    \t417.9156799316406,\tnse: 0.7180906948872646\n",
      "Epoch 288 mean train loss:\t129.03503167824667\n",
      "Epoch 288 mean val mse:    \t401.1952209472656,\tnse: 0.7293696355628332\n",
      "Epoch 289 mean train loss:\t106.14386503292563\n",
      "Epoch 289 mean val mse:    \t403.6777648925781,\tnse: 0.7276950074738705\n",
      "Epoch 290 mean train loss:\t94.03359223454376\n",
      "Epoch 290 mean val mse:    \t406.9079284667969,\tnse: 0.7255160721962517\n",
      "Epoch 291 mean train loss:\t80.37418322745567\n",
      "Epoch 291 mean val mse:    \t386.0872802734375,\tnse: 0.7395608551573403\n",
      "Epoch 292 mean train loss:\t104.54439111094658\n",
      "Epoch 292 mean val mse:    \t454.85137939453125,\tnse: 0.6931753184397974\n",
      "Epoch 293 mean train loss:\t129.0320743081348\n",
      "Epoch 293 mean val mse:    \t442.7417297363281,\tnse: 0.7013439999959933\n",
      "Epoch 294 mean train loss:\t100.20285357282461\n",
      "Epoch 294 mean val mse:    \t451.0645751953125,\tnse: 0.695729731108101\n",
      "Epoch 295 mean train loss:\t85.56938844691209\n",
      "Epoch 295 mean val mse:    \t366.8140563964844,\tnse: 0.752561803982341\n",
      "Epoch 296 mean train loss:\t85.77982470507179\n",
      "Epoch 296 mean val mse:    \t355.6938781738281,\tnse: 0.7600630394836024\n",
      "Epoch 297 mean train loss:\t78.50293136033855\n",
      "Epoch 297 mean val mse:    \t400.8170471191406,\tnse: 0.7296247346170932\n",
      "Epoch 298 mean train loss:\t74.61882573529019\n",
      "Epoch 298 mean val mse:    \t385.6192321777344,\tnse: 0.7398765674900485\n",
      "Epoch 299 mean train loss:\t66.58973892399523\n",
      "Epoch 299 mean val mse:    \t354.251708984375,\tnse: 0.7610358686739604\n",
      "Using best model from epoch 245 which had loss 306.70438\n",
      "Saved model as ../pickle/models/ConvLSTM+LinearLayer_VIC_allStations_20190717-093145.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mgauch/runoff-nn/gwf/lib/python3.6/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type ConvLSTMRegression. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ConvLSTMRegression(\n",
       "  (conv_lstm): ConvLSTM(\n",
       "    (cell_list): ModuleList(\n",
       "      (0): ConvLSTMCell(\n",
       "        (conv): Conv2d(22, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (1): ConvLSTMCell(\n",
       "        (conv): Conv2d(40, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "      (2): ConvLSTMCell(\n",
       "        (conv): Conv2d(40, 80, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=26520, out_features=45, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for epoch in range(300):\n",
    "    epoch_losses = []\n",
    "\n",
    "    shuffle_indices = np.arange(x_train.shape[1])\n",
    "    np.random.shuffle(shuffle_indices)\n",
    "    x_train = x_train[:,shuffle_indices,:,:,:]\n",
    "    y_train = y_train[shuffle_indices,:]\n",
    "\n",
    "    model.train()\n",
    "    #model.init_hidden()\n",
    "    for i in range(x_train.shape[1] // batch_size):\n",
    "        #model.hidden = model.init_hidden()\n",
    "        y_pred = model(x_train[:,i*batch_size : (i+1)*batch_size,:,:,:])\n",
    "        loss = loss_fn(y_pred, y_train[i*batch_size : (i+1)*batch_size,:].reshape((batch_size,y_train.shape[1]))).to(device)\n",
    "        epoch_losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    epoch_loss = np.array(epoch_losses).mean()\n",
    "    print('Epoch', epoch, 'mean train loss:\\t{}'.format(epoch_loss))\n",
    "    writer.add_scalar('loss', epoch_loss, epoch)\n",
    "\n",
    "    # eval on validation split\n",
    "    model.eval()\n",
    "    val_pred = pd.Series()\n",
    "    for i in range(x_val.shape[1] // batch_size):\n",
    "        #model.hidden = model.init_hidden()\n",
    "        batch_pred = model(x_val[:,i*batch_size : (i+1)*batch_size,:,:,:]).detach().cpu().numpy().reshape((batch_size, y_val.shape[1]))\n",
    "        val_pred = val_pred.append(pd.DataFrame(batch_pred))\n",
    "    model.train()\n",
    "    val_nse, val_mse = evaluate.evaluate_daily('All Stations', pd.DataFrame(val_pred.values.flatten()), pd.Series(y_val.cpu().numpy().flatten())[:val_pred.shape[0]*val_pred.shape[1]])\n",
    "    print('Epoch {} mean val mse:    \\t{},\\tnse: {}'.format(epoch, val_mse, val_nse))\n",
    "    writer.add_scalar('loss_eval', val_mse, epoch)\n",
    "\n",
    "    if val_mse < best_loss_model[1] - min_improvement:\n",
    "        best_loss_model = (epoch, val_mse, model.state_dict())  # new best model\n",
    "    elif epoch > best_loss_model[0] + patience:\n",
    "        print('Patience exhausted in epoch {}. Best val-loss was {}'.format(epoch, best_loss_model[1]))\n",
    "        break\n",
    "\n",
    "print('Using best model from epoch', str(best_loss_model[0]), 'which had loss', str(best_loss_model[1]))\n",
    "model.load_state_dict(best_loss_model[2])\n",
    "load_data.pickle_model('ConvLSTM+LinearLayer_VIC', model, 'allStations', time_stamp)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test shape: (120, 730, 2, 34, 39)\n",
      "Predicting\n"
     ]
    }
   ],
   "source": [
    "# scale test data\n",
    "x_test = x[:,num_train_days:,:,:,:].copy()\n",
    "for i in range(x.shape[2]):\n",
    "    x_test[:,:,i,:,:] = np.nan_to_num(scalers[i].transform(x_test[:,:,i,:,:].reshape((-1, 1))).reshape(x_test[:,:,i,:,:].shape))\n",
    "print('x_test shape: {}'.format(x_test.shape))\n",
    "# if batch size doesn't align with number of samples, add dummies to the last batch\n",
    "num_dummies = x_test.shape[1] % batch_size\n",
    "if num_dummies != 0:\n",
    "    x_test = np.concatenate([x_test, np.zeros((x_test.shape[0], batch_size - (x_test.shape[1] % batch_size), \n",
    "                                               x_test.shape[2], x_test.shape[3], x_test.shape[4]))], axis=1)\n",
    "    print('Appended dummy entries to x_test. New shape: {}'.format(x_test.shape))\n",
    "\n",
    "# Predict\n",
    "x_test = torch.from_numpy(x_test).float().to(device)\n",
    "predict = data_runoff.loc[test_start:test_end].copy()\n",
    "for station in predict.columns:\n",
    "    predict[station] = np.nan\n",
    "print('Predicting')\n",
    "for i in range(x_test.shape[1] // batch_size):\n",
    "    #model.hidden = model.init_hidden()\n",
    "    pred = model(x_test[:,i*batch_size : (i+1)*batch_size,:,:,:]).detach().cpu().numpy().reshape((batch_size, predict.shape[1]))\n",
    "    if (i+1) * batch_size <= predict.shape[0]:\n",
    "        predict.iloc[i*batch_size:(i+1)*batch_size,:] = pred\n",
    "    else:\n",
    "        predict.iloc[i*batch_size:,:] = pred[:-num_dummies,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mgauch/runoff-nn/gwf/lib/python3.6/site-packages/pandas/plotting/_converter.py:129: FutureWarning: Using an implicitly registered datetime converter for a matplotlib plotting method. The converter was registered by pandas on import. Future versions of pandas will require you to explicitly register matplotlib converters.\n",
      "\n",
      "To register the converters:\n",
      "\t>>> from pandas.plotting import register_matplotlib_converters\n",
      "\t>>> register_matplotlib_converters()\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02GA010 \tNSE: 0.27052143487496805 \tMSE: 347.79608388080277 (clipped to 0)\n",
      "02GA018 \tNSE: 0.29456965817628067 \tMSE: 177.17107397166683 (clipped to 0)\n",
      "02GA038 \tNSE: 0.20559615584708935 \tMSE: 130.70998269534851 (clipped to 0)\n",
      "02GA047 \tNSE: -0.017734177758347736 \tMSE: 79.78020729654098 (clipped to 0)\n",
      "02GB001 \tNSE: 0.10273569808882344 \tMSE: 6738.342927514126 (clipped to 0)\n",
      "02GB007 \tNSE: 0.25707669235344066 \tMSE: 23.23270847044957 (clipped to 0)\n",
      "02GC002 \tNSE: 0.3512607630609377 \tMSE: 83.3649606240527 (clipped to 0)\n",
      "02GC007 \tNSE: 0.09998970751507286 \tMSE: 27.109520049913932 (clipped to 0)\n",
      "02GC010 \tNSE: 0.31202343693621193 \tMSE: 40.90015149764304 (clipped to 0)\n",
      "02GC018 \tNSE: 0.3324082568041582 \tMSE: 45.21385362407279 (clipped to 0)\n",
      "02GC026 \tNSE: 0.380932973959316 \tMSE: 99.56625430111926 (clipped to 0)\n",
      "02GD004 \tNSE: 0.3387476443398857 \tMSE: 36.41058700421232 (clipped to 0)\n",
      "02GE007 \tNSE: 0.18891844445924477 \tMSE: 29.34092270343488 (clipped to 0)\n",
      "02GG002 \tNSE: 0.3823840467856615 \tMSE: 148.9583612430081 (clipped to 0)\n",
      "02GG003 \tNSE: 0.47132292513470186 \tMSE: 252.389204165623 (clipped to 0)\n",
      "02GG006 \tNSE: 0.347434777295221 \tMSE: 34.982226672305515 (clipped to 0)\n",
      "02GG009 \tNSE: 0.3637062314827958 \tMSE: 98.97767610388755 (clipped to 0)\n",
      "02GG013 \tNSE: 0.32210737407075785 \tMSE: 25.88355392728938 (clipped to 0)\n",
      "04159492 \tNSE: 0.2221914914574662 \tMSE: 361.1316702411038 (clipped to 0)\n",
      "04159900 \tNSE: 0.19357782968980164 \tMSE: 35.03659551980254 (clipped to 0)\n",
      "04160600 \tNSE: 0.21896246877038406 \tMSE: 22.596130777534803 (clipped to 0)\n",
      "04161820 \tNSE: -0.12090696063356998 \tMSE: 35.39605505190641 (clipped to 0)\n",
      "04164000 \tNSE: 0.2738206013864767 \tMSE: 127.24649719999539 (clipped to 0)\n",
      "04165500 \tNSE: 0.349976845411914 \tMSE: 290.98866204052393 (clipped to 0)\n",
      "04166100 \tNSE: 0.0807374860295762 \tMSE: 6.741234859263497 (clipped to 0)\n",
      "04166500 \tNSE: 0.18777208009999924 \tMSE: 28.074083284495213 (clipped to 0)\n",
      "04174500 \tNSE: -0.1717529060181331 \tMSE: 110.81763655386007 (clipped to 0)\n",
      "04176500 \tNSE: -0.4878021792719025 \tMSE: 1016.2793077244286 (clipped to 0)\n",
      "04177000 \tNSE: -0.018082825099461353 \tMSE: 27.437158869980614 (clipped to 0)\n",
      "04193500 \tNSE: 0.42505524934853145 \tMSE: 40706.51913373372 (clipped to 0)\n",
      "04195820 \tNSE: 0.2248019772218205 \tMSE: 909.9230478476967 (clipped to 0)\n",
      "04196800 \tNSE: 0.3307479829033483 \tMSE: 200.27265352316584 (clipped to 0)\n",
      "04197100 \tNSE: 0.43213348051622846 \tMSE: 58.24289438386411 (clipped to 0)\n",
      "04198000 \tNSE: 0.33278069943134925 \tMSE: 4929.122217068239 (clipped to 0)\n",
      "04199000 \tNSE: 0.2835230693268389 \tMSE: 732.6716735268018 (clipped to 0)\n",
      "04199500 \tNSE: 0.2744079096099006 \tMSE: 293.60947668547385 (clipped to 0)\n",
      "04200500 \tNSE: 0.3112717062163657 \tMSE: 689.2138168631585 (clipped to 0)\n",
      "04207200 \tNSE: 0.08949029433692113 \tMSE: 21.311655397428837 (clipped to 0)\n",
      "04208504 \tNSE: 0.1342200913088033 \tMSE: 845.4588907759377 (clipped to 0)\n",
      "04209000 \tNSE: 0.12343221987647623 \tMSE: 274.06418988649415 (clipped to 0)\n",
      "04212100 \tNSE: 0.24210257052699047 \tMSE: 1112.8397714195278 (clipped to 0)\n",
      "04213000 \tNSE: 0.19904168297424196 \tMSE: 143.7473775313286 (clipped to 0)\n",
      "04213500 \tNSE: 0.25234109851847986 \tMSE: 765.9143718080187 (clipped to 0)\n",
      "04215000 \tNSE: 0.16129400083182743 \tMSE: 80.56007034895339 (clipped to 0)\n",
      "04215500 \tNSE: 0.25078561322356996 \tMSE: 135.2968137655738 (clipped to 0)\n",
      "Median NSE (clipped to 0) 0.25234109851847986 / Min -0.4878021792719025 / Max 0.47132292513470186\n",
      "Median MSE (clipped to 0) 127.24649719999539 / Min 6.741234859263497 / Max 40706.51913373372\n"
     ]
    }
   ],
   "source": [
    "actuals = data_runoff.loc[test_start:test_end].copy()\n",
    "\n",
    "nse_list = []\n",
    "mse_list = []\n",
    "for station in predict.columns:\n",
    "    nse, mse = evaluate.evaluate_daily(station, predict[station], actuals[station], writer=writer)\n",
    "    nse_list.append(nse)\n",
    "    mse_list.append(mse)\n",
    "    \n",
    "    print(station, '\\tNSE:', nse, '\\tMSE:', mse, '(clipped to 0)')\n",
    "\n",
    "print('Median NSE (clipped to 0)', np.median(nse_list), '/ Min', np.min(nse_list), '/ Max', np.max(nse_list))\n",
    "print('Median MSE (clipped to 0)', np.median(mse_list), '/ Min', np.min(mse_list), '/ Max', np.max(mse_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ConvLSTM+LinearLayer_VIC_20190717-093145.pkl'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = predict.unstack().reset_index().rename({0: 'runoff'}, axis=1)\n",
    "save_df = pd.merge(prediction, actuals.unstack().reset_index().rename({0: 'actual'}, axis=1), on=['date', 'station'])\n",
    "\n",
    "load_data.pickle_results('ConvLSTM+LinearLayer_VIC', save_df, time_stamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20190717-192551'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.now().strftime('%Y%m%d-%H%M%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
