{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConvLSTM trained on gridded forcings for all stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn import preprocessing\n",
    "import netCDF4 as nc\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from src import load_data, evaluate, conv_lstm\n",
    "import torch.autograd as autograd\n",
    "import pickle\n",
    "\n",
    "time_stamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "time_stamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = False\n",
    "if torch.cuda.is_available():\n",
    "    print('CUDA Available')\n",
    "    USE_CUDA = True\n",
    "device = torch.device('cuda' if USE_CUDA else 'cpu')\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdrs_data, rdrs_var_names, rdrs_time_index = load_data.load_rdrs_forcings(as_grid=True)\n",
    "data_runoff = load_data.load_discharge_gr4j_vic()\n",
    "data_runoff = data_runoff.pivot(index='date', columns='station', values='runoff')\n",
    "data_runoff = data_runoff.loc[:,data_runoff.columns != '04214500']  # This station has >1K nan target values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdrs_data = rdrs_data[:,[4,5],:,:]  # only keep precipitation and temperature\n",
    "rdrs_var_names = rdrs_var_names[4:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLSTMRegression(nn.Module):\n",
    "    def __init__(self, input_size, output_size, batch_size, input_dim, hidden_dim, kernel_size, num_layers):\n",
    "        super(ConvLSTMRegression, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.conv_lstm = conv_lstm.ConvLSTM(input_size, input_dim, hidden_dim, kernel_size, num_layers)\n",
    "        self.linear = nn.Linear(hidden_dim * input_size[0] * input_size[1], output_size)\n",
    "\n",
    "    def forward(self, input):\n",
    "        lstm_out, hidden = self.conv_lstm(input)\n",
    "        linear_in = lstm_out[-1][:,-1,:,:,:].reshape((batch_size, -1))\n",
    "        \n",
    "        return self.linear(linear_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vars, n_rows, n_cols = rdrs_data.shape[1], rdrs_data.shape[2], rdrs_data.shape[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {}\n",
    "actuals = {}\n",
    "seq_len = 7 * 24\n",
    "validation_fraction = 0.1\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "train_start = datetime.strptime('2010-01-01', '%Y-%m-%d') + timedelta(hours=seq_len)  # first day for which to make a prediction in train set\n",
    "train_end = '2012-12-31'\n",
    "test_start = '2013-01-01'\n",
    "test_end = '2014-12-31'\n",
    "\n",
    "# For training, only include dates where no station has an nan target value.\n",
    "data_runoff = data_runoff.loc[train_start:test_end]\n",
    "non_nan_train_dates = pd.Series(data_runoff[(pd.isna(data_runoff).sum(axis=1)==0) | (data_runoff.index >= test_start)].index)\n",
    "data_runoff = data_runoff.loc[non_nan_train_dates,:]\n",
    "\n",
    "num_train_days = non_nan_train_dates[(non_nan_train_dates >= train_start) & (non_nan_train_dates <= train_end)].shape[0]\n",
    "num_total_days = num_train_days + len(pd.date_range(test_start, test_end, freq='D'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input: (seq_len, samples, variables, height, width)\n",
    "x = np.zeros((seq_len, num_total_days, n_vars, n_rows, n_cols))\n",
    "i = 0\n",
    "for day in range(x.shape[1]):\n",
    "    # For each day that is to be predicted, cut out a sequence that ends with that day's 23:00 and is seq_len long\n",
    "    day_date = train_start + timedelta(days=day)\n",
    "    if len(non_nan_train_dates[non_nan_train_dates == day_date]) == 0:\n",
    "        continue\n",
    "    end_of_day_index = rdrs_time_index[rdrs_time_index == day_date].index.values[0] + 23\n",
    "    x[:,i,:,:,:] = rdrs_data[end_of_day_index - seq_len : end_of_day_index]\n",
    "    i += 1\n",
    "    \n",
    "# Scale training data\n",
    "scalers = []  # save scalers to apply them to test data later\n",
    "x_train = x[:,:num_train_days,:].copy()\n",
    "for i in range(x.shape[2]):\n",
    "    scalers.append(preprocessing.StandardScaler())\n",
    "    x_train[:,:,i,:,:] = np.nan_to_num(scalers[i].fit_transform(x_train[:,:,i,:,:].reshape((-1, 1))).reshape(x_train[:,:,i,:,:].shape))\n",
    "x_train = torch.from_numpy(x_train).float().to(device)    \n",
    "y_train = torch.from_numpy(data_runoff.loc[train_start:train_end].to_numpy()).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get validation split\n",
    "num_validation_samples = int(x_train.shape[1] * validation_fraction)\n",
    "validation_indices = np.random.choice(range(x_train.shape[1]), size=num_validation_samples)\n",
    "shuffle_indices = np.arange(x_train.shape[1])\n",
    "np.random.shuffle(shuffle_indices)\n",
    "x_train = x_train[:,shuffle_indices,:,:,:]\n",
    "y_train = y_train[shuffle_indices,:]\n",
    "\n",
    "x_val, x_train = x_train[:,-num_validation_samples:,:], x_train[:,:-num_validation_samples,:]\n",
    "y_val, y_train = y_train[-num_validation_samples:,:], y_train[:-num_validation_samples,:]\n",
    "print('Shapes: x_train {}, y_train {}, x_val {}, y_val {}'.format(x_train.shape, y_train.shape, x_val.shape, y_val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "learning_rate = 2e-3\n",
    "patience = 100\n",
    "min_improvement = 0.05\n",
    "best_loss_model = (-1, np.inf, None)\n",
    "\n",
    "# Prepare model\n",
    "H = 20\n",
    "batch_size = 1\n",
    "lstm_layers = 2\n",
    "kernel_size = (10,10)\n",
    "model = ConvLSTMRegression(x_train.shape[3:], data_runoff.shape[1], batch_size, x_train.shape[2], H, kernel_size, lstm_layers).to(device)\n",
    "loss_fn = torch.nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(300):\n",
    "    epoch_losses = []\n",
    "\n",
    "    shuffle_indices = np.arange(x_train.shape[1])\n",
    "    np.random.shuffle(shuffle_indices)\n",
    "    x_train = x_train[:,shuffle_indices,:,:,:]\n",
    "    y_train = y_train[shuffle_indices,:]\n",
    "\n",
    "    model.train()\n",
    "    #model.init_hidden()\n",
    "    for i in range(x_train.shape[1] // batch_size):\n",
    "        #model.hidden = model.init_hidden()\n",
    "        y_pred = model(x_train[:,i*batch_size : (i+1)*batch_size,:,:,:])\n",
    "        loss = loss_fn(y_pred, y_train[i*batch_size : (i+1)*batch_size,:].reshape((batch_size,y_train.shape[1]))).to(device)\n",
    "        epoch_losses.append(loss.item())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    epoch_loss = np.array(epoch_losses).mean()\n",
    "    print('Epoch', epoch, 'mean train loss:\\t{}'.format(epoch_loss))\n",
    "    writer.add_scalar('loss', epoch_loss, epoch)\n",
    "\n",
    "    # eval on validation split\n",
    "    model.eval()\n",
    "    val_pred = pd.Series()\n",
    "    for i in range(x_val.shape[1] // batch_size):\n",
    "        #model.hidden = model.init_hidden()\n",
    "        batch_pred = model(x_val[:,i*batch_size : (i+1)*batch_size,:,:,:]).detach().cpu().numpy().reshape((batch_size, y_val.shape[1]))\n",
    "        val_pred = val_pred.append(pd.DataFrame(batch_pred))\n",
    "    model.train()\n",
    "    val_nse, val_mse = evaluate.evaluate_daily('All Stations', pd.DataFrame(val_pred.values.flatten()), pd.Series(y_val.cpu().numpy().flatten())[:val_pred.shape[0]*val_pred.shape[1]])\n",
    "    print('Epoch {} mean val mse:    \\t{},\\tnse: {}'.format(epoch, val_mse, val_nse))\n",
    "    writer.add_scalar('loss_eval', val_mse, epoch)\n",
    "\n",
    "    if val_mse < best_loss_model[1] - min_improvement:\n",
    "        best_loss_model = (epoch, val_mse, model.state_dict())  # new best model\n",
    "    elif epoch > best_loss_model[0] + patience:\n",
    "        print('Patience exhausted in epoch {}. Best val-loss was {}'.format(epoch, best_loss_model[1]))\n",
    "        break\n",
    "\n",
    "print('Using best model from epoch', str(best_loss_model[0]), 'which had loss', str(best_loss_model[1]))\n",
    "model.load_state_dict(best_loss_model[2])\n",
    "load_data.pickle_model('ConvLSTM+LinearLayer_VIC', model, 'allStations', time_stamp)\n",
    "model.eval()\n",
    "\n",
    "# scale test data\n",
    "x_test = x[:,num_train_days:,:,:,:].copy()\n",
    "for i in range(x.shape[2]):\n",
    "    x_test[:,:,i,:,:] = np.nan_to_num(scalers[i].transform(x_test[:,:,i,:,:].reshape((-1, 1))).reshape(x_test[:,:,i,:,:].shape))\n",
    "print('x_test shape: {}'.format(x_test.shape))\n",
    "# if batch size doesn't align with number of samples, add dummies to the last batch\n",
    "num_dummies = x_test.shape[1] % batch_size\n",
    "if num_dummies != 0:\n",
    "    x_test = np.concatenate([x_test, np.zeros((x_test.shape[0], batch_size - (x_test.shape[1] % batch_size), \n",
    "                                               x_test.shape[2], x_test.shape[3], x_test.shape[4]))], axis=1)\n",
    "    print('Appended dummy entries to x_test. New shape: {}'.format(x_test.shape))\n",
    "\n",
    "# Predict\n",
    "x_test = torch.from_numpy(x_test).float().to(device)\n",
    "predict = data_runoff.loc[test_start:test_end].copy()\n",
    "for station in predict.columns:\n",
    "    predict[station] = np.nan\n",
    "print('Predicting')\n",
    "for i in range(x_test.shape[1] // batch_size):\n",
    "    #model.hidden = model.init_hidden()\n",
    "    pred = model(x_test[:,i*batch_size : (i+1)*batch_size,:,:,:]).detach().cpu().numpy().reshape((batch_size, predict.shape[1]))\n",
    "    if (i+1) * batch_size <= predict.shape[0]:\n",
    "        predict.iloc[i*batch_size:(i+1)*batch_size,:] = pred\n",
    "    else:\n",
    "        predict.iloc[i*batch_size:,:] = pred[:-num_dummies,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actuals = data_runoff.loc[test_start:test_end].copy()\n",
    "\n",
    "nse_list = []\n",
    "mse_list = []\n",
    "for station in predict.columns:\n",
    "    nse, mse = evaluate.evaluate_daily(station, predict[station], actuals[station], writer=writer)\n",
    "    nse_list.append(nse)\n",
    "    mse_list.append(mse)\n",
    "    \n",
    "    print(station, '\\tNSE:', nse, '\\tMSE:', mse, '(clipped to 0)')\n",
    "\n",
    "print('Median NSE (clipped to 0)', np.median(nse_list), '/ Min', np.min(nse_list), '/ Max', np.max(nse_list))\n",
    "print('Median MSE (clipped to 0)', np.median(mse_list), '/ Min', np.min(mse_list), '/ Max', np.max(mse_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = predict.unstack().reset_index().rename({0: 'runoff'}, axis=1)\n",
    "save_df = pd.merge(a, actuals.unstack().reset_index().rename({0: 'actual'}, axis=1), on=['date', 'station'])\n",
    "\n",
    "load_data.pickle_results('ConvLSTM+LinearLayer_VIC', save_df, time_stamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.now().strftime('%Y%m%d-%H%M%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
