{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConvLSTM trained on gridded forcings and landcover data for all stations.\n",
    "Test generalization by training on a subset of stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20190811-143311'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt \n",
    "from datetime import datetime, timedelta\n",
    "from sklearn import preprocessing\n",
    "import netCDF4 as nc\n",
    "import torch\n",
    "from torch import nn, utils\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from src import load_data, evaluate, conv_lstm, datasets\n",
    "import torch.autograd as autograd\n",
    "import pickle\n",
    "\n",
    "time_stamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "time_stamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "fhandler = logging.FileHandler(filename='../log.out', mode='a')\n",
    "chandler = logging.StreamHandler(sys.stdout)\n",
    "formatter = logging.Formatter('%(asctime)s - {} - %(message)s'.format(time_stamp))\n",
    "fhandler.setFormatter(formatter)\n",
    "chandler.setFormatter(formatter)\n",
    "logger.addHandler(fhandler)\n",
    "logger.addHandler(chandler)\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available\n",
      "2019-08-11 14:33:11,411 - 20190811-143311 - cuda devices: ['Tesla V100-SXM2-16GB']\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = False\n",
    "if torch.cuda.is_available():\n",
    "    print('CUDA Available')\n",
    "    USE_CUDA = True\n",
    "device = torch.device('cuda:0' if USE_CUDA else 'cpu')\n",
    "num_devices = torch.cuda.device_count() if USE_CUDA else 0\n",
    "logger.warning('cuda devices: {}'.format(list(torch.cuda.get_device_name(i) for i in range(num_devices))))\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "landcover_nc = nc.Dataset('../data/NA_NALCMS_LC_30m_LAEA_mmu12_urb05_n40-45w75-90_erie.nc', 'r')\n",
    "landcover_nc.set_auto_mask(False)\n",
    "landcover_lats = landcover_nc['lat'][:][::-1]\n",
    "landcover_lons = landcover_nc['lon'][:]\n",
    "landcover_nc.close()\n",
    "\n",
    "out_lats = landcover_lats[::len(landcover_lats)//34 + 1]\n",
    "out_lons = landcover_lons[::len(landcover_lons)//39 + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 5*24\n",
    "seq_steps = 2\n",
    "stateful_lstm = False\n",
    "validation_fraction, val_start, val_end = None, None, None\n",
    "\n",
    "if stateful_lstm:\n",
    "    val_start = datetime.strptime('2010-01-01', '%Y-%m-%d') + timedelta(hours=seq_len * seq_steps)  # first day for which to make a prediction in train set\n",
    "    val_end = '2010-09-30'\n",
    "    train_start = '2010-10-01'\n",
    "    train_end = '2012-12-31'\n",
    "else:\n",
    "    validation_fraction = 0.1\n",
    "    train_start = datetime.strptime('2010-01-01', '%Y-%m-%d') + timedelta(hours=seq_len * seq_steps)  # first day for which to make a prediction in train set\n",
    "    train_end = '2012-12-31'\n",
    "test_start = '2013-01-01'\n",
    "test_end = '2014-12-31'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exclude_downstream_stations = ['02GB001', '02GB007', '02GC026', '02GG009', '02GG003', '04165500', '04164000', '04166500', '04198000', '04208504']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../src/load_data.py:72: RuntimeWarning: invalid value encountered in greater\n",
      "  rdrs_data[:,i,:,:] = rdrs_nc[forcing_variables[i]][:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating upsampling map to quickly upsample during training/testing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../src/load_data.py:72: RuntimeWarning: invalid value encountered in greater\n",
      "  rdrs_data[:,i,:,:] = rdrs_nc[forcing_variables[i]][:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating upsampling map to quickly upsample during training/testing\n"
     ]
    }
   ],
   "source": [
    "rdrs_vars = list(range(8))\n",
    "train_dataset = datasets.RdrsGridDataset(rdrs_vars, seq_len, seq_steps, train_start, train_end, exclude_stations=exclude_downstream_stations, out_lats=out_lats, out_lons=out_lons, upsample='input')\n",
    "if stateful_lstm:\n",
    "    val_dataset = datasets.RdrsGridDataset(rdrs_vars, seq_len, seq_steps, val_start, val_end, conv_scalers=train_dataset.conv_scalers, exclude_stations=exclude_downstream_stations, out_lats=out_lats, out_lons=out_lons, upsample='input')\n",
    "test_dataset = datasets.RdrsGridDataset(rdrs_vars, seq_len, seq_steps, test_start, test_end, conv_scalers=train_dataset.conv_scalers, out_lats=out_lats, out_lons=out_lons, upsample='input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../src/load_data.py:72: RuntimeWarning: invalid value encountered in greater\n",
      "  rdrs_data[:,i,:,:] = rdrs_nc[forcing_variables[i]][:]\n",
      "../src/load_data.py:271: UserWarning: WARNING: valid_range not used since it\n",
      "cannot be safely cast to variable data type\n",
      "  landcover_fullres = np.array(landcover_nc['Band1'][:])[::-1,:]\n"
     ]
    }
   ],
   "source": [
    "landcover_types = None\n",
    "landcover, landcover_legend = load_data.load_landcover_reduced(values_to_use=landcover_types)\n",
    "landcover = torch.from_numpy(landcover).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = train_dataset.data_runoff['station'].unique()\n",
    "np.random.seed(2)\n",
    "test_stations = np.random.choice(stations, size=int(0.2*(len(stations)-len(exclude_downstream_stations))), replace=False)\n",
    "train_stations = list(s for s in stations if s not in test_stations)\n",
    "\n",
    "train_station_indices = list(train_dataset.station_to_index[s] for s in train_stations)\n",
    "test_station_indices = list(test_dataset.station_to_index[s] for s in test_stations)\n",
    "\n",
    "train_mask = torch.zeros((train_dataset.out_lats.shape[0], train_dataset.out_lats.shape[1]), dtype=torch.bool)\n",
    "for row in range(train_mask.shape[0]):\n",
    "    for col in range(train_mask.shape[1]):\n",
    "        train_mask[row, col] = True if (row, col) in train_station_indices else False\n",
    "train_mask = train_mask.reshape(-1).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'time_stamp': '20190811-143311', 'H_convlstm': [8, 8], 'H_conv': [4], 'batch_size': 32, 'num_convlstm_layers': 2, 'num_conv_layers': 2, 'convlstm_kernel_size': [(5, 5), (5, 5)], 'conv_kernel_size': [(3, 3), (3, 3)], 'loss': NSELoss(), 'optimizer': Adam (\\nParameter Group 0\\n    amsgrad: False\\n    betas: (0.9, 0.999)\\n    eps: 1e-08\\n    lr: 0.002\\n    weight_decay: 1e-05\\n), 'lr': 0.002, 'patience': 100, 'min_improvement': 0.01, 'stateful_lstm': False, 'dropout': 0.2, 'landcover_shape': torch.Size([19, 34, 39]), 'conv_activation': <class 'torch.nn.modules.activation.Sigmoid'>, 'num_epochs': 400, 'seq_len': 120, 'seq_steps': 2, 'train_start': datetime.datetime(2010, 1, 11, 0, 0), 'train_end': '2012-12-31', 'weight_decay': 1e-05, 'validation_fraction': 0.1, 'landcover_types': None, 'test_start': '2013-01-01', 'test_end': '2014-12-31', 'n_conv_vars': 8, 'model': 'ConvLSTMGridWithGeophysicalInput((conv_lstm):ConvLSTM((cell_list):ModuleList((0):ConvLSTMCell((conv):Conv2d(16,32,kernel_size=(5,5),stride=(1,1),padding=(2,2)))(1):Identity()(2):ConvLSTMCell((conv):Conv2d(16,32,kernel_size=(5,5),stride=(1,1),padding=(2,2)))(3):Identity()))(dropout):Dropout2d(p=0.2,inplace=False)(conv_out):Sequential((0):BatchNorm2d(27,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)(1):Conv2d(27,4,kernel_size=(3,3),stride=(1,1),padding=(1,1))(2):Dropout(p=0.2,inplace=False)(3):Sigmoid()(4):BatchNorm2d(4,eps=1e-05,momentum=0.1,affine=True,track_running_stats=True)(5):Conv2d(4,1,kernel_size=(3,3),stride=(1,1),padding=(1,1))))', 'val_start': None, 'val_end': None, 'train_stations': ['02GA047', '04193500', '04209000', '04195820', '04207200', '04215500', '04160600', '02GC007', '04215000', '02GE007', '02GG006', '02GA018', '02GG013', '04197100', '02GG002', '02GC010', '04199000', '04213000', '02GC002', '02GA010', '04174500', '04212100', '04199500', '04159492', '04196800', '02GD004', '04166100', '04177000', '02GC018', '04176500', '04214500'], 'test_stations': array(['04213500', '02GA038', '04200500', '04159900', '04161820'],\\n      dtype=object), 'train len': 1086, 'conv_height': 34, 'conv_width': 39, 'test len': 730}\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train model\n",
    "num_epochs = 400\n",
    "learning_rate = 2e-3\n",
    "patience = 100\n",
    "min_improvement = 0.01\n",
    "best_loss_model = (-1, np.inf, None)\n",
    "\n",
    "# Prepare model\n",
    "batch_size = 32\n",
    "num_convlstm_layers = 2\n",
    "num_conv_layers = 2\n",
    "convlstm_hidden_dims = [8] * num_convlstm_layers\n",
    "conv_hidden_dims = [4] * (num_conv_layers - 1)\n",
    "convlstm_kernel_size = [(5,5)] * num_convlstm_layers\n",
    "conv_kernel_size = [(3,3)] * num_conv_layers\n",
    "conv_activation = nn.Sigmoid\n",
    "dropout = 0.2\n",
    "weight_decay = 1e-5\n",
    "\n",
    "model = conv_lstm.ConvLSTMGridWithGeophysicalInput((train_dataset.conv_height, train_dataset.conv_width), \n",
    "                                         train_dataset.n_conv_vars, landcover.shape[0], convlstm_hidden_dims, \n",
    "                                         conv_hidden_dims, convlstm_kernel_size, conv_kernel_size, \n",
    "                                         num_convlstm_layers, num_conv_layers, conv_activation, dropout=dropout).to(device)\n",
    "if num_devices > 1:\n",
    "    model = torch.nn.DataParallel(model, device_ids=list(range(num_devices)))\n",
    "loss_fn = evaluate.NSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "writer = SummaryWriter(comment='ConvLSTM_withLandcover_generalizationTest')\n",
    "param_description = {'time_stamp': time_stamp, 'H_convlstm': convlstm_hidden_dims, 'H_conv': conv_hidden_dims, 'batch_size': batch_size, 'num_convlstm_layers': num_convlstm_layers, 'num_conv_layers': num_conv_layers, 'convlstm_kernel_size': convlstm_kernel_size, 'conv_kernel_size': conv_kernel_size, 'loss': loss_fn, \n",
    "                     'optimizer': optimizer, 'lr': learning_rate, 'patience': patience, 'min_improvement': min_improvement, 'stateful_lstm': stateful_lstm, 'dropout': dropout, 'landcover_shape': landcover.shape, 'conv_activation': conv_activation,\n",
    "                     'num_epochs': num_epochs, 'seq_len': seq_len, 'seq_steps': seq_steps, 'train_start': train_start, 'train_end': train_end, 'weight_decay': weight_decay, 'validation_fraction': validation_fraction, 'landcover_types': landcover_types,\n",
    "                     'test_start': test_start, 'test_end': test_end, 'n_conv_vars': train_dataset.n_conv_vars, 'model': str(model).replace('\\n','').replace(' ', ''), 'val_start': val_start, 'val_end': val_end, 'train_stations': train_stations, 'test_stations': test_stations,\n",
    "                     'train len':len(train_dataset), 'conv_height': train_dataset.conv_height, 'conv_width': train_dataset.conv_width, 'test len': len(test_dataset)}\n",
    "writer.add_text('Parameter Description', str(param_description))\n",
    "str(param_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if stateful_lstm:\n",
    "    train_sampler = datasets.StatefulBatchSampler(train_dataset, batch_size)\n",
    "    val_sampler = datasets.StatefulBatchSampler(val_dataset, batch_size)\n",
    "    test_sampler = datasets.StatefulBatchSampler(test_dataset, batch_size)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_sampler=train_sampler, pin_memory=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_sampler=val_sampler, pin_memory=True)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_sampler=test_sampler, pin_memory=True)\n",
    "else:\n",
    "    val_indices = np.random.choice(len(train_dataset), size=int(validation_fraction * len(train_dataset)), replace=False)\n",
    "    train_indices = list(i for i in range(len(train_dataset)) if i not in val_indices)\n",
    "    train_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\n",
    "    val_sampler = torch.utils.data.SubsetRandomSampler(val_indices)\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size, sampler=train_sampler, pin_memory=True, drop_last=False)\n",
    "    val_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size, sampler=val_sampler, pin_memory=True, drop_last=False)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size, shuffle=False, pin_memory=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 mean train loss:\t1.2503694295883179\n",
      "Epoch 0 mean val loss:  \t1.2870571613311768\n",
      "Saved model as ../pickle/models/ConvLSTM_withLandcover_generalizationTest_allStations_20190811-143311.pkl\n",
      "Epoch 1 mean train loss:\t1.0813013315200806\n",
      "Epoch 1 mean val loss:  \t1.2297961711883545\n",
      "Saved model as ../pickle/models/ConvLSTM_withLandcover_generalizationTest_allStations_20190811-143311.pkl\n",
      "Epoch 2 mean train loss:\t1.0365972518920898\n",
      "Epoch 2 mean val loss:  \t1.113328218460083\n",
      "Saved model as ../pickle/models/ConvLSTM_withLandcover_generalizationTest_allStations_20190811-143311.pkl\n",
      "Epoch 3 mean train loss:\t1.0124458074569702\n",
      "Epoch 3 mean val loss:  \t0.9912194013595581\n",
      "Saved model as ../pickle/models/ConvLSTM_withLandcover_generalizationTest_allStations_20190811-143311.pkl\n",
      "Epoch 4 mean train loss:\t0.9696850180625916\n",
      "Epoch 4 mean val loss:  \t0.9567195177078247\n",
      "Saved model as ../pickle/models/ConvLSTM_withLandcover_generalizationTest_allStations_20190811-143311.pkl\n",
      "Epoch 5 mean train loss:\t1.0155134201049805\n",
      "Epoch 5 mean val loss:  \t1.0718352794647217\n",
      "Epoch 6 mean train loss:\t0.9692971110343933\n",
      "Epoch 6 mean val loss:  \t1.0317940711975098\n",
      "Epoch 7 mean train loss:\t0.9359606504440308\n",
      "Epoch 7 mean val loss:  \t0.930875301361084\n",
      "Saved model as ../pickle/models/ConvLSTM_withLandcover_generalizationTest_allStations_20190811-143311.pkl\n",
      "Epoch 8 mean train loss:\t0.9525832533836365\n",
      "Epoch 8 mean val loss:  \t1.3840699195861816\n",
      "Epoch 9 mean train loss:\t0.8716502785682678\n",
      "Epoch 9 mean val loss:  \t0.9472893476486206\n",
      "Epoch 10 mean train loss:\t0.8291645050048828\n",
      "Epoch 10 mean val loss:  \t0.836591362953186\n",
      "Saved model as ../pickle/models/ConvLSTM_withLandcover_generalizationTest_allStations_20190811-143311.pkl\n",
      "Epoch 11 mean train loss:\t0.830068826675415\n",
      "Epoch 11 mean val loss:  \t0.9234714508056641\n",
      "Epoch 12 mean train loss:\t0.9128122925758362\n",
      "Epoch 12 mean val loss:  \t0.8020036816596985\n",
      "Saved model as ../pickle/models/ConvLSTM_withLandcover_generalizationTest_allStations_20190811-143311.pkl\n",
      "Epoch 13 mean train loss:\t0.8434849381446838\n",
      "Epoch 13 mean val loss:  \t0.769874095916748\n",
      "Saved model as ../pickle/models/ConvLSTM_withLandcover_generalizationTest_allStations_20190811-143311.pkl\n",
      "Epoch 14 mean train loss:\t1.086012363433838\n",
      "Epoch 14 mean val loss:  \t0.9479353427886963\n",
      "Epoch 15 mean train loss:\t0.9538553953170776\n",
      "Epoch 15 mean val loss:  \t0.8699934482574463\n",
      "Epoch 16 mean train loss:\t1.0154849290847778\n",
      "Epoch 16 mean val loss:  \t0.9262596368789673\n",
      "Epoch 17 mean train loss:\t0.8548434376716614\n",
      "Epoch 17 mean val loss:  \t0.9219949841499329\n",
      "Epoch 18 mean train loss:\t0.8539373278617859\n",
      "Epoch 18 mean val loss:  \t0.8311305046081543\n",
      "Epoch 19 mean train loss:\t0.8231889009475708\n",
      "Epoch 19 mean val loss:  \t0.79572993516922\n",
      "Epoch 20 mean train loss:\t0.7813704013824463\n",
      "Epoch 20 mean val loss:  \t0.8677074313163757\n",
      "Epoch 21 mean train loss:\t0.7890090346336365\n",
      "Epoch 21 mean val loss:  \t0.7990584969520569\n",
      "Epoch 22 mean train loss:\t0.773768961429596\n",
      "Epoch 22 mean val loss:  \t0.8798871040344238\n",
      "Epoch 23 mean train loss:\t0.782916784286499\n",
      "Epoch 23 mean val loss:  \t0.9029533863067627\n",
      "Epoch 24 mean train loss:\t0.7988897562026978\n",
      "Epoch 24 mean val loss:  \t0.8417243957519531\n",
      "Epoch 25 mean train loss:\t0.8223291039466858\n",
      "Epoch 25 mean val loss:  \t0.7955695390701294\n",
      "Epoch 26 mean train loss:\t0.7945885062217712\n",
      "Epoch 26 mean val loss:  \t0.770031213760376\n",
      "Epoch 27 mean train loss:\t0.883713960647583\n",
      "Epoch 27 mean val loss:  \t0.7897832989692688\n",
      "Epoch 28 mean train loss:\t0.8064813017845154\n",
      "Epoch 28 mean val loss:  \t0.8266939520835876\n",
      "Epoch 29 mean train loss:\t0.7953317761421204\n",
      "Epoch 29 mean val loss:  \t1.011857509613037\n",
      "Epoch 30 mean train loss:\t0.7940196990966797\n",
      "Epoch 30 mean val loss:  \t0.7613637447357178\n",
      "Epoch 31 mean train loss:\t0.793429970741272\n",
      "Epoch 31 mean val loss:  \t0.7895225882530212\n",
      "Epoch 32 mean train loss:\t0.7607446908950806\n",
      "Epoch 32 mean val loss:  \t0.705121636390686\n",
      "Saved model as ../pickle/models/ConvLSTM_withLandcover_generalizationTest_allStations_20190811-143311.pkl\n",
      "Epoch 33 mean train loss:\t0.7474106550216675\n",
      "Epoch 33 mean val loss:  \t0.7255546450614929\n",
      "Epoch 34 mean train loss:\t0.7351576089859009\n",
      "Epoch 34 mean val loss:  \t0.7053303122520447\n",
      "Epoch 35 mean train loss:\t0.7303853034973145\n",
      "Epoch 35 mean val loss:  \t0.7973635792732239\n",
      "Epoch 36 mean train loss:\t0.723499596118927\n",
      "Epoch 36 mean val loss:  \t0.7676516771316528\n",
      "Epoch 37 mean train loss:\t0.724023163318634\n",
      "Epoch 37 mean val loss:  \t0.6589671969413757\n",
      "Saved model as ../pickle/models/ConvLSTM_withLandcover_generalizationTest_allStations_20190811-143311.pkl\n",
      "Epoch 38 mean train loss:\t0.746102511882782\n",
      "Epoch 38 mean val loss:  \t0.8406375050544739\n",
      "Epoch 39 mean train loss:\t0.7182684540748596\n",
      "Epoch 39 mean val loss:  \t0.9876261353492737\n",
      "Epoch 40 mean train loss:\t0.6660782098770142\n",
      "Epoch 40 mean val loss:  \t1.0633654594421387\n",
      "Epoch 41 mean train loss:\t0.7267950773239136\n",
      "Epoch 41 mean val loss:  \t0.8131188750267029\n",
      "Epoch 42 mean train loss:\t0.7093583345413208\n",
      "Epoch 42 mean val loss:  \t0.8116170167922974\n",
      "Epoch 43 mean train loss:\t0.6862080693244934\n",
      "Epoch 43 mean val loss:  \t0.7400312423706055\n",
      "Epoch 44 mean train loss:\t0.6614354252815247\n",
      "Epoch 44 mean val loss:  \t0.7283146381378174\n",
      "Epoch 45 mean train loss:\t0.6655498147010803\n",
      "Epoch 45 mean val loss:  \t0.7608879804611206\n",
      "Epoch 46 mean train loss:\t0.6764371991157532\n",
      "Epoch 46 mean val loss:  \t0.7783669233322144\n",
      "Epoch 47 mean train loss:\t0.7041768431663513\n",
      "Epoch 47 mean val loss:  \t1.0889544486999512\n",
      "Epoch 48 mean train loss:\t0.9862306714057922\n",
      "Epoch 48 mean val loss:  \t0.9966647624969482\n",
      "Epoch 49 mean train loss:\t0.8064208626747131\n",
      "Epoch 49 mean val loss:  \t0.8082946538925171\n",
      "Epoch 50 mean train loss:\t0.7818430066108704\n",
      "Epoch 50 mean val loss:  \t0.9194124937057495\n",
      "Epoch 51 mean train loss:\t0.7672405242919922\n",
      "Epoch 51 mean val loss:  \t1.0767673254013062\n",
      "Epoch 52 mean train loss:\t0.7769011855125427\n",
      "Epoch 52 mean val loss:  \t0.9825149774551392\n",
      "Epoch 53 mean train loss:\t0.7979430556297302\n",
      "Epoch 53 mean val loss:  \t1.2835667133331299\n",
      "Epoch 54 mean train loss:\t0.7657055258750916\n",
      "Epoch 54 mean val loss:  \t0.746396541595459\n",
      "Epoch 55 mean train loss:\t0.7903902530670166\n",
      "Epoch 55 mean val loss:  \t0.8415573835372925\n",
      "Epoch 56 mean train loss:\t0.7855287194252014\n",
      "Epoch 56 mean val loss:  \t0.8119741082191467\n",
      "Epoch 57 mean train loss:\t0.6896173357963562\n",
      "Epoch 57 mean val loss:  \t0.8278578519821167\n",
      "Epoch 58 mean train loss:\t0.7334557771682739\n",
      "Epoch 58 mean val loss:  \t0.9552780389785767\n",
      "Epoch 59 mean train loss:\t0.7895343899726868\n",
      "Epoch 59 mean val loss:  \t0.9500346779823303\n",
      "Epoch 60 mean train loss:\t0.7459063529968262\n",
      "Epoch 60 mean val loss:  \t0.7257319688796997\n",
      "Epoch 61 mean train loss:\t0.6932182312011719\n",
      "Epoch 61 mean val loss:  \t0.8798717260360718\n",
      "Epoch 62 mean train loss:\t0.736589789390564\n",
      "Epoch 62 mean val loss:  \t0.8407195806503296\n",
      "Epoch 63 mean train loss:\t0.7295787930488586\n",
      "Epoch 63 mean val loss:  \t0.8576922416687012\n",
      "Epoch 64 mean train loss:\t0.6769940853118896\n",
      "Epoch 64 mean val loss:  \t0.7951231598854065\n",
      "Epoch 65 mean train loss:\t0.6906738877296448\n",
      "Epoch 65 mean val loss:  \t0.7359564900398254\n",
      "Epoch 66 mean train loss:\t0.6743109822273254\n",
      "Epoch 66 mean val loss:  \t0.7227106094360352\n",
      "Epoch 67 mean train loss:\t0.6811001896858215\n",
      "Epoch 67 mean val loss:  \t1.0089715719223022\n",
      "Epoch 68 mean train loss:\t0.7469565272331238\n",
      "Epoch 68 mean val loss:  \t0.7926279902458191\n",
      "Epoch 69 mean train loss:\t0.6646029353141785\n",
      "Epoch 69 mean val loss:  \t0.8623439073562622\n",
      "Epoch 70 mean train loss:\t0.6681345701217651\n",
      "Epoch 70 mean val loss:  \t0.8075118660926819\n",
      "Epoch 71 mean train loss:\t0.6414998769760132\n",
      "Epoch 71 mean val loss:  \t0.6882503032684326\n",
      "Epoch 72 mean train loss:\t0.636298656463623\n",
      "Epoch 72 mean val loss:  \t0.7370247840881348\n",
      "Epoch 73 mean train loss:\t0.6614628434181213\n",
      "Epoch 73 mean val loss:  \t0.7006235122680664\n",
      "Epoch 74 mean train loss:\t0.683977484703064\n",
      "Epoch 74 mean val loss:  \t0.8053182363510132\n",
      "Epoch 75 mean train loss:\t0.6342955231666565\n",
      "Epoch 75 mean val loss:  \t0.701245129108429\n",
      "Epoch 76 mean train loss:\t0.642171323299408\n",
      "Epoch 76 mean val loss:  \t0.7439881563186646\n",
      "Epoch 77 mean train loss:\t0.6675658822059631\n",
      "Epoch 77 mean val loss:  \t0.8074116110801697\n",
      "Epoch 78 mean train loss:\t0.6670085191726685\n",
      "Epoch 78 mean val loss:  \t0.8282960653305054\n",
      "Epoch 79 mean train loss:\t0.6099320650100708\n",
      "Epoch 79 mean val loss:  \t0.726337194442749\n",
      "Epoch 80 mean train loss:\t0.6247544884681702\n",
      "Epoch 80 mean val loss:  \t0.6928120851516724\n",
      "Epoch 81 mean train loss:\t0.6248584389686584\n",
      "Epoch 81 mean val loss:  \t0.8225947022438049\n",
      "Epoch 82 mean train loss:\t0.6324095726013184\n",
      "Epoch 82 mean val loss:  \t0.7136451005935669\n",
      "Epoch 83 mean train loss:\t0.6505932807922363\n",
      "Epoch 83 mean val loss:  \t0.808247447013855\n",
      "Epoch 84 mean train loss:\t0.747295081615448\n",
      "Epoch 84 mean val loss:  \t0.8127180337905884\n",
      "Epoch 85 mean train loss:\t0.9165938496589661\n",
      "Epoch 85 mean val loss:  \t1.122314214706421\n",
      "Epoch 86 mean train loss:\t0.9174622297286987\n",
      "Epoch 86 mean val loss:  \t1.0692334175109863\n",
      "Epoch 87 mean train loss:\t0.8452566862106323\n",
      "Epoch 87 mean val loss:  \t0.8775783777236938\n",
      "Epoch 88 mean train loss:\t0.7375586032867432\n",
      "Epoch 88 mean val loss:  \t0.7824092507362366\n",
      "Epoch 89 mean train loss:\t0.7325965762138367\n",
      "Epoch 89 mean val loss:  \t0.8496169447898865\n",
      "Epoch 90 mean train loss:\t0.7004016041755676\n",
      "Epoch 90 mean val loss:  \t0.7855859398841858\n",
      "Epoch 91 mean train loss:\t0.759660542011261\n",
      "Epoch 91 mean val loss:  \t0.9529120326042175\n",
      "Epoch 92 mean train loss:\t0.7371808290481567\n",
      "Epoch 92 mean val loss:  \t0.8761328458786011\n",
      "Epoch 93 mean train loss:\t0.6898078918457031\n",
      "Epoch 93 mean val loss:  \t0.7928408980369568\n",
      "Epoch 94 mean train loss:\t0.6882733702659607\n",
      "Epoch 94 mean val loss:  \t0.7489309310913086\n",
      "Epoch 95 mean train loss:\t0.6581540107727051\n",
      "Epoch 95 mean val loss:  \t0.946049690246582\n",
      "Epoch 96 mean train loss:\t0.6735075116157532\n",
      "Epoch 96 mean val loss:  \t0.7559612989425659\n",
      "Epoch 97 mean train loss:\t0.6389903426170349\n",
      "Epoch 97 mean val loss:  \t1.2679400444030762\n",
      "Epoch 98 mean train loss:\t0.6753173470497131\n",
      "Epoch 98 mean val loss:  \t0.8286410570144653\n",
      "Epoch 99 mean train loss:\t0.6223709583282471\n",
      "Epoch 99 mean val loss:  \t0.7740724086761475\n",
      "Epoch 100 mean train loss:\t0.7225012183189392\n",
      "Epoch 100 mean val loss:  \t0.8456772565841675\n",
      "Epoch 101 mean train loss:\t0.646129846572876\n",
      "Epoch 101 mean val loss:  \t0.7270346879959106\n",
      "Epoch 102 mean train loss:\t0.6368324756622314\n",
      "Epoch 102 mean val loss:  \t0.9298222064971924\n",
      "Epoch 103 mean train loss:\t0.6689601540565491\n",
      "Epoch 103 mean val loss:  \t0.7953765988349915\n",
      "Epoch 104 mean train loss:\t0.6503439545631409\n",
      "Epoch 104 mean val loss:  \t0.8137791156768799\n",
      "Epoch 105 mean train loss:\t0.6713256239891052\n",
      "Epoch 105 mean val loss:  \t0.9058530330657959\n",
      "Epoch 106 mean train loss:\t0.6231722831726074\n",
      "Epoch 106 mean val loss:  \t1.3670772314071655\n",
      "Epoch 107 mean train loss:\t0.6435114145278931\n",
      "Epoch 107 mean val loss:  \t0.7768715620040894\n",
      "Epoch 108 mean train loss:\t0.59864342212677\n",
      "Epoch 108 mean val loss:  \t0.8097561597824097\n",
      "Epoch 109 mean train loss:\t0.6098068952560425\n",
      "Epoch 109 mean val loss:  \t1.769096851348877\n",
      "Epoch 110 mean train loss:\t0.614549994468689\n",
      "Epoch 110 mean val loss:  \t1.3558597564697266\n",
      "Epoch 111 mean train loss:\t0.6361655592918396\n",
      "Epoch 111 mean val loss:  \t0.8978098034858704\n",
      "Epoch 112 mean train loss:\t0.5985426306724548\n",
      "Epoch 112 mean val loss:  \t0.7542738914489746\n",
      "Epoch 113 mean train loss:\t0.5859057903289795\n",
      "Epoch 113 mean val loss:  \t0.6937545537948608\n",
      "Epoch 114 mean train loss:\t0.5862167477607727\n",
      "Epoch 114 mean val loss:  \t0.9725726246833801\n",
      "Epoch 115 mean train loss:\t0.5971859097480774\n",
      "Epoch 115 mean val loss:  \t0.6985145807266235\n",
      "Epoch 116 mean train loss:\t0.5965811610221863\n",
      "Epoch 116 mean val loss:  \t1.488779067993164\n",
      "Epoch 117 mean train loss:\t0.6142082214355469\n",
      "Epoch 117 mean val loss:  \t0.8199269771575928\n",
      "Epoch 118 mean train loss:\t0.7445054650306702\n",
      "Epoch 118 mean val loss:  \t0.8629007935523987\n",
      "Epoch 119 mean train loss:\t0.7035221457481384\n",
      "Epoch 119 mean val loss:  \t1.1606301069259644\n",
      "Epoch 120 mean train loss:\t0.692122220993042\n",
      "Epoch 120 mean val loss:  \t0.895571231842041\n",
      "Epoch 121 mean train loss:\t0.6561862826347351\n",
      "Epoch 121 mean val loss:  \t1.0489182472229004\n",
      "Epoch 122 mean train loss:\t0.6379953622817993\n",
      "Epoch 122 mean val loss:  \t0.6726698279380798\n",
      "Epoch 123 mean train loss:\t0.6421846747398376\n",
      "Epoch 123 mean val loss:  \t0.6873157620429993\n",
      "Epoch 124 mean train loss:\t0.6310036778450012\n",
      "Epoch 124 mean val loss:  \t0.7112888097763062\n",
      "Epoch 125 mean train loss:\t0.6471426486968994\n",
      "Epoch 125 mean val loss:  \t0.7238321304321289\n",
      "Epoch 126 mean train loss:\t0.6514801383018494\n",
      "Epoch 126 mean val loss:  \t0.7511057257652283\n",
      "Epoch 127 mean train loss:\t0.6349472403526306\n",
      "Epoch 127 mean val loss:  \t1.2679975032806396\n",
      "Epoch 128 mean train loss:\t0.6187002062797546\n",
      "Epoch 128 mean val loss:  \t0.858946681022644\n",
      "Epoch 129 mean train loss:\t0.6472738981246948\n",
      "Epoch 129 mean val loss:  \t0.9318960905075073\n",
      "Epoch 130 mean train loss:\t0.6275908946990967\n",
      "Epoch 130 mean val loss:  \t0.731174111366272\n",
      "Epoch 131 mean train loss:\t0.6291204690933228\n",
      "Epoch 131 mean val loss:  \t0.7948099374771118\n",
      "Epoch 132 mean train loss:\t0.6340211629867554\n",
      "Epoch 132 mean val loss:  \t0.7443127036094666\n",
      "Epoch 133 mean train loss:\t0.5988135933876038\n",
      "Epoch 133 mean val loss:  \t0.9132701754570007\n",
      "Epoch 134 mean train loss:\t0.6350175142288208\n",
      "Epoch 134 mean val loss:  \t0.6563313603401184\n",
      "Epoch 135 mean train loss:\t0.6231490969657898\n",
      "Epoch 135 mean val loss:  \t0.8095847964286804\n",
      "Epoch 136 mean train loss:\t0.6499373316764832\n",
      "Epoch 136 mean val loss:  \t0.7785978317260742\n",
      "Epoch 137 mean train loss:\t0.6436819434165955\n",
      "Epoch 137 mean val loss:  \t0.6669150590896606\n",
      "Epoch 138 mean train loss:\t0.6110449433326721\n",
      "Epoch 138 mean val loss:  \t0.7930713891983032\n",
      "Patience exhausted in epoch 138. Best val-loss was 0.6589671969413757\n",
      "Using best model from epoch 37 which had loss 0.6589671969413757\n",
      "Saved model as ../pickle/models/ConvLSTM_withLandcover_generalizationTest_allStations_20190811-143311.pkl\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    epoch_losses = torch.tensor(0.0)\n",
    "    conv_hidden_states = None\n",
    "    for i, train_batch in enumerate(train_dataloader):\n",
    "        y_train = train_batch['y'].reshape((train_batch['y'].shape[0],-1)).to(device, non_blocking=True)\n",
    "        mask = train_batch['mask'].any(dim=0).reshape(-1).to(device, non_blocking=True)\n",
    "        mask = mask & train_mask\n",
    "        landcover_batch = landcover.repeat(y_train.shape[0],1,1,1).to(device, non_blocking=True)\n",
    "        \n",
    "        if not mask.any():\n",
    "            print('Batch {} has no target values. skipping.'.format(i))\n",
    "            continue\n",
    "        if not stateful_lstm:\n",
    "            conv_hidden_states = None\n",
    "        \n",
    "        y_pred, conv_hidden_states = model(train_batch['x_conv'].to(device), landcover_batch, hidden_state=conv_hidden_states)\n",
    "        y_pred = y_pred.reshape((train_batch['y'].shape[0], -1))\n",
    "        loss = loss_fn(y_pred[:,mask], y_train[:,mask])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_losses += loss.detach()\n",
    "        \n",
    "    epoch_loss = (epoch_losses / len(train_dataloader)).item()\n",
    "    print('Epoch', epoch, 'mean train loss:\\t{}'.format(epoch_loss))\n",
    "    writer.add_scalar('loss_nse', epoch_loss, epoch)\n",
    "    \n",
    "    # eval on validation split\n",
    "    model.eval()\n",
    "    val_losses = torch.tensor(0.0)\n",
    "    conv_hidden_states = None\n",
    "    for i, val_batch in enumerate(val_dataloader):\n",
    "        y_val = val_batch['y'].reshape((val_batch['y'].shape[0],-1)).to(device, non_blocking=True)\n",
    "        mask = val_batch['mask'].any(dim=0).reshape(-1).to(device, non_blocking=True)\n",
    "        mask = mask & train_mask\n",
    "        landcover_batch = landcover.repeat(y_val.shape[0],1,1,1).to(device, non_blocking=True)\n",
    "        \n",
    "        if not stateful_lstm:\n",
    "            conv_hidden_states = None\n",
    "        \n",
    "        batch_pred, conv_hidden_states = model(val_batch['x_conv'].to(device), landcover_batch, hidden_state=conv_hidden_states)\n",
    "        batch_pred = batch_pred.detach().reshape((val_batch['y'].shape[0], -1))\n",
    "        val_losses += loss_fn(batch_pred[:,mask], y_val[:,mask]).detach()\n",
    "        \n",
    "    val_nse = (val_losses / len(val_dataloader)).item()\n",
    "    print('Epoch {} mean val loss:  \\t{}'.format(epoch, val_nse))\n",
    "    writer.add_scalar('loss_nse_val', val_nse, epoch)\n",
    "    if val_nse < best_loss_model[1] - min_improvement:\n",
    "        best_loss_model = (epoch, val_nse, model.state_dict())  # new best model\n",
    "        load_data.pickle_model('ConvLSTM_withLandcover_generalizationTest', model, 'allStations', time_stamp)\n",
    "    elif epoch > best_loss_model[0] + patience:\n",
    "        print('Patience exhausted in epoch {}. Best val-loss was {}'.format(epoch, best_loss_model[1]))\n",
    "        break\n",
    "\n",
    "print('Using best model from epoch', str(best_loss_model[0]), 'which had loss', str(best_loss_model[1]))\n",
    "model.load_state_dict(best_loss_model[2])\n",
    "load_data.pickle_model('ConvLSTM_withLandcover_generalizationTest', model, 'allStations', time_stamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-11 15:26:55,882 - 20190811-143311 - predicting\n"
     ]
    }
   ],
   "source": [
    "logger.warning('predicting')\n",
    "model.eval()\n",
    "\n",
    "predictions = []\n",
    "conv_hidden_states = None\n",
    "for i, test_batch in enumerate(test_dataloader):\n",
    "    if not stateful_lstm:\n",
    "        conv_hidden_states = None\n",
    "        \n",
    "    landcover_batch = landcover.repeat(test_batch['y'].shape[0],1,1,1).to(device)\n",
    "    pred, conv_hidden_states = model(test_batch['x_conv'].to(device), landcover_batch, hidden_state=conv_hidden_states)\n",
    "    predictions.append(pred.detach())\n",
    "\n",
    "predictions = torch.cat(predictions).cpu()\n",
    "\n",
    "if stateful_lstm:\n",
    "    # reorder time series\n",
    "    pred_indices = np.array(list(test_sampler.__iter__())).reshape(-1)\n",
    "    predictions = predictions[pred_indices.argsort()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mgauch/runoff-nn/gwf/lib/python3.6/site-packages/pandas/plotting/_converter.py:129: FutureWarning: Using an implicitly registered datetime converter for a matplotlib plotting method. The converter was registered by pandas on import. Future versions of pandas will require you to explicitly register matplotlib converters.\n",
      "\n",
      "To register the converters:\n",
      "\t>>> from pandas.plotting import register_matplotlib_converters\n",
      "\t>>> register_matplotlib_converters()\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02GA047 \tNSE: -0.12616296282789863 \tMSE: 88.27994243250185 (clipped to 0)\n",
      "04213000 \tNSE: 0.1723401198533555 \tMSE: 148.53948667487 (clipped to 0)\n",
      "04176500 \tNSE: -0.07058806609493296 \tMSE: 731.2911043062488 (clipped to 0)\n",
      "02GG003 \tNSE: -0.2027643029771522 \tMSE: 574.1968768072062 (clipped to 0)\n",
      "04214500 \tNSE: 0.3055449072174673 \tMSE: 83.70759397452733 (clipped to 0)\n",
      "02GC026 \tNSE: -0.14678084009926962 \tMSE: 184.4399199925588 (clipped to 0)\n",
      "04174500 \tNSE: 0.05194022118841535 \tMSE: 89.66202981881004 (clipped to 0)\n",
      "02GG013 \tNSE: 0.25710247148899823 \tMSE: 28.36559582766667 (clipped to 0)\n",
      "04161820 \tNSE: -2.0056195412986932 \tMSE: 94.91160148453451 (clipped to 0)\n",
      "04159492 \tNSE: 0.06675022236352368 \tMSE: 433.30208817273365 (clipped to 0)\n",
      "04200500 \tNSE: 0.07954161082699884 \tMSE: 921.1072717522692 (clipped to 0)\n",
      "02GB001 \tNSE: -0.8139419431995021 \tMSE: 13622.477610938757 (clipped to 0)\n",
      "04208504 \tNSE: -1.263790489920674 \tMSE: 2210.656284979821 (clipped to 0)\n",
      "04199000 \tNSE: 0.12749665106422658 \tMSE: 892.2248036959141 (clipped to 0)\n",
      "02GG002 \tNSE: 0.016549465481629277 \tMSE: 237.19137956687084 (clipped to 0)\n",
      "04193500 \tNSE: -0.39196544406479084 \tMSE: 98552.19639472471 (clipped to 0)\n",
      "04207200 \tNSE: 0.24132648314604754 \tMSE: 17.757733333080807 (clipped to 0)\n",
      "04160600 \tNSE: 0.15805958412215282 \tMSE: 24.35810698381774 (clipped to 0)\n",
      "04215000 \tNSE: 0.2176764746328791 \tMSE: 75.14437514662305 (clipped to 0)\n",
      "02GB007 \tNSE: 0.24462646077076666 \tMSE: 23.622052293388908 (clipped to 0)\n",
      "02GC002 \tNSE: 0.1324524287235873 \tMSE: 111.48249558665768 (clipped to 0)\n",
      "02GA038 \tNSE: 0.11221716270965199 \tMSE: 146.0744181357618 (clipped to 0)\n",
      "02GA010 \tNSE: -0.0008170932528845931 \tMSE: 477.163116718393 (clipped to 0)\n",
      "02GG009 \tNSE: 0.012043919089762212 \tMSE: 153.6799538506807 (clipped to 0)\n",
      "04165500 \tNSE: -0.8375213007036779 \tMSE: 822.5827972259848 (clipped to 0)\n",
      "04198000 \tNSE: -0.10666945922737048 \tMSE: 8175.586368349164 (clipped to 0)\n",
      "04213500 \tNSE: -0.38172411278312546 \tMSE: 1415.4614540898863 (clipped to 0)\n",
      "02GA018 \tNSE: 0.10550584787631245 \tMSE: 224.65505124633782 (clipped to 0)\n",
      "02GG006 \tNSE: 0.27833500230006847 \tMSE: 38.68647554702574 (clipped to 0)\n",
      "02GC007 \tNSE: -0.22125350577253133 \tMSE: 36.785797537223814 (clipped to 0)\n",
      "04215500 \tNSE: 0.2349263100964225 \tMSE: 138.1607646179828 (clipped to 0)\n",
      "04195820 \tNSE: 0.14068154199256633 \tMSE: 1008.6631382000431 (clipped to 0)\n",
      "04209000 \tNSE: 0.22841245650337294 \tMSE: 241.24148734409803 (clipped to 0)\n",
      "02GE007 \tNSE: 0.2405144014604088 \tMSE: 27.47443594160537 (clipped to 0)\n",
      "02GC010 \tNSE: 0.26745154590054565 \tMSE: 43.54994684499727 (clipped to 0)\n",
      "04166100 \tNSE: -0.19746777920736003 \tMSE: 8.78139966914524 (clipped to 0)\n",
      "04177000 \tNSE: 0.2797976424468879 \tMSE: 19.409330965571332 (clipped to 0)\n",
      "04164000 \tNSE: -0.925852807036569 \tMSE: 337.4620986027218 (clipped to 0)\n",
      "04159900 \tNSE: 0.3018904357889505 \tMSE: 30.330741552355406 (clipped to 0)\n",
      "02GD004 \tNSE: 0.194716818243569 \tMSE: 44.341366955284535 (clipped to 0)\n",
      "04196800 \tNSE: 0.22201471493076652 \tMSE: 232.81091944813733 (clipped to 0)\n",
      "04199500 \tNSE: 0.19398122665671458 \tMSE: 326.1539828979628 (clipped to 0)\n",
      "04212100 \tNSE: -0.06326880052502681 \tMSE: 1561.2242012174877 (clipped to 0)\n",
      "02GC018 \tNSE: 0.20698464225318203 \tMSE: 53.70839389828703 (clipped to 0)\n",
      "04166500 \tNSE: -0.27069845815142024 \tMSE: 43.92079300600092 (clipped to 0)\n",
      "04197100 \tNSE: 0.3664266887325076 \tMSE: 64.98207269929071 (clipped to 0)\n"
     ]
    }
   ],
   "source": [
    "actuals = test_dataset.data_runoff.copy()\n",
    "if len(actuals['date'].unique()) != len(predictions):\n",
    "    print('Warning: length of prediction {} and actuals {} does not match.'.format(len(predictions), len(actuals['date'].unique())))\n",
    "\n",
    "nse_dict = {}\n",
    "mse_dict = {}\n",
    "predictions_df = pd.DataFrame(columns=actuals.columns)\n",
    "predictions_df['is_test_station'] = False\n",
    "for station in actuals['station'].unique():\n",
    "    row, col = test_dataset.station_to_index[station]\n",
    "    \n",
    "    act = actuals[actuals['station'] == station].set_index('date')['runoff']\n",
    "    if predictions.shape[0] != act.shape[0]:\n",
    "        print('Warning: length of prediction {} and actuals {} does not match for station {}. Ignoring excess actuals.'.format(len(predictions), len(act), station))\n",
    "        act = act.iloc[:predictions.shape[0]]\n",
    "    pred = pd.DataFrame({'runoff': predictions[:,row,col]}, index=act.index)\n",
    "    pred['station'] = station\n",
    "    pred['is_test_station'] = station in test_stations\n",
    "    predictions_df = predictions_df.append(pred.reset_index(), sort=True)\n",
    "    \n",
    "    nse, mse = evaluate.evaluate_daily(station, pred['runoff'], act, writer=writer)\n",
    "    nse_dict[station] = nse\n",
    "    mse_dict[station] = mse\n",
    "    \n",
    "    print(station, '\\tNSE:', nse, '\\tMSE:', mse, '(clipped to 0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Median NSE (clipped to 0) 0.1723401198533555 / Min -0.39196544406479084 / Max 0.3664266887325076\n",
      "Train Median MSE (clipped to 0) 89.66202981881004 / Min 8.78139966914524 / Max 98552.19639472471\n",
      "Test Median NSE (clipped to 0) 0.07954161082699884 / Min -2.0056195412986932 / Max 0.3018904357889505\n",
      "Test Median MSE (clipped to 0) 146.0744181357618 / Min 30.330741552355406 / Max 1415.4614540898863\n",
      "Test Median NSE (clipped to 0) -0.23673138056428622 / Min -1.263790489920674 / Max 0.24462646077076666\n",
      "Test Median MSE (clipped to 0) 455.829487704964 / Min 23.622052293388908 / Max 13622.477610938757\n"
     ]
    }
   ],
   "source": [
    "nse_train = list(nse_dict[s] for s in train_stations)\n",
    "mse_train = list(mse_dict[s] for s in train_stations)\n",
    "print('Train Median NSE (clipped to 0)', np.median(nse_train), '/ Min', np.min(nse_train), '/ Max', np.max(nse_train))\n",
    "print('Train Median MSE (clipped to 0)', np.median(mse_train), '/ Min', np.min(mse_train), '/ Max', np.max(mse_train))\n",
    "\n",
    "nse_test = list(nse_dict[s] for s in test_stations)\n",
    "mse_test = list(mse_dict[s] for s in test_stations)\n",
    "print('Test Median NSE (clipped to 0)', np.median(nse_test), '/ Min', np.min(nse_test), '/ Max', np.max(nse_test))\n",
    "print('Test Median MSE (clipped to 0)', np.median(mse_test), '/ Min', np.min(mse_test), '/ Max', np.max(mse_test))\n",
    "\n",
    "nse_test = list(nse_dict[s] for s in exclude_downstream_stations)\n",
    "mse_test = list(mse_dict[s] for s in exclude_downstream_stations)\n",
    "print('Test Median NSE (clipped to 0)', np.median(nse_test), '/ Min', np.min(nse_test), '/ Max', np.max(nse_test))\n",
    "print('Test Median MSE (clipped to 0)', np.median(mse_test), '/ Min', np.min(mse_test), '/ Max', np.max(mse_test))\n",
    "\n",
    "writer.add_scalar('nse_median', np.median(nse_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('02GA047', -0.12616296282789863), ('04213000', 0.1723401198533555), ('04176500', -0.07058806609493296), ('02GG003', -0.2027643029771522), ('04214500', 0.3055449072174673), ('02GC026', -0.14678084009926962), ('04174500', 0.05194022118841535), ('02GG013', 0.25710247148899823), ('04161820', -2.0056195412986932), ('04159492', 0.06675022236352368), ('04200500', 0.07954161082699884), ('02GB001', -0.8139419431995021), ('04208504', -1.263790489920674), ('04199000', 0.12749665106422658), ('02GG002', 0.016549465481629277), ('04193500', -0.39196544406479084), ('04207200', 0.24132648314604754), ('04160600', 0.15805958412215282), ('04215000', 0.2176764746328791), ('02GB007', 0.24462646077076666), ('02GC002', 0.1324524287235873), ('02GA038', 0.11221716270965199), ('02GA010', -0.0008170932528845931), ('02GG009', 0.012043919089762212), ('04165500', -0.8375213007036779), ('04198000', -0.10666945922737048), ('04213500', -0.38172411278312546), ('02GA018', 0.10550584787631245), ('02GG006', 0.27833500230006847), ('02GC007', -0.22125350577253133), ('04215500', 0.2349263100964225), ('04195820', 0.14068154199256633), ('04209000', 0.22841245650337294), ('02GE007', 0.2405144014604088), ('02GC010', 0.26745154590054565), ('04166100', -0.19746777920736003), ('04177000', 0.2797976424468879), ('04164000', -0.925852807036569), ('04159900', 0.3018904357889505), ('02GD004', 0.194716818243569), ('04196800', 0.22201471493076652), ('04199500', 0.19398122665671458), ('04212100', -0.06326880052502681), ('02GC018', 0.20698464225318203), ('04166500', -0.27069845815142024), ('04197100', 0.3664266887325076)]\n"
     ]
    }
   ],
   "source": [
    "print(list((s, nse_dict[s]) for s in nse_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ConvLSTM_withLandcover_generalizationTest_20190811-143311.pkl'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_df = pd.merge(predictions_df.rename({'runoff': 'prediction'}, axis=1), actuals.rename({'runoff': 'actual'}, axis=1), \n",
    "                   on=['date', 'station'])[['date', 'station', 'prediction', 'actual', 'is_test_station']]\n",
    "load_data.pickle_results('ConvLSTM_withLandcover_generalizationTest', save_df, time_stamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20190811-152712'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.now().strftime('%Y%m%d-%H%M%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
