{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression to predict streamflow. Streamflow is divided by 24 to match forcing resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import metrics, svm, neural_network, ensemble\n",
    "from datetime import datetime, timedelta\n",
    "import hydroeval\n",
    "import netCDF4 as nc\n",
    "from src import load_data, evaluate\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "station_data_dict = load_data.load_train_test_gridded_dividedStreamflow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test and train splits for each station (by time), then create predictions for each subbasin\n",
    "history = 7 * 24\n",
    "train_start = datetime.strptime('2010-01-01', '%Y-%m-%d') + timedelta(days=history // 24 + 1)\n",
    "use_runoff_history = False\n",
    "\n",
    "def train_predict(station):\n",
    "    station_data = station_data_dict[station]\n",
    "    station_train = station_data.loc[train_start : '2013-12-31']\n",
    "    station_test = station_data.loc['2014-01-01' : '2014-12-31']\n",
    "    print('Fitting model for', station)\n",
    "    m = linear_model.Lasso(alpha=.1)\n",
    "    m.fit(station_train.drop(['station', 'runoff'], axis=1), station_train['runoff'])\n",
    "    \n",
    "    station_test = station_test[~pd.isna(station_test['runoff'])]\n",
    "    if len(station_test) == 0:\n",
    "        print('Skipping', station)\n",
    "        return (station, None, None)\n",
    "        \n",
    "    predict = pd.DataFrame(index=station_test.index)\n",
    "    predict = predict.join(station_test.drop(['station', 'runoff'], axis=1))\n",
    "    predict['runoff'] = np.nan\n",
    "    if use_runoff_history:\n",
    "        for i in range(history):\n",
    "            for j in range(i + 1, history + 1):\n",
    "                predict.iloc[i]['runoff_-{}'.format(j)] = station_test.iloc[i]['runoff_-{}'.format(j)]\n",
    "    print('Predicting', station)\n",
    "    if not use_runoff_history:\n",
    "        predict['runoff'] = m.predict(predict.drop('runoff', axis=1))\n",
    "    else:\n",
    "        for i in range(len(predict)):\n",
    "            predict.iloc[i]['runoff'] = m.predict([predict.iloc[i].drop('runoff')])[0]\n",
    "            for j in range(1, history + 1):\n",
    "                if (i + j) >= len(predict):\n",
    "                    break\n",
    "                predict.iloc[i + j]['runoff_-{}'.format(j)] = predict.iloc[i]['runoff']\n",
    "    \n",
    "    return (station, predict[['runoff']], station_test['runoff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = Parallel(n_jobs=-1)(delayed(train_predict)(station) for station in station_data_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate each station\n",
    "nse_list_clipped_before = []\n",
    "nse_list_clipped_after = []\n",
    "#plot_list = ['04159492', '04200500']\n",
    "for result in result_list:\n",
    "    station, predict, actual = result\n",
    "    if predict is None:\n",
    "        print(station, '\\tskipped')\n",
    "        continue\n",
    "    nses = evaluate.evaluate_hourly(station, predict['runoff'], actual, plot=True)\n",
    "    nse_list_clipped_before.append(nses[0])\n",
    "    nse_list_clipped_after.append(nses[1])\n",
    "    \n",
    "    print(station, '\\tNSE: clipped before aggregating:', nse_list_clipped_before[-1], \n",
    "          '\\tclipped after aggregating:', nse_list_clipped_after[-1])\n",
    "    \n",
    "print('Median NSE (clipped before aggregating)', np.median(nse_list_clipped_before), '/ Min', np.min(nse_list_clipped_before), '/ Max', np.max(nse_list_clipped_after))\n",
    "print('Median NSE (clipped after aggregating)', np.median(nse_list_clipped_after), '/ Min', np.min(nse_list_clipped_after), '/ Max', np.max(nse_list_clipped_after))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data.pickle_results('LinReg_VIC_divideStreamflow', result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
