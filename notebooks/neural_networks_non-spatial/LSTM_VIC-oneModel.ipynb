{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM trained on gridded forcings for each station, one model for all stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20190904-215419'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('../..')\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn import preprocessing\n",
    "import netCDF4 as nc\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from src import load_data, evaluate, datasets, utils\n",
    "\n",
    "time_stamp = datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "time_stamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available\n",
      "cuda devices: ['Tesla V100-SXM2-16GB']\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = False\n",
    "if torch.cuda.is_available():\n",
    "    print('CUDA Available')\n",
    "    USE_CUDA = True\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "device = torch.device('cuda:0' if USE_CUDA else 'cpu')\n",
    "num_devices = torch.cuda.device_count() if USE_CUDA else 0\n",
    "print('cuda devices: {}'.format(list(torch.cuda.get_device_name(i) for i in range(num_devices))))\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdrs_vars = [4,5]\n",
    "agg = None #['sum', 'minmax']\n",
    "seq_len = 5*24\n",
    "seq_steps = 1\n",
    "validation_fraction = 0.1\n",
    "batch_size = 4\n",
    "\n",
    "train_start = datetime.strptime('2010-01-01', '%Y-%m-%d') + timedelta(hours=seq_len * seq_steps)\n",
    "train_end = '2012-12-31'\n",
    "test_start = '2013-01-01'\n",
    "test_end = '2014-12-31'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.RdrsDataset(rdrs_vars, seq_len, seq_steps, train_start, train_end, station=True, aggregate_daily=agg)\n",
    "test_dataset = datasets.RdrsDataset(rdrs_vars, seq_len, seq_steps, test_start, test_end, station=True, aggregate_daily=agg,\n",
    "                                    conv_scalers=train_dataset.conv_scalers, fc_scalers=train_dataset.fc_scalers)\n",
    "\n",
    "val_indices = np.random.choice(len(train_dataset), size=int(validation_fraction * len(train_dataset)), replace=False)\n",
    "train_indices = list(i for i in range(len(train_dataset)) if i not in val_indices)\n",
    "train_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\n",
    "val_sampler = torch.utils.data.SubsetRandomSampler(val_indices)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size, sampler=train_sampler, pin_memory=True, drop_last=False)\n",
    "val_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size, sampler=val_sampler, pin_memory=True, drop_last=False)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size, shuffle=False, pin_memory=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMRegression(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, batch_size, dropout):\n",
    "        super(LSTMRegression, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, dropout=dropout)\n",
    "        self.linear = nn.Linear(hidden_dim, 1)\n",
    "        self.init_hidden(batch_size)\n",
    "    def init_hidden(self, batch_size):\n",
    "        self.hidden = (torch.randn(self.num_layers, batch_size, self.hidden_dim, device=device, requires_grad=True),\n",
    "                       torch.randn(self.num_layers, batch_size, self.hidden_dim, device=device, requires_grad=True))\n",
    "\n",
    "    def forward(self, input):\n",
    "        lstm_out, self.hidden = self.lstm(input.permute(1,0,2), self.hidden)\n",
    "        return self.linear(lstm_out[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mask of the grid that contains all grid cells that fall into a station's subwatershed\n",
    "station_cell_mask = torch.zeros(train_dataset.x_conv.shape[-2:]).bool()\n",
    "station_cell_mapping = load_data.get_station_cell_mapping()\n",
    "for station in station_cell_mapping['station'].unique():    \n",
    "    for _, row in station_cell_mapping[station_cell_mapping['station'] == station].iterrows():\n",
    "        station_cell_mask[row['col'] - 1, row['row'] - 1] = True\n",
    "\n",
    "onehot_vars = list(i for i,v in enumerate(train_dataset.fc_var_names) if v.startswith('station') or v.startswith('month'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mgauch/miniconda3/envs/gwf/lib/python3.7/site-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"{'time_stamp': '20190904-215419', 'H': 20, 'batch_size': 4, 'lstm_layers': 1, 'loss': NSELoss(), 'optimizer': Adam (\\nParameter Group 0\\n    amsgrad: False\\n    betas: (0.9, 0.999)\\n    eps: 1e-08\\n    lr: 0.002\\n    weight_decay: 1e-05\\n), 'lr': 0.002, 'patience': 100, 'min_improvement': 0.01, 'dropout': 0.3, 'num_epochs': 300, 'seq_len': 120, 'seq_steps': 1, 'train_start': datetime.datetime(2010, 1, 6, 0, 0), 'train_end': '2012-12-31', 'weight_decay': 1e-05, 'validation_fraction': 0.1, 'test_start': '2013-01-01', 'test_end': '2014-12-31', 'input_dim': 784, 'model': 'LSTMRegression((lstm):LSTM(784,20,dropout=0.3)(linear):Linear(in_features=20,out_features=1,bias=True))', 'train len': 49113, 'test len': 33580, 'rdrs_vars': [4, 5], 'aggregate_daily': None}\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 300\n",
    "learning_rate = 2e-3\n",
    "patience = 100\n",
    "min_improvement = 0.01\n",
    "H = 20\n",
    "lstm_layers = 1\n",
    "dropout = 0.3\n",
    "weight_decay = 1e-5\n",
    "best_loss_model = (-1, np.inf, None)\n",
    "input_dim = train_dataset.x_conv.shape[2] * int(station_cell_mask.sum()) + len(onehot_vars)\n",
    "model = LSTMRegression(input_dim, H, lstm_layers, batch_size, dropout).to(device)\n",
    "loss_fn = evaluate.NSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "param_description = {'time_stamp': time_stamp, 'H': H, 'batch_size': batch_size, 'lstm_layers': lstm_layers, 'loss': loss_fn, 'optimizer': optimizer, 'lr': learning_rate, \n",
    "                     'patience': patience, 'min_improvement': min_improvement, 'dropout': dropout, 'num_epochs': num_epochs, 'seq_len': seq_len, 'seq_steps': seq_steps, \n",
    "                     'train_start': train_start, 'train_end': train_end, 'weight_decay': weight_decay, 'validation_fraction': validation_fraction, 'test_start': test_start, \n",
    "                     'test_end': test_end, 'input_dim': input_dim, 'model': str(model).replace('\\n','').replace(' ', ''), 'train len':len(train_dataset), \n",
    "                     'test len': len(test_dataset), 'rdrs_vars': rdrs_vars, 'aggregate_daily': agg}\n",
    "writer = SummaryWriter()\n",
    "writer.add_text('Parameter Description', str(param_description))\n",
    "str(param_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 mean loss: 2.2175967693328857\n",
      "Epoch 0 mean val loss: 2.160881996154785\n",
      "Saved model as /home/mgauch/runoff-nn/src/../pickle/models/LSTM_VIC-oneModel_allStations_20190904-215419.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mgauch/miniconda3/envs/gwf/lib/python3.7/site-packages/torch/serialization.py:256: UserWarning: Couldn't retrieve source code for container of type LSTMRegression. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 mean loss: 2.524139881134033\n",
      "Epoch 1 mean val loss: 2.033656120300293\n",
      "Saved model as /home/mgauch/runoff-nn/src/../pickle/models/LSTM_VIC-oneModel_allStations_20190904-215419.pkl\n",
      "Epoch 2 mean loss: 2.2277350425720215\n",
      "Epoch 2 mean val loss: 2.5408828258514404\n",
      "Epoch 3 mean loss: 2.3025639057159424\n",
      "Epoch 3 mean val loss: 1.8486477136611938\n",
      "Saved model as /home/mgauch/runoff-nn/src/../pickle/models/LSTM_VIC-oneModel_allStations_20190904-215419.pkl\n",
      "Epoch 4 mean loss: 1.9110186100006104\n",
      "Epoch 4 mean val loss: 1.7836331129074097\n",
      "Saved model as /home/mgauch/runoff-nn/src/../pickle/models/LSTM_VIC-oneModel_allStations_20190904-215419.pkl\n",
      "Epoch 5 mean loss: 1.8606027364730835\n",
      "Epoch 5 mean val loss: 1.4809465408325195\n",
      "Saved model as /home/mgauch/runoff-nn/src/../pickle/models/LSTM_VIC-oneModel_allStations_20190904-215419.pkl\n",
      "Epoch 6 mean loss: 2.106511116027832\n",
      "Epoch 6 mean val loss: 1.8561232089996338\n",
      "Epoch 7 mean loss: 1.502906322479248\n",
      "Epoch 7 mean val loss: 1.712312936782837\n",
      "Epoch 8 mean loss: 1.6169322729110718\n",
      "Epoch 8 mean val loss: 1.3655179738998413\n",
      "Saved model as /home/mgauch/runoff-nn/src/../pickle/models/LSTM_VIC-oneModel_allStations_20190904-215419.pkl\n",
      "Epoch 9 mean loss: 1.4891104698181152\n",
      "Epoch 9 mean val loss: 1.6734448671340942\n",
      "Epoch 10 mean loss: 1.5315932035446167\n",
      "Epoch 10 mean val loss: 1.261775255203247\n",
      "Saved model as /home/mgauch/runoff-nn/src/../pickle/models/LSTM_VIC-oneModel_allStations_20190904-215419.pkl\n",
      "Epoch 11 mean loss: 1.7460788488388062\n",
      "Epoch 11 mean val loss: 8.202181816101074\n",
      "Epoch 12 mean loss: 1.3526352643966675\n",
      "Epoch 12 mean val loss: 1.2371695041656494\n",
      "Saved model as /home/mgauch/runoff-nn/src/../pickle/models/LSTM_VIC-oneModel_allStations_20190904-215419.pkl\n",
      "Epoch 13 mean loss: 1.462404727935791\n",
      "Epoch 13 mean val loss: 1.320753812789917\n",
      "Epoch 14 mean loss: 1.330930471420288\n",
      "Epoch 14 mean val loss: 0.9733155965805054\n",
      "Saved model as /home/mgauch/runoff-nn/src/../pickle/models/LSTM_VIC-oneModel_allStations_20190904-215419.pkl\n",
      "Epoch 15 mean loss: 1.3101552724838257\n",
      "Epoch 15 mean val loss: 1.087993860244751\n",
      "Epoch 16 mean loss: 1.5027648210525513\n",
      "Epoch 16 mean val loss: 1.327367901802063\n",
      "Epoch 17 mean loss: 1.1937925815582275\n",
      "Epoch 17 mean val loss: 1.4759225845336914\n",
      "Epoch 18 mean loss: 1.2724859714508057\n",
      "Epoch 18 mean val loss: 1.144997239112854\n",
      "Epoch 19 mean loss: 1.3178300857543945\n",
      "Epoch 19 mean val loss: 1.3615763187408447\n",
      "Epoch 20 mean loss: 1.3216557502746582\n",
      "Epoch 20 mean val loss: 1.5046809911727905\n",
      "Epoch 21 mean loss: 1.2668864727020264\n",
      "Epoch 21 mean val loss: 1.2364460229873657\n",
      "Epoch 22 mean loss: 1.3049893379211426\n",
      "Epoch 22 mean val loss: 1.1603679656982422\n",
      "Epoch 23 mean loss: 1.1840314865112305\n",
      "Epoch 23 mean val loss: 1.334949254989624\n",
      "Epoch 24 mean loss: 1.2352157831192017\n",
      "Epoch 24 mean val loss: 0.9590833783149719\n",
      "Saved model as /home/mgauch/runoff-nn/src/../pickle/models/LSTM_VIC-oneModel_allStations_20190904-215419.pkl\n",
      "Epoch 25 mean loss: 1.1718722581863403\n",
      "Epoch 25 mean val loss: 2.100438356399536\n",
      "Epoch 26 mean loss: 1.195245623588562\n",
      "Epoch 26 mean val loss: 1.8815653324127197\n",
      "Epoch 27 mean loss: 1.3283964395523071\n",
      "Epoch 27 mean val loss: 0.9251906871795654\n",
      "Saved model as /home/mgauch/runoff-nn/src/../pickle/models/LSTM_VIC-oneModel_allStations_20190904-215419.pkl\n",
      "Epoch 28 mean loss: 1.3308945894241333\n",
      "Epoch 28 mean val loss: 0.9748001098632812\n",
      "Epoch 29 mean loss: 1.3072948455810547\n",
      "Epoch 29 mean val loss: 1.1798185110092163\n",
      "Epoch 30 mean loss: 1.4426974058151245\n",
      "Epoch 30 mean val loss: 1.6455376148223877\n",
      "Epoch 31 mean loss: 1.2576202154159546\n",
      "Epoch 31 mean val loss: 1.377976655960083\n",
      "Epoch 32 mean loss: 1.2014845609664917\n",
      "Epoch 32 mean val loss: 1.103752851486206\n",
      "Epoch 33 mean loss: 1.2872222661972046\n",
      "Epoch 33 mean val loss: 1.6913800239562988\n",
      "Epoch 34 mean loss: 1.1749842166900635\n",
      "Epoch 34 mean val loss: 1.028209924697876\n",
      "Epoch 35 mean loss: 1.2614648342132568\n",
      "Epoch 35 mean val loss: 1.232909917831421\n",
      "Epoch 36 mean loss: 1.352677583694458\n",
      "Epoch 36 mean val loss: 1.058549165725708\n",
      "Epoch 37 mean loss: 1.417582631111145\n",
      "Epoch 37 mean val loss: 1.8918825387954712\n",
      "Epoch 38 mean loss: 1.4038045406341553\n",
      "Epoch 38 mean val loss: 1.1319100856781006\n",
      "Epoch 39 mean loss: 1.1825014352798462\n",
      "Epoch 39 mean val loss: 1.4163594245910645\n",
      "Epoch 40 mean loss: 1.1537641286849976\n",
      "Epoch 40 mean val loss: 1.1808767318725586\n",
      "Epoch 41 mean loss: 1.3433809280395508\n",
      "Epoch 41 mean val loss: 1.4687496423721313\n",
      "Epoch 42 mean loss: 1.2600538730621338\n",
      "Epoch 42 mean val loss: 1.5080196857452393\n",
      "Epoch 43 mean loss: 1.233349084854126\n",
      "Epoch 43 mean val loss: 1.228512167930603\n",
      "Epoch 44 mean loss: 1.2084062099456787\n",
      "Epoch 44 mean val loss: 1.477436900138855\n",
      "Epoch 45 mean loss: 1.322654128074646\n",
      "Epoch 45 mean val loss: 1.4342613220214844\n",
      "Epoch 46 mean loss: 1.270841121673584\n",
      "Epoch 46 mean val loss: 2.374101161956787\n",
      "Epoch 47 mean loss: 1.561462163925171\n",
      "Epoch 47 mean val loss: 1.5044385194778442\n",
      "Epoch 48 mean loss: 1.2769230604171753\n",
      "Epoch 48 mean val loss: 1.2981536388397217\n",
      "Epoch 49 mean loss: 1.1878427267074585\n",
      "Epoch 49 mean val loss: 1.0463578701019287\n",
      "Epoch 50 mean loss: 1.266930341720581\n",
      "Epoch 50 mean val loss: 1.7601163387298584\n",
      "Epoch 51 mean loss: 1.2636407613754272\n",
      "Epoch 51 mean val loss: 1.7379685640335083\n",
      "Epoch 52 mean loss: 1.2394814491271973\n",
      "Epoch 52 mean val loss: 1.7619340419769287\n",
      "Epoch 53 mean loss: 1.1061971187591553\n",
      "Epoch 53 mean val loss: 1.9048020839691162\n",
      "Epoch 54 mean loss: 1.1262625455856323\n",
      "Epoch 54 mean val loss: 0.9728798866271973\n",
      "Epoch 55 mean loss: 1.4354647397994995\n",
      "Epoch 55 mean val loss: 1.0461344718933105\n",
      "Epoch 56 mean loss: 1.1554291248321533\n",
      "Epoch 56 mean val loss: 1.2435146570205688\n",
      "Epoch 57 mean loss: 1.392531156539917\n",
      "Epoch 57 mean val loss: 1.1084578037261963\n",
      "Epoch 58 mean loss: 1.2529698610305786\n",
      "Epoch 58 mean val loss: 1.0767202377319336\n",
      "Epoch 59 mean loss: 1.1334929466247559\n",
      "Epoch 59 mean val loss: 1.0557368993759155\n",
      "Epoch 60 mean loss: 1.2738896608352661\n",
      "Epoch 60 mean val loss: 0.9573447704315186\n",
      "Epoch 61 mean loss: 1.1587982177734375\n",
      "Epoch 61 mean val loss: 1.2035181522369385\n",
      "Epoch 62 mean loss: 1.165303111076355\n",
      "Epoch 62 mean val loss: 0.9511855840682983\n",
      "Epoch 63 mean loss: 1.2948516607284546\n",
      "Epoch 63 mean val loss: 1.5563840866088867\n",
      "Epoch 64 mean loss: 1.1666244268417358\n",
      "Epoch 64 mean val loss: 2.152470350265503\n",
      "Epoch 65 mean loss: 1.2117966413497925\n",
      "Epoch 65 mean val loss: 0.8310253024101257\n",
      "Saved model as /home/mgauch/runoff-nn/src/../pickle/models/LSTM_VIC-oneModel_allStations_20190904-215419.pkl\n",
      "Epoch 66 mean loss: 1.3343042135238647\n",
      "Epoch 66 mean val loss: 0.8906835317611694\n",
      "Epoch 67 mean loss: 1.1935018301010132\n",
      "Epoch 67 mean val loss: 0.8153788447380066\n",
      "Saved model as /home/mgauch/runoff-nn/src/../pickle/models/LSTM_VIC-oneModel_allStations_20190904-215419.pkl\n",
      "Epoch 68 mean loss: 1.2925152778625488\n",
      "Epoch 68 mean val loss: 1.2444603443145752\n",
      "Epoch 69 mean loss: 1.1564414501190186\n",
      "Epoch 69 mean val loss: 1.3238190412521362\n",
      "Epoch 70 mean loss: 1.3463988304138184\n",
      "Epoch 70 mean val loss: 1.567818284034729\n",
      "Epoch 71 mean loss: 1.2118831872940063\n",
      "Epoch 71 mean val loss: 0.9962506890296936\n",
      "Epoch 72 mean loss: 1.478639006614685\n",
      "Epoch 72 mean val loss: 1.0263556241989136\n",
      "Epoch 73 mean loss: 1.251753807067871\n",
      "Epoch 73 mean val loss: 0.9903688430786133\n",
      "Epoch 74 mean loss: 1.1534324884414673\n",
      "Epoch 74 mean val loss: 0.8984220027923584\n",
      "Epoch 75 mean loss: 1.1754751205444336\n",
      "Epoch 75 mean val loss: 1.507264494895935\n",
      "Epoch 76 mean loss: 1.038627028465271\n",
      "Epoch 76 mean val loss: 0.9959069490432739\n",
      "Epoch 77 mean loss: 1.1765592098236084\n",
      "Epoch 77 mean val loss: 1.0680779218673706\n",
      "Epoch 78 mean loss: 1.1643158197402954\n",
      "Epoch 78 mean val loss: 0.985262393951416\n",
      "Epoch 79 mean loss: 1.1239664554595947\n",
      "Epoch 79 mean val loss: 0.9435823559761047\n",
      "Epoch 80 mean loss: 1.4580224752426147\n",
      "Epoch 80 mean val loss: 1.4289937019348145\n",
      "Epoch 81 mean loss: 1.359839677810669\n",
      "Epoch 81 mean val loss: 1.0083065032958984\n",
      "Epoch 82 mean loss: 1.1588799953460693\n",
      "Epoch 82 mean val loss: 1.1990940570831299\n",
      "Epoch 83 mean loss: 1.2481348514556885\n",
      "Epoch 83 mean val loss: 1.1180169582366943\n",
      "Epoch 84 mean loss: 1.168422818183899\n",
      "Epoch 84 mean val loss: 1.447758436203003\n",
      "Epoch 85 mean loss: 1.099773645401001\n",
      "Epoch 85 mean val loss: 0.849648118019104\n",
      "Epoch 86 mean loss: 1.2559140920639038\n",
      "Epoch 86 mean val loss: 1.1246567964553833\n",
      "Epoch 87 mean loss: 1.2221239805221558\n",
      "Epoch 87 mean val loss: 1.019024133682251\n",
      "Epoch 88 mean loss: 1.0326080322265625\n",
      "Epoch 88 mean val loss: 1.1605730056762695\n",
      "Epoch 89 mean loss: 1.1972483396530151\n",
      "Epoch 89 mean val loss: 1.1236296892166138\n",
      "Epoch 90 mean loss: 1.0025300979614258\n",
      "Epoch 90 mean val loss: 0.8580048680305481\n",
      "Epoch 91 mean loss: 1.0288339853286743\n",
      "Epoch 91 mean val loss: 1.1370768547058105\n",
      "Epoch 92 mean loss: 1.045613169670105\n",
      "Epoch 92 mean val loss: 1.3096874952316284\n",
      "Epoch 93 mean loss: 1.0939134359359741\n",
      "Epoch 93 mean val loss: 0.8925728797912598\n",
      "Epoch 94 mean loss: 1.154430627822876\n",
      "Epoch 94 mean val loss: 1.1949355602264404\n",
      "Epoch 95 mean loss: 1.2536983489990234\n",
      "Epoch 95 mean val loss: 1.2268078327178955\n",
      "Epoch 96 mean loss: 1.1046228408813477\n",
      "Epoch 96 mean val loss: 0.981997013092041\n",
      "Epoch 97 mean loss: 1.0392471551895142\n",
      "Epoch 97 mean val loss: 1.8823295831680298\n",
      "Epoch 98 mean loss: 1.0244286060333252\n",
      "Epoch 98 mean val loss: 0.7746825814247131\n",
      "Saved model as /home/mgauch/runoff-nn/src/../pickle/models/LSTM_VIC-oneModel_allStations_20190904-215419.pkl\n",
      "Epoch 99 mean loss: 1.0618423223495483\n",
      "Epoch 99 mean val loss: 2.229618549346924\n",
      "Epoch 100 mean loss: 1.1586418151855469\n",
      "Epoch 100 mean val loss: 1.1365182399749756\n",
      "Epoch 101 mean loss: 1.1563420295715332\n",
      "Epoch 101 mean val loss: 1.0172089338302612\n",
      "Epoch 102 mean loss: 1.0273158550262451\n",
      "Epoch 102 mean val loss: 1.0280638933181763\n",
      "Epoch 103 mean loss: 1.1087414026260376\n",
      "Epoch 103 mean val loss: 0.953856348991394\n",
      "Epoch 104 mean loss: 0.9797042012214661\n",
      "Epoch 104 mean val loss: 2.049159288406372\n",
      "Epoch 105 mean loss: 1.1008650064468384\n",
      "Epoch 105 mean val loss: 0.9622957706451416\n",
      "Epoch 106 mean loss: 1.049791693687439\n",
      "Epoch 106 mean val loss: 3.616608142852783\n",
      "Epoch 107 mean loss: 0.9753413200378418\n",
      "Epoch 107 mean val loss: 1.2454711198806763\n",
      "Epoch 108 mean loss: 1.3372076749801636\n",
      "Epoch 108 mean val loss: 1.3810428380966187\n",
      "Epoch 109 mean loss: 1.093772053718567\n",
      "Epoch 109 mean val loss: 1.7767744064331055\n",
      "Epoch 110 mean loss: 1.1022429466247559\n",
      "Epoch 110 mean val loss: 1.467098355293274\n",
      "Epoch 111 mean loss: 1.2501333951950073\n",
      "Epoch 111 mean val loss: 1.080365777015686\n",
      "Epoch 112 mean loss: 1.108113408088684\n",
      "Epoch 112 mean val loss: 1.2489699125289917\n",
      "Epoch 113 mean loss: 1.0751374959945679\n",
      "Epoch 113 mean val loss: 0.8827255964279175\n",
      "Epoch 114 mean loss: 1.0955382585525513\n",
      "Epoch 114 mean val loss: 1.1301440000534058\n",
      "Epoch 115 mean loss: 1.019180417060852\n",
      "Epoch 115 mean val loss: 0.9512001872062683\n",
      "Epoch 116 mean loss: 0.9224234223365784\n",
      "Epoch 116 mean val loss: 0.7315791249275208\n",
      "Saved model as /home/mgauch/runoff-nn/src/../pickle/models/LSTM_VIC-oneModel_allStations_20190904-215419.pkl\n",
      "Epoch 117 mean loss: 0.965649425983429\n",
      "Epoch 117 mean val loss: 1.0604357719421387\n",
      "Epoch 118 mean loss: 0.9939622282981873\n",
      "Epoch 118 mean val loss: 1.0738509893417358\n",
      "Epoch 119 mean loss: 1.1093477010726929\n",
      "Epoch 119 mean val loss: 1.1150784492492676\n",
      "Epoch 120 mean loss: 1.1481750011444092\n",
      "Epoch 120 mean val loss: 1.1215955018997192\n",
      "Epoch 121 mean loss: 1.1080032587051392\n",
      "Epoch 121 mean val loss: 0.7643274068832397\n",
      "Epoch 122 mean loss: 1.0053447484970093\n",
      "Epoch 122 mean val loss: 0.9808339476585388\n",
      "Epoch 123 mean loss: 1.1194196939468384\n",
      "Epoch 123 mean val loss: 1.4745001792907715\n",
      "Epoch 124 mean loss: 1.108853816986084\n",
      "Epoch 124 mean val loss: 2.062023401260376\n",
      "Epoch 125 mean loss: 0.9941748976707458\n",
      "Epoch 125 mean val loss: 1.3436946868896484\n",
      "Epoch 126 mean loss: 0.9995077252388\n",
      "Epoch 126 mean val loss: 0.7890411019325256\n",
      "Epoch 127 mean loss: 0.9593319296836853\n",
      "Epoch 127 mean val loss: 1.0849473476409912\n",
      "Epoch 128 mean loss: 1.2027453184127808\n",
      "Epoch 128 mean val loss: 1.1298154592514038\n",
      "Epoch 129 mean loss: 0.9388056397438049\n",
      "Epoch 129 mean val loss: 3.0012073516845703\n",
      "Epoch 130 mean loss: 0.9712797999382019\n",
      "Epoch 130 mean val loss: 0.9974569082260132\n",
      "Epoch 131 mean loss: 1.088514804840088\n",
      "Epoch 131 mean val loss: 2.3013548851013184\n",
      "Epoch 132 mean loss: 1.0672990083694458\n",
      "Epoch 132 mean val loss: 0.7806261777877808\n",
      "Epoch 133 mean loss: 1.3236757516860962\n",
      "Epoch 133 mean val loss: 0.798508882522583\n",
      "Epoch 134 mean loss: 0.9809991121292114\n",
      "Epoch 134 mean val loss: 0.9415357708930969\n",
      "Epoch 135 mean loss: 1.0539907217025757\n",
      "Epoch 135 mean val loss: 0.7904759645462036\n",
      "Epoch 136 mean loss: 1.1164404153823853\n",
      "Epoch 136 mean val loss: 0.8513737320899963\n",
      "Epoch 137 mean loss: 1.0749216079711914\n",
      "Epoch 137 mean val loss: 0.9987998604774475\n",
      "Epoch 138 mean loss: 1.0036213397979736\n",
      "Epoch 138 mean val loss: 1.3464709520339966\n",
      "Epoch 139 mean loss: 0.9853699207305908\n",
      "Epoch 139 mean val loss: 0.8894572854042053\n",
      "Epoch 140 mean loss: 0.8565386533737183\n",
      "Epoch 140 mean val loss: 1.192791223526001\n",
      "Epoch 141 mean loss: 0.9586414098739624\n",
      "Epoch 141 mean val loss: 1.0945138931274414\n",
      "Epoch 142 mean loss: 0.9346694350242615\n",
      "Epoch 142 mean val loss: 0.9309226870536804\n",
      "Epoch 143 mean loss: 0.9539884328842163\n",
      "Epoch 143 mean val loss: 0.8313456773757935\n",
      "Epoch 144 mean loss: 1.0062692165374756\n",
      "Epoch 144 mean val loss: 0.7747256755828857\n",
      "Epoch 145 mean loss: 1.1546374559402466\n",
      "Epoch 145 mean val loss: 0.9840619564056396\n",
      "Epoch 146 mean loss: 0.9382234811782837\n",
      "Epoch 146 mean val loss: 0.7340859770774841\n",
      "Epoch 147 mean loss: 1.0368798971176147\n",
      "Epoch 147 mean val loss: 1.4988304376602173\n",
      "Epoch 148 mean loss: 1.0078885555267334\n",
      "Epoch 148 mean val loss: 0.9403427839279175\n",
      "Epoch 149 mean loss: 1.0026850700378418\n",
      "Epoch 149 mean val loss: 0.9204046130180359\n",
      "Epoch 150 mean loss: 0.9898104667663574\n",
      "Epoch 150 mean val loss: 0.9538678526878357\n",
      "Epoch 151 mean loss: 1.0976186990737915\n",
      "Epoch 151 mean val loss: 0.9557613134384155\n",
      "Epoch 152 mean loss: 1.0230430364608765\n",
      "Epoch 152 mean val loss: 1.1241769790649414\n",
      "Epoch 153 mean loss: 1.039351224899292\n",
      "Epoch 153 mean val loss: 0.8487571477890015\n",
      "Epoch 154 mean loss: 1.0556211471557617\n",
      "Epoch 154 mean val loss: 1.0479248762130737\n",
      "Epoch 155 mean loss: 1.023077368736267\n",
      "Epoch 155 mean val loss: 0.8075686693191528\n",
      "Epoch 156 mean loss: 1.3698573112487793\n",
      "Epoch 156 mean val loss: 0.9864452481269836\n",
      "Epoch 157 mean loss: 0.9751328825950623\n",
      "Epoch 157 mean val loss: 1.0064022541046143\n",
      "Epoch 158 mean loss: 1.234394907951355\n",
      "Epoch 158 mean val loss: 1.4325193166732788\n",
      "Epoch 159 mean loss: 1.0124181509017944\n",
      "Epoch 159 mean val loss: 1.1290102005004883\n",
      "Epoch 160 mean loss: 1.135797381401062\n",
      "Epoch 160 mean val loss: 0.7842928767204285\n",
      "Epoch 161 mean loss: 1.0579084157943726\n",
      "Epoch 161 mean val loss: 4.08690071105957\n",
      "Epoch 162 mean loss: 1.2228567600250244\n",
      "Epoch 162 mean val loss: 1.0180511474609375\n",
      "Epoch 163 mean loss: 1.0492819547653198\n",
      "Epoch 163 mean val loss: 0.9094087481498718\n",
      "Epoch 164 mean loss: 1.0410852432250977\n",
      "Epoch 164 mean val loss: 1.0857031345367432\n",
      "Epoch 165 mean loss: 1.099050521850586\n",
      "Epoch 165 mean val loss: 1.338416576385498\n",
      "Epoch 166 mean loss: 1.0191279649734497\n",
      "Epoch 166 mean val loss: 0.8142310976982117\n",
      "Epoch 167 mean loss: 1.0992251634597778\n",
      "Epoch 167 mean val loss: 1.2127264738082886\n",
      "Epoch 168 mean loss: 0.9496379494667053\n",
      "Epoch 168 mean val loss: 0.9881578087806702\n",
      "Epoch 169 mean loss: 0.9522808194160461\n",
      "Epoch 169 mean val loss: 0.8685530424118042\n",
      "Epoch 170 mean loss: 1.0820201635360718\n",
      "Epoch 170 mean val loss: 0.7673674821853638\n",
      "Epoch 171 mean loss: 0.9964067339897156\n",
      "Epoch 171 mean val loss: 0.9100220799446106\n",
      "Epoch 172 mean loss: 0.8956103324890137\n",
      "Epoch 172 mean val loss: 1.3788764476776123\n",
      "Epoch 173 mean loss: 1.0550214052200317\n",
      "Epoch 173 mean val loss: 1.3751050233840942\n",
      "Epoch 174 mean loss: 0.9601740837097168\n",
      "Epoch 174 mean val loss: 1.3346147537231445\n",
      "Epoch 175 mean loss: 1.0512382984161377\n",
      "Epoch 175 mean val loss: 1.0433855056762695\n",
      "Epoch 176 mean loss: 1.037461757659912\n",
      "Epoch 176 mean val loss: 3.1688461303710938\n",
      "Epoch 177 mean loss: 1.0571329593658447\n",
      "Epoch 177 mean val loss: 1.9800639152526855\n",
      "Epoch 178 mean loss: 1.1615071296691895\n",
      "Epoch 178 mean val loss: 5.401323318481445\n",
      "Epoch 179 mean loss: 0.9928908348083496\n",
      "Epoch 179 mean val loss: 0.9565341472625732\n",
      "Epoch 180 mean loss: 0.9609962701797485\n",
      "Epoch 180 mean val loss: 1.1456358432769775\n",
      "Epoch 181 mean loss: 0.9722944498062134\n",
      "Epoch 181 mean val loss: 1.2220990657806396\n",
      "Epoch 182 mean loss: 1.2662380933761597\n",
      "Epoch 182 mean val loss: 0.9042859673500061\n",
      "Epoch 183 mean loss: 1.12318754196167\n",
      "Epoch 183 mean val loss: 1.1762562990188599\n",
      "Epoch 184 mean loss: 1.074098825454712\n",
      "Epoch 184 mean val loss: 1.1273854970932007\n",
      "Epoch 185 mean loss: 1.0486470460891724\n",
      "Epoch 185 mean val loss: 0.9661852121353149\n",
      "Epoch 186 mean loss: 0.9492425322532654\n",
      "Epoch 186 mean val loss: 0.8355059623718262\n",
      "Epoch 187 mean loss: 1.1523817777633667\n",
      "Epoch 187 mean val loss: 0.8931122422218323\n",
      "Epoch 188 mean loss: 0.9518054127693176\n",
      "Epoch 188 mean val loss: 1.0834161043167114\n",
      "Epoch 189 mean loss: 1.102471947669983\n",
      "Epoch 189 mean val loss: 1.3696821928024292\n",
      "Epoch 190 mean loss: 0.9707769155502319\n",
      "Epoch 190 mean val loss: 0.8559222221374512\n",
      "Epoch 191 mean loss: 0.9788451790809631\n",
      "Epoch 191 mean val loss: 0.8624089360237122\n",
      "Epoch 192 mean loss: 1.0940635204315186\n",
      "Epoch 192 mean val loss: 1.5575201511383057\n",
      "Epoch 193 mean loss: 1.1784030199050903\n",
      "Epoch 193 mean val loss: 1.4781432151794434\n",
      "Epoch 194 mean loss: 0.972357451915741\n",
      "Epoch 194 mean val loss: 3.326887369155884\n",
      "Epoch 195 mean loss: 0.9065272808074951\n",
      "Epoch 195 mean val loss: 0.8190571069717407\n",
      "Epoch 196 mean loss: 0.9803063273429871\n",
      "Epoch 196 mean val loss: 0.7984086871147156\n",
      "Epoch 197 mean loss: 1.2679272890090942\n",
      "Epoch 197 mean val loss: 1.188342809677124\n",
      "Epoch 198 mean loss: 1.1020785570144653\n",
      "Epoch 198 mean val loss: 0.7556446194648743\n",
      "Epoch 199 mean loss: 1.1035937070846558\n",
      "Epoch 199 mean val loss: 1.307687520980835\n",
      "Epoch 200 mean loss: 1.1117023229599\n",
      "Epoch 200 mean val loss: 1.0809009075164795\n",
      "Epoch 201 mean loss: 1.2024860382080078\n",
      "Epoch 201 mean val loss: 1.1262314319610596\n",
      "Epoch 202 mean loss: 1.220383882522583\n",
      "Epoch 202 mean val loss: 1.4123146533966064\n",
      "Epoch 203 mean loss: 1.1870461702346802\n",
      "Epoch 203 mean val loss: 1.175682783126831\n",
      "Epoch 204 mean loss: 1.061303734779358\n",
      "Epoch 204 mean val loss: 1.3226205110549927\n",
      "Epoch 205 mean loss: 1.2917804718017578\n",
      "Epoch 205 mean val loss: 1.0767154693603516\n",
      "Epoch 206 mean loss: 1.0997995138168335\n",
      "Epoch 206 mean val loss: 1.237014651298523\n",
      "Epoch 207 mean loss: 0.9729100465774536\n",
      "Epoch 207 mean val loss: 0.8437915444374084\n",
      "Epoch 208 mean loss: 1.1895751953125\n",
      "Epoch 208 mean val loss: 1.2093164920806885\n",
      "Epoch 209 mean loss: 1.1255313158035278\n",
      "Epoch 209 mean val loss: 1.4673212766647339\n",
      "Epoch 210 mean loss: 1.2016270160675049\n",
      "Epoch 210 mean val loss: 1.4391331672668457\n",
      "Epoch 211 mean loss: 1.1450119018554688\n",
      "Epoch 211 mean val loss: 3.2912604808807373\n",
      "Epoch 212 mean loss: 1.4326460361480713\n",
      "Epoch 212 mean val loss: 1.4286468029022217\n",
      "Epoch 213 mean loss: 1.219313383102417\n",
      "Epoch 213 mean val loss: 1.4489500522613525\n",
      "Epoch 214 mean loss: 1.065698266029358\n",
      "Epoch 214 mean val loss: 1.1686888933181763\n",
      "Epoch 215 mean loss: 1.1490474939346313\n",
      "Epoch 215 mean val loss: 1.0066851377487183\n",
      "Epoch 216 mean loss: 1.0722770690917969\n",
      "Epoch 216 mean val loss: 1.4007351398468018\n",
      "Epoch 217 mean loss: 1.1681135892868042\n",
      "Epoch 217 mean val loss: 1.8708157539367676\n",
      "Patience exhausted in epoch 217. Best loss was 0.7315791249275208\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_losses = torch.tensor(0.0)\n",
    "    for i, train_batch in enumerate(train_dataloader):\n",
    "        x_train = train_batch['x_conv'][...,station_cell_mask].reshape(*train_batch['x_conv'].shape[:2], -1)\n",
    "        x_train = torch.cat([x_train, train_batch['x_fc'][:,onehot_vars].unsqueeze(dim=1).repeat(1,x_train.shape[1],1)], dim=2).to(device)\n",
    "        model.init_hidden(x_train.shape[0])\n",
    "        y_pred = model(x_train)\n",
    "\n",
    "        loss = loss_fn(y_pred.reshape(-1), train_batch['y'].to(device), means=train_batch['y_mean'].to(device))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses += loss.detach()\n",
    "        \n",
    "    train_loss = (train_losses / len(train_dataloader)).item()\n",
    "    print('Epoch', epoch, 'mean loss:', train_loss)\n",
    "    writer.add_scalar('loss_nse', train_loss, epoch)\n",
    "\n",
    "    model.eval()\n",
    "    val_losses = torch.tensor(0.0)\n",
    "    for i, val_batch in enumerate(val_dataloader):\n",
    "        x_val = val_batch['x_conv'][...,station_cell_mask].reshape(*val_batch['x_conv'].shape[:2], -1)\n",
    "        x_val = torch.cat([x_val, val_batch['x_fc'][:,onehot_vars].unsqueeze(dim=1).repeat(1,x_val.shape[1],1)], dim=2).to(device)\n",
    "        \n",
    "        model.init_hidden(x_val.shape[0])\n",
    "        y_pred = model(x_val)\n",
    "\n",
    "        loss = loss_fn(y_pred.reshape(-1), val_batch['y'].to(device), means=val_batch['y_mean'].to(device))\n",
    "        val_losses += loss.detach()\n",
    "        \n",
    "    val_loss = (val_losses / len(val_dataloader)).item()\n",
    "    print('Epoch', epoch, 'mean val loss:', val_loss)\n",
    "    writer.add_scalar('loss_nse_val', val_loss, epoch)\n",
    "    if val_loss < best_loss_model[1] - min_improvement:\n",
    "        best_loss_model = (epoch, val_loss, model.state_dict())  # new best model\n",
    "        load_data.pickle_model('LSTM_VIC-oneModel', model, 'allStations', time_stamp)\n",
    "    elif epoch > best_loss_model[0] + patience:\n",
    "        print('Patience exhausted in epoch {}. Best loss was {}'.format(epoch, best_loss_model[1]))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using best model from epoch 116 which had loss 0.7315791249275208\n"
     ]
    }
   ],
   "source": [
    "print('Using best model from epoch', str(best_loss_model[0]), 'which had loss', str(best_loss_model[1]))\n",
    "model.load_state_dict(best_loss_model[2])\n",
    "model.eval()\n",
    "predict = test_dataset.data_runoff.copy()\n",
    "predict['actual'] = predict['runoff']\n",
    "predict['runoff'] = np.nan\n",
    "pred_array = np.array([])\n",
    "for i, test_batch in enumerate(test_dataloader):\n",
    "    x_test = test_batch['x_conv'][...,station_cell_mask].reshape(*test_batch['x_conv'].shape[:2], -1)\n",
    "    x_test = torch.cat([x_test, test_batch['x_fc'][:,onehot_vars].unsqueeze(dim=1).repeat(1,x_test.shape[1],1)], dim=2).to(device)\n",
    "    model.init_hidden(x_test.shape[0])\n",
    "    pred_array = np.concatenate([pred_array, model(x_test).detach().cpu().numpy().reshape(-1)])\n",
    "    \n",
    "predict['runoff'] = pred_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mgauch/miniconda3/envs/gwf/lib/python3.7/site-packages/pandas/plotting/_converter.py:129: FutureWarning: Using an implicitly registered datetime converter for a matplotlib plotting method. The converter was registered by pandas on import. Future versions of pandas will require you to explicitly register matplotlib converters.\n",
      "\n",
      "To register the converters:\n",
      "\t>>> from pandas.plotting import register_matplotlib_converters\n",
      "\t>>> register_matplotlib_converters()\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02GA010 \tNSE: -0.14521424424021445 \tMSE: 546.0078587545494 (clipped to 0)\n",
      "02GA018 \tNSE: -0.11914420261325254 \tMSE: 281.07662592673546 (clipped to 0)\n",
      "02GA038 \tNSE: -0.0518527754170941 \tMSE: 173.07023258356543 (clipped to 0)\n",
      "02GA047 \tNSE: -0.4593485833655171 \tMSE: 114.3983714443547 (clipped to 0)\n",
      "02GB001 \tNSE: -0.21653837312290758 \tMSE: 9136.051356463951 (clipped to 0)\n",
      "02GB007 \tNSE: 0.02276503812807873 \tMSE: 30.560105925634023 (clipped to 0)\n",
      "02GC002 \tNSE: -0.003552777015745079 \tMSE: 128.95957724834642 (clipped to 0)\n",
      "02GC007 \tNSE: -0.5340803971868235 \tMSE: 46.2085640941852 (clipped to 0)\n",
      "02GC010 \tNSE: -0.03342948046527394 \tMSE: 61.43729973144633 (clipped to 0)\n",
      "02GC018 \tNSE: 0.0204710935297715 \tMSE: 66.34036003153619 (clipped to 0)\n",
      "02GC026 \tNSE: -0.16336877486771173 \tMSE: 187.10780321361818 (clipped to 0)\n",
      "02GD004 \tNSE: -0.01572587575239215 \tMSE: 55.928988464007354 (clipped to 0)\n",
      "02GE007 \tNSE: 0.03350107254075507 \tMSE: 34.96315522133642 (clipped to 0)\n",
      "02GG002 \tNSE: -0.08161154881481081 \tMSE: 260.8661304398859 (clipped to 0)\n",
      "02GG003 \tNSE: -0.092295077195518 \tMSE: 521.4591257198849 (clipped to 0)\n",
      "02GG006 \tNSE: 0.04537907138938524 \tMSE: 51.174602244917864 (clipped to 0)\n",
      "02GG009 \tNSE: -0.01847685566763002 \tMSE: 158.4275649508438 (clipped to 0)\n",
      "02GG013 \tNSE: 0.040020530783088204 \tMSE: 36.654301006012346 (clipped to 0)\n",
      "04159492 \tNSE: -0.027566356791612456 \tMSE: 477.09269137087153 (clipped to 0)\n",
      "04159900 \tNSE: 0.010513012937408694 \tMSE: 42.99020614039493 (clipped to 0)\n",
      "04160600 \tNSE: -0.004876588770474699 \tMSE: 29.071999625156693 (clipped to 0)\n",
      "04161820 \tNSE: -0.6340837886139801 \tMSE: 51.60117812857123 (clipped to 0)\n",
      "04164000 \tNSE: -0.3080122609067837 \tMSE: 229.19953225444638 (clipped to 0)\n",
      "04165500 \tNSE: -0.14020236327527447 \tMSE: 510.4217562144619 (clipped to 0)\n",
      "04166100 \tNSE: -0.3141878322728162 \tMSE: 9.637343731414761 (clipped to 0)\n",
      "04166500 \tNSE: -0.13796466093648596 \tMSE: 39.33294323331871 (clipped to 0)\n",
      "04174500 \tNSE: -0.12368176947201137 \tMSE: 106.27134551319877 (clipped to 0)\n",
      "04176500 \tNSE: 0.06338916286782259 \tMSE: 639.7747136206677 (clipped to 0)\n",
      "04177000 \tNSE: 0.04211880516402966 \tMSE: 25.814735180031942 (clipped to 0)\n",
      "04193500 \tNSE: -0.20962281709877173 \tMSE: 85642.20178206485 (clipped to 0)\n",
      "04195820 \tNSE: 0.0071592740499033525 \tMSE: 1165.390819943228 (clipped to 0)\n",
      "04196800 \tNSE: -0.011151559693038937 \tMSE: 302.5855743436143 (clipped to 0)\n",
      "04197100 \tNSE: 0.020653415746018866 \tMSE: 100.44610434817646 (clipped to 0)\n",
      "04198000 \tNSE: 0.012011406585890394 \tMSE: 7298.824422280656 (clipped to 0)\n",
      "04199000 \tNSE: -0.020007967242417157 \tMSE: 1043.0635131099382 (clipped to 0)\n",
      "04199500 \tNSE: -0.024380246114703352 \tMSE: 414.5135427633637 (clipped to 0)\n",
      "04200500 \tNSE: -0.02271216817542898 \tMSE: 1023.4331351602926 (clipped to 0)\n",
      "04207200 \tNSE: -0.02377322198047871 \tMSE: 23.962734253419292 (clipped to 0)\n",
      "04208504 \tNSE: -0.09512400398782028 \tMSE: 1069.4199719571993 (clipped to 0)\n",
      "04209000 \tNSE: -0.07355755839239198 \tMSE: 335.65422915256164 (clipped to 0)\n",
      "04212100 \tNSE: 0.010441823916359616 \tMSE: 1452.993045833335 (clipped to 0)\n",
      "04213000 \tNSE: -0.12013106442640265 \tMSE: 201.0290667816275 (clipped to 0)\n",
      "04213500 \tNSE: 0.019730155615175327 \tMSE: 1004.2049396811801 (clipped to 0)\n",
      "04214500 \tNSE: -0.049572295745008654 \tMSE: 126.51238718274905 (clipped to 0)\n",
      "04215000 \tNSE: -0.0067669353946584465 \tMSE: 96.7027959986383 (clipped to 0)\n",
      "04215500 \tNSE: -0.02921417985560426 \tMSE: 185.86055163188465 (clipped to 0)\n",
      "Median NSE (clipped to 0) -0.025973301453157904 / Min -0.6340837886139801 / Max 0.06338916286782259\n",
      "Median MSE (clipped to 0) 179.46539210772505 / Min 9.637343731414761 / Max 85642.20178206485\n"
     ]
    }
   ],
   "source": [
    "nse_list = []\n",
    "mse_list = []\n",
    "grouped_predict = predict.groupby('station')\n",
    "for station in grouped_predict.groups.keys():\n",
    "    station_predict = grouped_predict.get_group(station).set_index('date')\n",
    "    nse, mse = evaluate.evaluate_daily(station, station_predict[['runoff']], station_predict['actual'], writer=writer)\n",
    "    nse_list.append(nse)\n",
    "    mse_list.append(mse)\n",
    "    \n",
    "    print(station, '\\tNSE:', nse, '\\tMSE:', mse, '(clipped to 0)')\n",
    "\n",
    "print('Median NSE (clipped to 0)', np.median(nse_list), '/ Min', np.min(nse_list), '/ Max', np.max(nse_list))\n",
    "print('Median MSE (clipped to 0)', np.median(mse_list), '/ Min', np.min(mse_list), '/ Max', np.max(mse_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LSTM_VIC-oneModel_20190904-215419.pkl'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_data.pickle_results('LSTM_VIC-oneModel', predict[['date', 'station', 'runoff', 'actual']].rename({'runoff': 'prediction'}, axis=1).reset_index(drop=True), time_stamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'20190905-080828'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.now().strftime('%Y%m%d-%H%M%S')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
