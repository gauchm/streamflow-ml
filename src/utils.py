import os
import networkx as nx
import numpy as np
import pandas as pd
import torch
import torchvision.transforms.functional as TF
import random

module_dir = os.path.dirname(os.path.abspath(__file__))


def add_border(data, style, fill_value=np.nan):
    """Adds a one-pixel border to the last two dimension of data to avoid artefacts when rescaling.
    
    Args:
        data: The dataset to enlarge
        style: Determines the filling algorithm, either 'extrapolate', or 'fill_value'.
        fill_value: Value to use for filling if style is 'fill_value'
    Returns:
        Enlarged tensor.
    """
    new_shape = list(s for s in data.shape)
    new_shape[-1] += 2
    new_shape[-2] += 2
    data_with_border = torch.full(new_shape, fill_value, dtype=data.dtype) if isinstance(data,torch.Tensor) \
                    else np.full(new_shape, fill_value, dtype=data.dtype)
    data_with_border[...,1:-1,1:-1] = data
    if style == 'extrapolate':
        data_with_border[...,1:-1,0] = data[:,0] + (data[...,:,0] - data[...,:,1])
        data_with_border[...,1:-1,-1] = data[:,-1] + (data[...,:,-1] - data[...,:,-2])
        data_with_border[...,0,:] = data_with_border[...,1,:] + (data_with_border[...,1,:] - data_with_border[...,2,:])
        data_with_border[...,-1,:] = data_with_border[...,-2,:] + (data_with_border[...,-2,:] - data_with_border[...,-3,:])
    elif style == 'fill_value':
        data_with_border[...,[0,-1],[0,-1]] = fill_value
    else:
        raise Exception('Unsupported style')
    return data_with_border


def map_to_coords(source_lats, source_lons, target_lats, target_lons):
    """Generates a mapping from target cells to (row, col) in the source grid plus a 1-pixel border.
    
    Args:
        source_lats, source_lons: 1d- or 2d-arrays of lats/lons of the source grid
        target_lats, target_lons: 2d-arrays of lats/lons in the target grid
    Returns:
        A dict mapping (row, col) in the target dataset to a row in the source dataset,
        A dict mapping (row, col) in the target dataset to a column in the source dataset,
    """
    source_lats_with_border, source_lons_with_border = add_border(source_lats, 'extrapolate'), add_border(source_lons, 'extrapolate')
    
    if len(target_lats.shape) == 1:
        target_lats = np.tile(target_lats,len(target_lons)).reshape(len(target_lons),-1).T
        target_lons = np.tile(target_lons,len(target_lats)).reshape(len(target_lats),-1)
    
    resample_map_rows = np.zeros(target_lats.shape, dtype=int)
    resample_map_cols = np.zeros(target_lats.shape, dtype=int)
    for i in range(target_lats.shape[0]):
        for j in range(target_lats.shape[1]):
            target_lat, target_lon = target_lats[i,j], target_lons[i,j]
            closest_source_cell = np.argmin(np.square(source_lats_with_border - target_lat) + 
                                            np.square(source_lons_with_border - target_lon))
            resample_map_rows[i,j] = closest_source_cell // source_lats_with_border.shape[1]
            resample_map_cols[i,j] = closest_source_cell % source_lats_with_border.shape[1]
    
    return resample_map_rows, resample_map_cols


def resample_by_map(lowres_data, resample_map_rows, resample_map_cols, fill_value=np.nan):
    """Resamples last two dimensions of lowres_data based on resample_map, adding the 1-pixel border to lowres_data to avoid artifacts.
    
    Args:
        lowres_data: Dataset to resample
        resample_map_rows, resample_map_cols: dicts as generated by map_to_coords, mapping target to source cells.
        fill_value: Value to use for filling of the border.
    Returns:
        The resampled tensor.
    """
    return add_border(lowres_data, 'fill_value', fill_value=fill_value)[..., resample_map_rows, resample_map_cols]


def random_transform(rdrs_batch, geophysical_batch, y_batch, y_means, train_mask, val_mask, rdrs_contains_month=False, border_masking=0, p=0.5):
    """Randomly rotates and flips input and target data.
    
    Args:
        rdrs_batch: Batch of RDRS input data of shape (#batch, #timesteps, #vars, #rows, #cols)
        geophysical_batch: Geophysical data of shape (#batch, #vars, #rows, #cols)
        y_batch: Target values of shape (#rows, #cols)
        y_means: Mean target values per subbasin/station of shape (#rows, #cols)
        train_mask, val_mask: Masks of shape (#rows, #cols)
        rdrs_contains_month (bool): If True, will not rotate the last 12 RDRS variables, which are assumed to be one-hot-encoded months.
        border_masking (int): train_mask and val_mask values within this border will be set to False, as the corresponding station/subbasin might be rotated out of the image.
        p: Probability of transforming.
        
    Returns:
        Transformed RDRS batch, geophysical batch, target values, target means, train_mask and val_mask
    """
    if random.random() < p:
        angle = random.randint(-180, 180)
        horizontal_flip = random.choice([True, False])
        vertical_flip = random.choice([True, False])
        transformed_tensors = []
        for tensor in [rdrs_batch, geophysical_batch, y_batch, y_means, train_mask.float(), val_mask.float()]:
            images = [TF.to_pil_image(image, mode='F') for image in tensor.reshape((-1,*tensor.shape[-2:]))]
            images = [TF.rotate(image, angle) for image in images]
            images = [TF.hflip(image) for image in images] if horizontal_flip else images
            images = [TF.vflip(image) for image in images] if vertical_flip else images
            
            transformed_tensors.append(torch.cat([TF.to_tensor(image) for image in images]).reshape(tensor.shape))
        
        rdrs_transformed, geophysical_transformed, y_transformed, \
            y_means_transformed, train_mask_transformed, val_mask_transformed = transformed_tensors
        
        # Fix "month" features that are all-0/all-1 images
        if rdrs_contains_month:
            rdrs_transformed[:,:,-12:] = rdrs_batch[:,:,-12:]
            
        # Do not consider subbasins that are rotated almost out of the image for training
        border_mask = torch.zeros(train_mask.shape, dtype=torch.bool)
        border_mask[border_masking:-border_masking,border_masking:-border_masking] = True
        return rdrs_transformed, geophysical_transformed, y_transformed, y_means_transformed, \
                train_mask_transformed.bool() & border_mask, val_mask_transformed.bool() & border_mask
    
    return rdrs_batch, geophysical_batch, y_batch, y_means, train_mask, val_mask


def create_subbasin_graph():
    """Creates a directed graph of subbasin downstream relationships.
    
    Returns:
        An nx.DiGraph of subbasins
    """
    subbasin_to_downstream = pd.read_csv(module_dir + '/../data/simulations_shervan/test.rvh', sep='\s+', skiprows=7, nrows=724, names=['subbasin', 'downstream_subbasin'], usecols=[1,2])
    subbasin_to_downstream['subbasin'] = subbasin_to_downstream['subbasin']
    subbasin_to_downstream['downstream_subbasin'] = 'sub' + subbasin_to_downstream['downstream_subbasin'].astype(str)
    subbasin_to_downstream['edge'] = 1

    for subbasin in subbasin_to_downstream['subbasin'].unique():
        is_sink = 1 if len(subbasin_to_downstream[(subbasin_to_downstream['subbasin'] == subbasin) & subbasin_to_downstream['edge'] == 1]) == 0 else 0
        subbasin_to_downstream = subbasin_to_downstream.append({'subbasin': subbasin, 'downstream_subbasin': subbasin, 'edge': is_sink}, ignore_index=True)
    subbasin_to_downstream = subbasin_to_downstream.append({'subbasin': 'sub-1', 'downstream_subbasin': 'sub-1', 'edge': 1}, ignore_index=True)
    
    adj = subbasin_to_downstream.pivot(index='subbasin', columns='downstream_subbasin', values='edge').fillna(0)    
    adj = adj.sort_index(axis=0).sort_index(axis=1)
    
    G = nx.from_numpy_matrix(adj.values, parallel_edges=False, create_using=nx.DiGraph())
    label_mapping = dict(zip(range(len(adj.values)), adj.index))
    G = nx.relabel_nodes(G, label_mapping)
    
    return G


def create_hop_matrix(G, max_hops, node_list):
    """Creates a matrix of hop-connectedness.
    
    Args:
        G: nx.Graph
        max_hops: Maximum number of hops to consider
        node_list: List of nodes determining the order in the output tensor.
        
    Returns:
        torch.Tensor of shape (max_hops, #nodes, #nodes), 
            where entry (i,v,w) is 1 if shortest-path distance from node v to w is i, else 0.
    """
    distances = dict(nx.all_pairs_dijkstra_path_length(G))
    hop_matrix = torch.zeros(max_hops, G.number_of_nodes(), G.number_of_nodes(), dtype=torch.int)
    for hop in range(max_hops):
        for i, node_from in enumerate(node_list):
            for j, node_to in enumerate(node_list):
                if node_to in distances[node_from].keys() and distances[node_from][node_to] == hop:
                    hop_matrix[hop,i,j] = 1
    return hop_matrix


def random_graph_subsample_with_sources(graph, components, comp_subsample_fraction, p_node_subsample):
    """Randomly selects components from the graph, and reduces each component with probability p_node_subsample.
    
    First, selects a subset of comp_subsample_fraction connected components of graph.
    With probability p_node_subsample, further subsamples each selected component's nodes. If this subsampling 
    is performed for a component, the node subset is always a sub-component that contains the component's source(s).
    
    Arguments:
        graph: nx.DiGraph, the input graph
        components: list of pre-computed connected components
        comp_subsample_fraction: fraction of components to subsample
        p_node_subsample: Probability of node subsampling in each selected component
    
    Returns:
        list of selected nodes
    """
    subsampled_components = np.random.choice(components, size=int(comp_subsample_fraction * len(components)), replace=False)
    subsampled_component_nodes = []
    for comp in subsampled_components:
        if np.random.random() < p_node_subsample:
            # select a subgraph, rooted in the component's source nodes
            # start with sources
            subsampled_nodes = [n for n, in_deg in graph.subgraph(['sub' + str(c) for c in comp]).in_degree if in_deg==0]
            subsampled_comp_size = np.random.randint(len(comp))
            while len(subsampled_nodes) < subsampled_comp_size:  # expand neighborhood subsampled_comp_size steps
                subsampled_nodes += [s for n in subsampled_nodes for s in graph.successors(n)]
            subsampled_component_nodes += subsampled_nodes
        else:
            # use all nodes in the component
            subsampled_component_nodes = ['sub' + str(n) for comp in subsampled_components for n in comp]
            
    return sorted(int(n[3:]) for n in subsampled_component_nodes)